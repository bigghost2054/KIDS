  0%|          | 0/7600 [00:00<?, ?it/s]  0%|          | 1/7600 [00:03<6:33:13,  3.10s/it] 13%|█▎        | 1001/7600 [00:03<00:15, 425.43it/s] 26%|██▋       | 2001/7600 [00:03<00:05, 936.39it/s] 39%|███▉      | 3001/7600 [00:06<00:08, 548.67it/s] 53%|█████▎    | 4001/7600 [00:06<00:04, 847.21it/s] 66%|██████▌   | 5001/7600 [00:06<00:02, 1235.57it/s] 79%|███████▉  | 6001/7600 [00:09<00:02, 711.95it/s] 100%|██████████| 7600/7600 [00:09<00:00, 836.77it/s]
  0%|          | 0/7600 [00:00<?, ?it/s]  0%|          | 1/7600 [00:03<6:27:53,  3.06s/it] 13%|█▎        | 1001/7600 [00:03<00:15, 427.65it/s] 26%|██▋       | 2001/7600 [00:03<00:05, 947.19it/s] 39%|███▉      | 3001/7600 [00:06<00:08, 554.40it/s] 53%|█████▎    | 4001/7600 [00:06<00:04, 848.83it/s] 66%|██████▌   | 5001/7600 [00:06<00:02, 1242.26it/s] 79%|███████▉  | 6001/7600 [00:09<00:02, 718.55it/s] 100%|██████████| 7600/7600 [00:09<00:00, 844.03it/s]
  0%|          | 0/7600 [00:00<?, ?it/s]  0%|          | 1/7600 [00:03<6:45:22,  3.20s/it] 13%|█▎        | 1001/7600 [00:03<00:16, 410.15it/s] 26%|██▋       | 2001/7600 [00:03<00:06, 914.66it/s] 39%|███▉      | 3001/7600 [00:06<00:08, 540.80it/s] 53%|█████▎    | 4001/7600 [00:06<00:04, 828.90it/s] 66%|██████▌   | 5001/7600 [00:06<00:02, 1221.11it/s] 79%|███████▉  | 6001/7600 [00:09<00:02, 704.04it/s] 100%|██████████| 7600/7600 [00:09<00:00, 822.31it/s]
  0%|          | 0/7600 [00:00<?, ?it/s]  0%|          | 1/7600 [00:03<6:49:39,  3.23s/it] 13%|█▎        | 1001/7600 [00:03<00:16, 405.66it/s] 26%|██▋       | 2001/7600 [00:03<00:06, 889.92it/s] 39%|███▉      | 3001/7600 [00:06<00:08, 543.10it/s] 53%|█████▎    | 4001/7600 [00:06<00:04, 829.09it/s] 66%|██████▌   | 5001/7600 [00:06<00:02, 1229.91it/s] 79%|███████▉  | 6001/7600 [00:09<00:02, 709.29it/s] 100%|██████████| 7600/7600 [00:09<00:00, 822.98it/s]
  0%|          | 0/7600 [00:00<?, ?it/s]  0%|          | 1/7600 [00:03<6:49:13,  3.23s/it] 13%|█▎        | 1001/7600 [00:03<00:16, 404.57it/s] 26%|██▋       | 2001/7600 [00:03<00:06, 905.22it/s] 39%|███▉      | 3001/7600 [00:06<00:08, 543.04it/s] 53%|█████▎    | 4001/7600 [00:06<00:04, 825.74it/s] 66%|██████▌   | 5001/7600 [00:06<00:02, 1226.63it/s] 79%|███████▉  | 6001/7600 [00:09<00:02, 706.59it/s] 100%|██████████| 7600/7600 [00:09<00:00, 821.84it/s]
  0%|          | 0/7600 [00:00<?, ?it/s]  0%|          | 1/7600 [00:03<6:32:49,  3.10s/it] 13%|█▎        | 1001/7600 [00:03<00:15, 425.08it/s] 26%|██▋       | 2001/7600 [00:03<00:05, 936.47it/s] 39%|███▉      | 3001/7600 [00:06<00:08, 548.68it/s] 53%|█████▎    | 4001/7600 [00:06<00:04, 845.76it/s] 66%|██████▌   | 5001/7600 [00:06<00:02, 1221.49it/s] 79%|███████▉  | 6001/7600 [00:09<00:02, 707.41it/s] 100%|██████████| 7600/7600 [00:09<00:00, 833.52it/s]
  0%|          | 0/7600 [00:00<?, ?it/s]  0%|          | 1/7600 [00:03<6:52:15,  3.26s/it] 13%|█▎        | 1001/7600 [00:03<00:16, 404.88it/s] 26%|██▋       | 2001/7600 [00:03<00:06, 887.28it/s] 39%|███▉      | 3001/7600 [00:06<00:08, 529.78it/s] 53%|█████▎    | 4001/7600 [00:06<00:04, 823.53it/s] 66%|██████▌   | 5001/7600 [00:06<00:02, 1208.40it/s] 79%|███████▉  | 6001/7600 [00:09<00:02, 670.25it/s] 100%|██████████| 7600/7600 [00:09<00:00, 797.13it/s]
  0%|          | 0/7600 [00:00<?, ?it/s]  0%|          | 1/7600 [00:03<6:28:34,  3.07s/it] 13%|█▎        | 1001/7600 [00:03<00:15, 426.44it/s] 26%|██▋       | 2001/7600 [00:03<00:05, 935.20it/s] 39%|███▉      | 3001/7600 [00:06<00:08, 550.50it/s] 53%|█████▎    | 4001/7600 [00:06<00:04, 840.05it/s] 66%|██████▌   | 5001/7600 [00:06<00:02, 1226.75it/s] 79%|███████▉  | 6001/7600 [00:09<00:02, 710.97it/s] 100%|██████████| 7600/7600 [00:09<00:00, 836.56it/s]
  0%|          | 0/7600 [00:00<?, ?it/s]  0%|          | 1/7600 [00:03<6:27:09,  3.06s/it] 13%|█▎        | 1001/7600 [00:03<00:15, 430.69it/s] 26%|██▋       | 2001/7600 [00:03<00:05, 948.38it/s] 39%|███▉      | 3001/7600 [00:06<00:08, 553.05it/s] 53%|█████▎    | 4001/7600 [00:06<00:04, 848.19it/s] 66%|██████▌   | 5001/7600 [00:06<00:02, 1226.81it/s] 79%|███████▉  | 6001/7600 [00:09<00:02, 707.71it/s] 100%|██████████| 7600/7600 [00:09<00:00, 837.77it/s]
  0%|          | 0/7600 [00:00<?, ?it/s]  0%|          | 1/7600 [00:03<6:47:29,  3.22s/it] 13%|█▎        | 1001/7600 [00:03<00:15, 412.84it/s] 26%|██▋       | 2001/7600 [00:03<00:06, 908.78it/s] 39%|███▉      | 3001/7600 [00:06<00:08, 544.22it/s] 53%|█████▎    | 4001/7600 [00:06<00:04, 843.61it/s] 66%|██████▌   | 5001/7600 [00:06<00:02, 1221.38it/s] 79%|███████▉  | 6001/7600 [00:09<00:02, 705.65it/s] 100%|██████████| 7600/7600 [00:09<00:00, 824.94it/s]
  0%|          | 0/9500 [00:00<?, ?it/s]  0%|          | 1/9500 [00:03<9:47:41,  3.71s/it] 11%|█         | 1001/9500 [00:03<00:23, 362.29it/s] 21%|██        | 2001/9500 [00:04<00:09, 811.93it/s] 32%|███▏      | 3001/9500 [00:07<00:14, 454.21it/s] 42%|████▏     | 4001/9500 [00:07<00:07, 711.92it/s] 53%|█████▎    | 5001/9500 [00:07<00:04, 1044.11it/s] 63%|██████▎   | 6001/9500 [00:11<00:06, 568.36it/s]  74%|███████▎  | 7001/9500 [00:11<00:03, 806.94it/s] 84%|████████▍ | 8001/9500 [00:11<00:01, 1126.64it/s] 95%|█████████▍| 9001/9500 [00:12<00:00, 938.44it/s] 100%|██████████| 9500/9500 [00:12<00:00, 741.19it/s]
  0%|          | 0/9500 [00:00<?, ?it/s]  0%|          | 1/9500 [00:03<10:26:13,  3.96s/it] 11%|█         | 1001/9500 [00:04<00:24, 340.67it/s] 21%|██        | 2001/9500 [00:04<00:09, 765.08it/s] 32%|███▏      | 3001/9500 [00:07<00:14, 438.91it/s] 42%|████▏     | 4001/9500 [00:07<00:07, 693.31it/s] 53%|█████▎    | 5001/9500 [00:08<00:04, 1019.08it/s] 63%|██████▎   | 6001/9500 [00:11<00:06, 550.31it/s]  74%|███████▎  | 7001/9500 [00:11<00:03, 791.29it/s] 84%|████████▍ | 8001/9500 [00:11<00:01, 1098.86it/s] 95%|█████████▍| 9001/9500 [00:13<00:00, 910.94it/s] 100%|██████████| 9500/9500 [00:13<00:00, 715.50it/s]
  0%|          | 0/9500 [00:00<?, ?it/s]  0%|          | 1/9500 [00:03<10:18:00,  3.90s/it] 11%|█         | 1001/9500 [00:04<00:24, 346.09it/s] 21%|██        | 2001/9500 [00:04<00:09, 784.65it/s] 32%|███▏      | 3001/9500 [00:07<00:14, 444.33it/s] 42%|████▏     | 4001/9500 [00:07<00:07, 701.63it/s] 53%|█████▎    | 5001/9500 [00:07<00:04, 1021.31it/s] 63%|██████▎   | 6001/9500 [00:11<00:06, 558.61it/s]  74%|███████▎  | 7001/9500 [00:11<00:03, 807.04it/s] 84%|████████▍ | 8001/9500 [00:11<00:01, 1117.83it/s] 95%|█████████▍| 9001/9500 [00:13<00:00, 922.34it/s] 100%|██████████| 9500/9500 [00:13<00:00, 725.53it/s]
  0%|          | 0/9500 [00:00<?, ?it/s]  0%|          | 1/9500 [00:03<10:20:07,  3.92s/it] 11%|█         | 1001/9500 [00:04<00:24, 346.64it/s] 21%|██        | 2001/9500 [00:04<00:09, 773.20it/s] 32%|███▏      | 3001/9500 [00:07<00:14, 445.07it/s] 42%|████▏     | 4001/9500 [00:07<00:07, 699.21it/s] 53%|█████▎    | 5001/9500 [00:07<00:04, 1036.16it/s] 63%|██████▎   | 6001/9500 [00:11<00:06, 561.85it/s]  74%|███████▎  | 7001/9500 [00:11<00:03, 799.40it/s] 84%|████████▍ | 8001/9500 [00:11<00:01, 1120.51it/s] 95%|█████████▍| 9001/9500 [00:13<00:00, 929.36it/s] 100%|██████████| 9500/9500 [00:13<00:00, 726.90it/s]
  0%|          | 0/9500 [00:00<?, ?it/s]  0%|          | 1/9500 [00:03<10:14:19,  3.88s/it] 11%|█         | 1001/9500 [00:04<00:24, 348.37it/s] 21%|██        | 2001/9500 [00:04<00:09, 783.16it/s] 32%|███▏      | 3001/9500 [00:07<00:14, 444.61it/s] 42%|████▏     | 4001/9500 [00:07<00:07, 700.98it/s] 53%|█████▎    | 5001/9500 [00:07<00:04, 1031.15it/s] 63%|██████▎   | 6001/9500 [00:11<00:06, 556.46it/s]  74%|███████▎  | 7001/9500 [00:11<00:03, 796.25it/s] 84%|████████▍ | 8001/9500 [00:11<00:01, 1114.87it/s] 95%|█████████▍| 9001/9500 [00:13<00:00, 923.48it/s] 100%|██████████| 9500/9500 [00:13<00:00, 725.51it/s]
85
Iteration: 208
Loss: 0.0020456058720563666
Iteration: 209
Loss: 0.002034478345138515
Iteration: 210
Loss: 0.0020325590287231738
Iteration: 211
Loss: 0.0020432533943496737
Iteration: 212
Loss: 0.0020272118099792687
Iteration: 213
Loss: 0.00203219333226666
Iteration: 214
Loss: 0.002022675441198603
Iteration: 215
Loss: 0.0020216660901766132
Iteration: 216
Loss: 0.0020195503509315995
Iteration: 217
Loss: 0.0020231692951149594
Iteration: 218
Loss: 0.002025616316435238
Iteration: 219
Loss: 0.0020225330135198653
Iteration: 220
Loss: 0.0020250999282493635
Iteration: 221
Loss: 0.002025207477322791
Iteration: 222
Loss: 0.002014106628197579
Iteration: 223
Loss: 0.0020162755678074414
Iteration: 224
Loss: 0.0020022091907046643
Iteration: 225
Loss: 0.0020166710565257587
Iteration: 226
Loss: 0.0020070474747177444
Iteration: 227
Loss: 0.002006378846909897
Iteration: 228
Loss: 0.002010382765064729
Iteration: 229
Loss: 0.0020038553910260953
Iteration: 230
Loss: 0.0020075806324588662
Iteration: 231
Loss: 0.0020053415231544663
Iteration: 232
Loss: 0.002009340408143162
Iteration: 233
Loss: 0.002002466734452748
Iteration: 234
Loss: 0.00199762980574397
Iteration: 235
Loss: 0.0019869586570694306
Iteration: 236
Loss: 0.001985002746000702
Iteration: 237
Loss: 0.0019971460109737553
Iteration: 238
Loss: 0.0020043169461581626
Iteration: 239
Loss: 0.0019965256265376085
Iteration: 240
Loss: 0.0019903183098376534
Iteration: 241
Loss: 0.0019853296502849753
Iteration: 242
Loss: 0.0019923097074583725
Iteration: 243
Loss: 0.001988614406321704
Iteration: 244
Loss: 0.0019846929346475704
Iteration: 245
Loss: 0.0019827069381228935
Iteration: 246
Loss: 0.001996977918746847
Iteration: 247
Loss: 0.00198398961076214
Iteration: 248
Loss: 0.001974908318830493
Iteration: 249
Loss: 0.0019828450095690326
Iteration: 250
Loss: 0.001979730582083173
Iteration: 251
Loss: 0.0019786681916656684
Iteration: 252
Loss: 0.0019771199627422993
Iteration: 253
Loss: 0.0019775920182489503
Iteration: 254
Loss: 0.001985606573591078
Iteration: 255
Loss: 0.0019775640699483546
Iteration: 256
Loss: 0.0019722881333120628
Iteration: 257
Loss: 0.0019682672300180533
Iteration: 258
Loss: 0.001973280629982459
Iteration: 259
Loss: 0.0019691241833231883
Iteration: 260
Loss: 0.001973262174560884
Iteration: 261
Loss: 0.0019717769608200517
Iteration: 262
Loss: 0.001973318312616076
Iteration: 263
Loss: 0.0019738729669494027
Iteration: 264
Loss: 0.0019678654548723573
Iteration: 265
Loss: 0.0019698293583559584
Iteration: 266
Loss: 0.001964204734835176
Iteration: 267
Loss: 0.001961478448967323
Iteration: 268
Loss: 0.0019543760378534594
Iteration: 269
Loss: 0.001961005761081146
Iteration: 270
Loss: 0.001962036286756672
Iteration: 271
Loss: 0.001955258973333755
Iteration: 272
Loss: 0.0019578182290848575
Iteration: 273
Loss: 0.0019671214919790257
Iteration: 274
Loss: 0.0019581579002494244
Iteration: 275
Loss: 0.0019607129233693817
Iteration: 276
Loss: 0.0019643292906061736
Iteration: 277
Loss: 0.0019640339449346986
Iteration: 278
Loss: 0.0019459775189098753
Iteration: 279
Loss: 0.0019489343574178623
Iteration: 280
Loss: 0.0019586581052683386
Iteration: 281
Loss: 0.001954851135967966
Iteration: 282
Loss: 0.001953020220461451
Iteration: 283
Loss: 0.0019556054900066906
Iteration: 284
Loss: 0.001963113579772597
Iteration: 285
Loss: 0.001943154072239535
Iteration: 286
Loss: 0.001942812202118889
Iteration: 287
Loss: 0.001956344577828767
Iteration: 288
Loss: 0.0019487005279020028
Iteration: 289
Loss: 0.001939918855952536
Iteration: 290
Loss: 0.0019479812177496008
Iteration: 291
Loss: 0.0019438279372996387
Iteration: 292
Loss: 0.0019540415076469936
Iteration: 293
Loss: 0.0019494006597080532
Iteration: 294
Loss: 0.0019472433745654093
Iteration: 295
Loss: 0.0019461589917126629
Iteration: 296
Loss: 0.0019431064483123245
Iteration: 297
Loss: 0.0019376519636285525
Iteration: 298
Loss: 0.0019438272316215767
Iteration: 299
Loss: 0.0019524083400927024
Iteration: 300
Loss: 0.0019501702742145202
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.1962421711899791
accuracy: 0.9493421052631579
confusion: 47 280 105 7168
precision: 0.1437308868501529
recall: 0.3092105263157895
Processing fold: ../../../kg_constructor/output/salmonella/folds/fold_1
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 84531
Starting training...
Iteration: 1
Loss: 0.7018552657998638
Iteration: 2
Loss: 0.693918823460002
Iteration: 3
Loss: 0.6733588348200292
Iteration: 4
Loss: 0.6356749240263009
Iteration: 5
Loss: 0.5794258794666808
Iteration: 6
Loss: 0.520954081305751
Iteration: 7
Loss: 0.4690368951838694
Iteration: 8
Loss: 0.42923241412198104
Iteration: 9
Loss: 0.39786275411829536
Iteration: 10
Loss: 0.3731868899898765
Iteration: 11
Loss: 0.3514754124629645
Iteration: 12
Loss: 0.3340383205148909
Iteration: 13
Loss: 0.3182666684374397
Iteration: 14
Loss: 0.3016539013680117
Iteration: 15
Loss: 0.2877259210303978
Iteration: 16
Loss: 0.2735641355867739
Iteration: 17
Loss: 0.2592047467643832
Iteration: 18
Loss: 0.2459510772684474
Iteration: 19
Loss: 0.23149200354093386
Iteration: 20
Loss: 0.2175874415739083
Iteration: 21
Loss: 0.20451684903215478
Iteration: 22
Loss: 0.19124698804484475
Iteration: 23
Loss: 0.17869243264934162
Iteration: 24
Loss: 0.16632911636505598
Iteration: 25
Loss: 0.15387047790450814
Iteration: 26
Loss: 0.1430209588121485
Iteration: 27
Loss: 0.1324329709197268
Iteration: 28
Loss: 0.12264008378541028
Iteration: 29
Loss: 0.11289892134107189
Iteration: 30
Loss: 0.10433308495047652
Iteration: 31
Loss: 0.09584490439774078
Iteration: 32
Loss: 0.08843291606064196
Iteration: 33
Loss: 0.0816285103375529
Iteration: 34
Loss: 0.07530164617447206
Iteration: 35
Loss: 0.06950880292757058
Iteration: 36
Loss: 0.06414149844168145
Iteration: 37
Loss: 0.05925526814880194
Iteration: 38
Loss: 0.055072621982774614
Iteration: 39
Loss: 0.05095556750893593
Iteration: 40
Loss: 0.047188305836400865
Iteration: 41
Loss: 0.04355424716148847
Iteration: 42
Loss: 0.04058566873456225
Iteration: 43
Loss: 0.037980162895020146
Iteration: 44
Loss: 0.03547817243286121
Iteration: 45
Loss: 0.03293316254829183
Iteration: 46
Loss: 0.030882048110167187
Iteration: 47
Loss: 0.028965719366147193
Iteration: 48
Loss: 0.02731228385258604
Iteration: 49
Loss: 0.0255163732778143
Iteration: 50
Loss: 0.02395789095281083
Iteration: 51
Loss: 0.022604117997818522
Iteration: 52
Loss: 0.021426233934399522
Iteration: 53
Loss: 0.020271791789082835
Iteration: 54
Loss: 0.019170674897822332
Iteration: 55
Loss: 0.018130912135044735
Iteration: 56
Loss: 0.01727546307683727
Iteration: 57
Loss: 0.016449507797297872
Iteration: 58
Loss: 0.015611513414316706
Iteration: 59
Loss: 0.01491042470306526
Iteration: 60
Loss: 0.014209534386517826
Iteration: 61
Loss: 0.013580389688780278
Iteration: 62
Loss: 0.013074980013900332
Iteration: 63
Loss: 0.01249129444728663
Iteration: 64
Loss: 0.01192702244921231
Iteration: 65
Loss: 0.011456187756985058
Iteration: 66
Loss: 0.0110615937407186
Iteration: 67
Loss: 0.010557890846681448
Iteration: 68
Loss: 0.01021224001629485
Iteration: 69
Loss: 0.009770727992333748
Iteration: 70
Loss: 0.009458386603696847
Iteration: 71
Loss: 0.009092337313901496
Iteration: 72
Loss: 0.008779873531081794
Iteration: 73
Loss: 0.008510176690272342
Iteration: 74
Loss: 0.008234016401808203
Iteration: 75
Loss: 0.007990116874376932
Iteration: 76
Loss: 0.007711031698380355
Iteration: 77
Loss: 0.007514741961602811
Iteration: 78
Loss: 0.007256334509562563
Iteration: 79
Loss: 0.00711077402840242
Iteration: 80
Loss: 0.006843053718545922
Iteration: 81
Loss: 0.006698421283084669
Iteration: 82
Loss: 0.0064816694522713434
Iteration: 83
Loss: 0.006353311280915399
Iteration: 84
Loss: 0.006183991059200999
Iteration: 85
Loss: 0.00599778637887887
Iteration: 86
Loss: 0.00589487731548739
Iteration: 87
Loss: 0.005740826667780861
Iteration: 88
Loss: 0.005579849992545299
Iteration: 89
Loss: 0.0054471541403068435
Iteration: 90
Loss: 0.0053334131284996315
Iteration: 91
Loss: 0.005210942605993262
Iteration: 92
Loss: 0.005119562677947091
Iteration: 93
Loss: 0.005018840497934524
Iteration: 94
Loss: 0.004922480414403073
Iteration: 95
Loss: 0.004799154955019922
Iteration: 96
Loss: 0.00471048891245399
Iteration: 97
Loss: 0.004613219320567118
Iteration: 98
Loss: 0.0045277833260227865
Iteration: 99
Loss: 0.00442797134998312
Iteration: 100
Loss: 0.00434317482623881
Iteration: 101
Loss: 0.0042909385527219665
Iteration: 102
Loss: 0.004204675503489044
Iteration: 103
Loss: 0.004097907987917647
Iteration: 104
Loss: 0.00404110252512274
Iteration: 105
Loss: 0.003956697734624699
Iteration: 106
Loss: 0.0038944299212070525
Iteration: 107
Loss: 0.0038289099552289204
Iteration: 108
Loss: 0.003777550665638697
Iteration: 109
Loss: 0.0037070817348581774
Iteration: 110
Loss: 0.0036336540669938662
Iteration: 111
Loss: 0.0035959522234114967
Iteration: 112
Loss: 0.0035147039668151627
Iteration: 113
Loss: 0.003471468859869573
Iteration: 114
Loss: 0.003409748120854298
Iteration: 115
Loss: 0.0033654221534774997
Iteration: 116
Loss: 0.0033131915397574137
Iteration: 117
Loss: 0.003252285644756975
Iteration: 118
Loss: 0.003205226392220752
Iteration: 119
Loss: 0.0031782221997639646
Iteration: 120
Loss: 0.003123019206809041
Iteration: 121
Loss: 0.0030839146965723715
Iteration: 122
Loss: 0.003052950356684533
Iteration: 123
Loss: 0.0029967321646342305
Iteration: 124
Loss: 0.002988556146989634
Iteration: 125
Loss: 0.002952918309695375
Iteration: 126
Loss: 0.0028997236333879422
Iteration: 127
Loss: 0.002876707910341613
Iteration: 128
Loss: 0.002855257487591402
Iteration: 129
Loss: 0.0028172752814206443
Iteration: 130
Loss: 0.00277891555617069
Iteration: 131
Loss: 0.002758797692180968
Iteration: 132
Loss: 0.0027340891753771792
Iteration: 133
Loss: 0.0027072327872623265
Iteration: 134
Loss: 0.0026732013951757073
Iteration: 135
Loss: 0.002674741521017787
Iteration: 136
Loss: 0.00262217067848937
Iteration: 137
Loss: 0.0025954308177999876
Iteration: 138
Loss: 0.0025823703064456766
Iteration: 139
Loss: 0.0025553209333462113
Iteration: 140
Loss: 0.002539598451996291
Iteration: 141
Loss: 0.0025137571336264594
Iteration: 142
Loss: 0.0024997842739577646
Iteration: 143
Loss: 0.0024902528629028873
Iteration: 144
Loss: 0.0024545250063831054
Iteration: 145
Loss: 0.0024377366780866812
Iteration: 146
Loss: 0.0024482385905022605
Iteration: 147
Loss: 0.0024204972792232844
Iteration: 148
Loss: 0.0024009327530676934
Iteration: 149
Loss: 0.0023958225889752307
Iteration: 150
Loss: 0.002384490117913595
Iteration: 151
Loss: 0.0023649838569079653
Iteration: 152
Loss: 0.0023570025977851065
Iteration: 153
Loss: 0.002331002828101685
Iteration: 154
Loss: 0.002323044941151216
Iteration: 155
Loss: 0.0023242374307211535
Iteration: 156
Loss: 0.0023124775515846263
Iteration: 157
Loss: 0.00229395813320154
Iteration: 158
Loss: 0.0022797001910936318
Iteration: 159
Loss: 0.0022828837965879543
Iteration: 160
Loss: 0.0022534481738406566
Iteration: 161
Loss: 0.002251952012747894
Iteration: 162
Loss: 0.0022450456838410946
Iteration: 163
Loss: 0.0022312036922408473
Iteration: 164
Loss: 0.0022199057031100913
Iteration: 165
Loss: 0.002220726640963996
Iteration: 166
Loss: 0.002213536252923034
Iteration: 167
Loss: 0.002214455399317322
Iteration: 168
Loss: 0.0022042500757738764
Iteration: 169
Loss: 0.002199480659328401
Iteration: 170
Loss: 0.002180085794850724
Iteration: 171
Loss: 0.0021844923490497432
Iteration: 172
Loss: 0.0021784024637905353
Iteration: 173
Loss: 0.002173438872337157
Iteration: 174
Loss: 0.0021616159090312357
Iteration: 175
Loss: 0.0021659225702607707
Iteration: 176
Loss: 0.0021582144703486085
Iteration: 177
Loss: 0.0021480467317073985
Iteration: 178
Loss: 0.0021343166425387247
Iteration: 179
Loss: 0.002140009655030789
Iteration: 180
Loss: 0.002134208836201808
Iteration: 181
Loss: 0.00212514481972903
Iteration: 182
Loss: 0.00212355670346706
Iteration: 183
Loss: 0.002113315923553374
Iteration: 184
Loss: 0.002112890471543335
Iteration: 185
Loss: 0.002113066357847901
Iteration: 186
Loss: 0.0021161556726804484
Iteration: 187
Loss: 0.0021007831731558214
Iteration: 188
Loss: 0.0020999464703677796
Iteration: 189
Loss: 0.0021048817211578104
Iteration: 190
Loss: 0.002099064628616619
Iteration: 191
Loss: 0.002077401006295357
Iteration: 192
Loss: 0.0020885738058383633
Iteration: 193
Loss: 0.002089401285945909
Iteration: 194
Loss: 0.002078788800532987
Iteration: 195
Loss: 0.0020796571280462324
Iteration: 196
Loss: 0.0020668806240859406
Iteration: 197
Loss: 0.002063532292348459
Iteration: 198
Loss: 0.002063353278639692
Iteration: 199
Loss: 0.002066588855670466
Iteration: 200
Loss: 0.002067412593240999
Iteration: 201
Loss: 0.0020554739030445982
Iteration: 202
Loss: 0.0020558752139662336
Iteration: 203
Loss: 0.002038506558164954
Iteration: 204
Loss: 0.0020446592300877343
Iteration: 205
Loss: 0.002049114517752587
Iteration: 206
Loss: 0.002049851924020015
Iteration: 207
Loss: 0.0020473900791487575
Iteration: 208
Loss: 0.0020459974357099446
Iteration: 209
Loss: 0.0020488951438578005
Iteration: 210
Loss: 0.002040733682440111
Iteration: 211
Loss: 0.0020321516728281606
Iteration: 212
Loss: 0.002036421913308678
Iteration: 213
Loss: 0.0020267487141407197
Iteration: 214
Loss: 0.0020252893790947617
Iteration: 215
Loss: 0.002028623325635254
Iteration: 216
Loss: 0.0020300078818022652
Iteration: 217
Loss: 0.0020213012057529005
Iteration: 218
Loss: 0.0020112726940876907
Iteration: 219
Loss: 0.0020099525529615305
Iteration: 220
Loss: 0.0020276738825127667
Iteration: 221
Loss: 0.002025380839858158
Iteration: 222
Loss: 0.002015244153926126
Iteration: 223
Loss: 0.002008642511223845
Iteration: 224
Loss: 0.0020076496150045666
Iteration: 225
Loss: 0.0020121963260649346
Iteration: 226
Loss: 0.0020044011431803676
Iteration: 227
Loss: 0.0020134900279780046
Iteration: 228
Loss: 0.001997040745652752
Iteration: 229
Loss: 0.0020008540703673606
Iteration: 230
Loss: 0.0020032127942590985
Iteration: 231
Loss: 0.0020046218393232535
Iteration: 232
Loss: 0.0020004332677840634
Iteration: 233
Loss: 0.002000992528427346
Iteration: 234
Loss: 0.0019962672621333674
Iteration: 235
Loss: 0.002007774160714981
Iteration: 236
Loss: 0.0019857606557370337
Iteration: 237
Loss: 0.0020078606299918007
Iteration: 238
Loss: 0.0019955022941791902
Iteration: 239
Loss: 0.001998525552658571
Iteration: 240
Loss: 0.0019922675751724546
Iteration: 241
Loss: 0.0019908332421133914
Iteration: 242
Loss: 0.0019829151131511655
Iteration: 243
Loss: 0.0019804905341179284
Iteration: 244
Loss: 0.001991867991796706
Iteration: 245
Loss: 0.0019959457345122907
Iteration: 246
Loss: 0.001987446303605849
Iteration: 247
Loss: 0.0019785354609520717
Iteration: 248
Loss: 0.001982803784172844
Iteration: 249
Loss: 0.001990394554687319
Iteration: 250
Loss: 0.0019771836059926835
Iteration: 251
Loss: 0.0019697841245359107
Iteration: 252
Loss: 0.0019763920852845468
Iteration: 253
Loss: 0.0019805209875037827
Iteration: 254
Loss: 0.001980979984373222
Iteration: 255
Loss: 0.0019801871730359614
Iteration: 256
Loss: 0.0019787008066397206
Iteration: 257
Loss: 0.001976545568107179
Iteration: 258
Loss: 0.0019658032911082294
Iteration: 259
Loss: 0.0019739439228066693
Iteration: 260
Loss: 0.0019699046503246567
Iteration: 261
Loss: 0.0019655418539143822
Iteration: 262
Loss: 0.0019649605936497266
Iteration: 263
Loss: 0.0019686317148928842
Iteration: 264
Loss: 0.001971434380709665
Iteration: 265
Loss: 0.001968336147888575
Iteration: 266
Loss: 0.0019734898441252703
Iteration: 267
Loss: 0.0019641490954974733
Iteration: 268
Loss: 0.001968882339834063
Iteration: 269
Loss: 0.001954408670074226
Iteration: 270
Loss: 0.0019657881326835464
Iteration: 271
Loss: 0.001955336584985532
Iteration: 272
Loss: 0.001960617412502567
Iteration: 273
Loss: 0.0019717511942288204
Iteration: 274
Loss: 0.00195652446968274
Iteration: 275
Loss: 0.0019735944411360925
Iteration: 276
Loss: 0.0019604634194646353
Iteration: 277
Loss: 0.001956623375277828
Iteration: 278
Loss: 0.0019505175669896015
Iteration: 279
Loss: 0.0019547567432631314
Iteration: 280
Loss: 0.0019514449098848817
Iteration: 281
Loss: 0.0019614257085148567
Iteration: 282
Loss: 0.001953807031379346
Iteration: 283
Loss: 0.0019508286258550706
Iteration: 284
Loss: 0.001954011790120951
Iteration: 285
Loss: 0.0019521028059247283
Iteration: 286
Loss: 0.0019500703381282495
Iteration: 287
Loss: 0.0019559415307378878
Iteration: 288
Loss: 0.0019570155281941463
Iteration: 289
Loss: 0.0019411000891669113
Iteration: 290
Loss: 0.00194668721401903
Iteration: 291
Loss: 0.0019422595915212124
Iteration: 292
Loss: 0.001954306516347936
Iteration: 293
Loss: 0.0019462217525061634
Iteration: 294
Loss: 0.0019498441790885947
Iteration: 295
Loss: 0.001945533367149808
Iteration: 296
Loss: 0.0019477177037639014
Iteration: 297
Loss: 0.001952551263614477
Iteration: 298
Loss: 0.0019433337930635905
Iteration: 299
Loss: 0.0019529587402364907
Iteration: 300
Loss: 0.001938156444395398
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.26415094339622647
accuracy: 0.9692105263157895
confusion: 42 124 110 7324
precision: 0.25301204819277107
recall: 0.27631578947368424
Processing fold: ../../../kg_constructor/output/salmonella/folds/fold_2
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 84531
Starting training...
Iteration: 1
Loss: 0.7018168082943669
Iteration: 2
Loss: 0.6948369594267857
Iteration: 3
Loss: 0.6756306732142413
Iteration: 4
Loss: 0.6375830092547853
Iteration: 5
Loss: 0.5833262896832124
Iteration: 6
Loss: 0.5235893921351727
Iteration: 7
Loss: 0.4708548464156963
Iteration: 8
Loss: 0.43027124802271527
Iteration: 9
Loss: 0.39720774432759226
Iteration: 10
Loss: 0.3714312057436248
Iteration: 11
Loss: 0.35111759695006006
Iteration: 12
Loss: 0.3329579502712061
Iteration: 13
Loss: 0.3161984314153224
Iteration: 14
Loss: 0.3001474153112482
Iteration: 15
Loss: 0.28586763033160456
Iteration: 16
Loss: 0.27139087996365113
Iteration: 17
Loss: 0.257168706368517
Iteration: 18
Loss: 0.2430798672599557
Iteration: 19
Loss: 0.22871364689903495
Iteration: 20
Loss: 0.2153879764639301
Iteration: 21
Loss: 0.2026233249976311
Iteration: 22
Loss: 0.18894480003250969
Iteration: 23
Loss: 0.17651348294299327
Iteration: 24
Loss: 0.164285996813833
Iteration: 25
Loss: 0.1521252445232721
Iteration: 26
Loss: 0.14145113601360793
Iteration: 27
Loss: 0.1311436433483053
Iteration: 28
Loss: 0.1209936128170402
Iteration: 29
Loss: 0.11158490116581504
Iteration: 30
Loss: 0.10279594959668172
Iteration: 31
Loss: 0.0949693281709412
Iteration: 32
Loss: 0.08760495097548873
Iteration: 33
Loss: 0.08069261733764484
Iteration: 34
Loss: 0.07462999610989182
Iteration: 35
Loss: 0.06879255212383506
Iteration: 36
Loss: 0.06356094933954286
Iteration: 37
Loss: 0.05890457810443125
Iteration: 38
Loss: 0.05422711634525546
Iteration: 39
Loss: 0.05040366404954298
Iteration: 40
Loss: 0.04654974724959444
Iteration: 41
Loss: 0.043294507458254146
Iteration: 42
Loss: 0.0404323679337531
Iteration: 43
Loss: 0.03754327243860857
Iteration: 44
Loss: 0.035103653132179634
Iteration: 45
Loss: 0.0328921075956321
Iteration: 46
Loss: 0.030576650352205758
Iteration: 47
Loss: 0.02874746412774663
Iteration: 48
Loss: 0.02690889132151633
Iteration: 49
Loss: 0.02533314094223358
Iteration: 50
Loss: 0.023938533378604018
Iteration: 51
Loss: 0.022416370702378542
Iteration: 52
Loss: 0.02132398950188984
Iteration: 53
Loss: 0.020126065216314645
Iteration: 54
Loss: 0.018996176551337594
Iteration: 55
Loss: 0.01809768109685845
Iteration: 56
Loss: 0.0172035776561609
Iteration: 57
Loss: 0.016330768135778696
Iteration: 58
Loss: 0.015583940160771212
Iteration: 59
Loss: 0.014841063764452198
Iteration: 60
Loss: 0.014156034699192754
Iteration: 61
Loss: 0.013538426892073066
Iteration: 62
Loss: 0.012967142897347609
Iteration: 63
Loss: 0.012435694271123704
Iteration: 64
Loss: 0.011963392607867718
Iteration: 65
Loss: 0.011394999162466437
Iteration: 66
Loss: 0.011001394693682224
Iteration: 67
Loss: 0.010551843585239517
Iteration: 68
Loss: 0.01019808480998984
Iteration: 69
Loss: 0.009803737571210037
Iteration: 70
Loss: 0.009434773851140046
Iteration: 71
Loss: 0.00914666943114113
Iteration: 72
Loss: 0.008790777599508012
Iteration: 73
Loss: 0.008503500567634164
Iteration: 74
Loss: 0.008281009530441629
Iteration: 75
Loss: 0.007983743098139026
Iteration: 76
Loss: 0.007757313296566775
Iteration: 77
Loss: 0.007482958635614242
Iteration: 78
Loss: 0.007314513150004693
Iteration: 79
Loss: 0.007085854354326004
Iteration: 80
Loss: 0.006900755274626944
Iteration: 81
Loss: 0.006715032036335748
Iteration: 82
Loss: 0.00651954621490505
Iteration: 83
Loss: 0.0063401531474089915
Iteration: 84
Loss: 0.006165851424965594
Iteration: 85
Loss: 0.006022232452430475
Iteration: 86
Loss: 0.005902220374318185
Iteration: 87
Loss: 0.005730022411839461
Iteration: 88
Loss: 0.005606075018690324
Iteration: 89
Loss: 0.00548968613607648
Iteration: 90
Loss: 0.005341867910537087
Iteration: 91
Loss: 0.005236722069022096
Iteration: 92
Loss: 0.005116164431343844
Iteration: 93
Loss: 0.005001524147106174
Iteration: 94
Loss: 0.004923452623188496
Iteration: 95
Loss: 0.004808059163437581
Iteration: 96
Loss: 0.004719295452728316
Iteration: 97
Loss: 0.0046241031988397425
Iteration: 98
Loss: 0.0045401132205662165
Iteration: 99
Loss: 0.004464340146148094
Iteration: 100
Loss: 0.004341475817915282
Iteration: 101
Loss: 0.004277070423726131
Iteration: 102
Loss: 0.004192184297551895
Iteration: 103
Loss: 0.0041371499487187395
Iteration: 104
Loss: 0.004044585671551802
Iteration: 105
Loss: 0.003979199177895983
Iteration: 106
Loss: 0.003919819611366148
Iteration: 107
Loss: 0.00383192855278375
Iteration: 108
Loss: 0.0037663357970477256
Iteration: 109
Loss: 0.003714538849085385
Iteration: 110
Loss: 0.0036799549556302802
Iteration: 111
Loss: 0.003589754722966456
Iteration: 112
Loss: 0.003523345116847827
Iteration: 113
Loss: 0.003495963152360033
Iteration: 114
Loss: 0.003430192761215163
Iteration: 115
Loss: 0.0033579933296107216
Iteration: 116
Loss: 0.0033082362569086714
Iteration: 117
Loss: 0.0032840419853864995
Iteration: 118
Loss: 0.00323994832550302
Iteration: 119
Loss: 0.003186578937104823
Iteration: 120
Loss: 0.003149878590103285
Iteration: 121
Loss: 0.003113442388029746
Iteration: 122
Loss: 0.0030529074756037674
Iteration: 123
Loss: 0.003034984894142843
Iteration: 124
Loss: 0.0029858396371343623
Iteration: 125
Loss: 0.002960298250624795
Iteration: 126
Loss: 0.0029152641187847397
Iteration: 127
Loss: 0.002887758098680664
Iteration: 128
Loss: 0.0028581287493400368
Iteration: 129
Loss: 0.0028432545878774957
Iteration: 130
Loss: 0.0027917363381956095
Iteration: 131
Loss: 0.002772069067613762
Iteration: 132
Loss: 0.002739611757275315
Iteration: 133
Loss: 0.002717065765740879
Iteration: 134
Loss: 0.002686711128824103
Iteration: 135
Loss: 0.0026725848165145257
Iteration: 136
Loss: 0.002647427623562607
Iteration: 137
Loss: 0.0026226422812697697
Iteration: 138
Loss: 0.0025891351605547065
Iteration: 139
Loss: 0.00255781262078219
Iteration: 140
Loss: 0.002548534362749369
Iteration: 141
Loss: 0.002546260668033803
Iteration: 142
Loss: 0.002508576990990543
Iteration: 143
Loss: 0.0024873164775608865
Iteration: 144
Loss: 0.002481150098811881
Iteration: 145
Loss: 0.0024486445839068402
Iteration: 146
Loss: 0.002445280724377544
Iteration: 147
Loss: 0.002443401168824898
Iteration: 148
Loss: 0.0024188243536807505
Iteration: 149
Loss: 0.002393393162178037
Iteration: 150
Loss: 0.0023807989243693926
Iteration: 151
Loss: 0.002363291238492102
Iteration: 152
Loss: 0.002350968956935829
Iteration: 153
Loss: 0.002343706226114322
Iteration: 154
Loss: 0.0023290121950853017
Iteration: 155
Loss: 0.0023200571864705395
Iteration: 156
Loss: 0.0023109468453230315
Iteration: 157
Loss: 0.002295325258689254
Iteration: 158
Loss: 0.002292788751354372
Iteration: 159
Loss: 0.0022859819446126987
Iteration: 160
Loss: 0.0022683769470241704
Iteration: 161
Loss: 0.002257485267487757
Iteration: 162
Loss: 0.002253199111164352
Iteration: 163
Loss: 0.0022315799422493138
Iteration: 164
Loss: 0.002228959108541869
Iteration: 165
Loss: 0.0022274707314661807
Iteration: 166
Loss: 0.002216454098047114
Iteration: 167
Loss: 0.0022080007216168773
Iteration: 168
Loss: 0.002189723461475453
Iteration: 169
Loss: 0.002198820981128072
Iteration: 170
Loss: 0.0021880723817710892
Iteration: 171
Loss: 0.0021935276195613872
Iteration: 172
Loss: 0.0021785757630879495
Iteration: 173
Loss: 0.002174307436992725
Iteration: 174
Loss: 0.002168549016336508
Iteration: 175
Loss: 0.0021704255019740007
Iteration: 176
Loss: 0.0021629076836239777
Iteration: 177
Loss: 0.002154697465744835
Iteration: 178
Loss: 0.0021490744447312604
Iteration: 179
Loss: 0.0021475280813274926
Iteration: 180
Loss: 0.0021401778421146268
Iteration: 181
Loss: 0.002131148915519409
Iteration: 182
Loss: 0.002125812155470528
Iteration: 183
Loss: 0.002111891127927344
Iteration: 184
Loss: 0.0021140062865329747
Iteration: 185
Loss: 0.0021184122270364084
Iteration: 186
Loss: 0.0021206625021511208
Iteration: 187
Loss: 0.002105300311849993
Iteration: 188
Loss: 0.002106159883781256
Iteration: 189
Loss: 0.002084017167856664
Iteration: 190
Loss: 0.002102854217540611
Iteration: 191
Loss: 0.002095217247390085
Iteration: 192
Loss: 0.002083965150328974
Iteration: 193
Loss: 0.0020794048661044165
Iteration: 194
Loss: 0.0020937848172185047
Iteration: 195
Loss: 0.0020842501968375325
Iteration: 196
Loss: 0.002074542530110957
Iteration: 197
Loss: 0.002071256704777939
Iteration: 198
Loss: 0.002075092858393435
Iteration: 199
Loss: 0.0020667529811530753
Iteration: 200
Loss: 0.0020618042937353438
Iteration: 201
Loss: 0.0020589789987921162
Iteration: 202
Loss: 0.002068168292523442
Iteration: 203
Loss: 0.002056947637659808
Iteration: 204
Loss: 0.002053371501258678
Iteration: 205
Loss: 0.002054173827033352
Iteration: 206
Loss: 0.002038660367237933
Iteration: 207
Loss: 0.002049371762557622
Iteration: 208
Loss: 0.0020414290354690617
Iteration: 209
Loss: 0.0020365858663241435
Iteration: 210
Loss: 0.002041663412568102
Iteration: 211
Loss: 0.002034742633789134
Iteration: 212
Loss: 0.002038982356208618
Iteration: 213
Loss: 0.002051863664140304
Iteration: 214
Loss: 0.002034041323457603
Iteration: 215
Loss: 0.0020358793446104283
Iteration: 216
Loss: 0.0020431410966808964
Iteration: 217
Loss: 0.0020328364694307062
Iteration: 218
Loss: 0.0020178521373941573
Iteration: 219
Loss: 0.0020189617909950977
Iteration: 220
Loss: 0.0020175893664544013
Iteration: 221
Loss: 0.002022851239832371
Iteration: 222
Loss: 0.0020328819705748265
Iteration: 223
Loss: 0.0020162634462417093
Iteration: 224
Loss: 0.0020123430567989013
Iteration: 225
Loss: 0.0020132547612359496
Iteration: 226
Loss: 0.0020195467032515157
Iteration: 227
Loss: 0.0020184081887113458
Iteration: 228
Loss: 0.0020101633566765136
Iteration: 229
Loss: 0.0020024415326914116
Iteration: 230
Loss: 0.002006036130263022
Iteration: 231
Loss: 0.0019993125532903238
Iteration: 232
Loss: 0.001998045054885248
Iteration: 233
Loss: 0.0019968208399842734
Iteration: 234
Loss: 0.001996686157517503
Iteration: 235
Loss: 0.002003176646583058
Iteration: 236
Loss: 0.001990123003732735
Iteration: 237
Loss: 0.001985743018097164
Iteration: 238
Loss: 0.0019891656874851497
Iteration: 239
Loss: 0.0019957888469008017
Iteration: 240
Loss: 0.0019883007518257255
Iteration: 241
Loss: 0.0019994308298200737
Iteration: 242
Loss: 0.0019910816882196584
Iteration: 243
Loss: 0.001988548658972169
Iteration: 244
Loss: 0.0019893561645100513
Iteration: 245
Loss: 0.0019881928779391777
Iteration: 246
Loss: 0.0019825100841253628
Iteration: 247
Loss: 0.0019827176425835967
Iteration: 248
Loss: 0.001982967120618272
Iteration: 249
Loss: 0.0019797493278244397
Iteration: 250
Loss: 0.0019892552942267537
Iteration: 251
Loss: 0.001978059606856586
Iteration: 252
Loss: 0.001972159133637669
Iteration: 253
Loss: 0.0019835548225307356
Iteration: 254
Loss: 0.0019859926341055534
Iteration: 255
Loss: 0.0019743163584168126
Iteration: 256
Loss: 0.0019791040707517553
Iteration: 257
Loss: 0.001975596398057669
Iteration: 258
Loss: 0.001985505900207769
Iteration: 259
Loss: 0.001972082490676347
Iteration: 260
Loss: 0.0019657251016915213
Iteration: 261
Loss: 0.001969376879307315
Iteration: 262
Loss: 0.0019676068346418532
Iteration: 263
Loss: 0.001965314565214938
Iteration: 264
Loss: 0.001973683059066074
Iteration: 265
Loss: 0.001966389804434629
Iteration: 266
Loss: 0.001975885345198122
Iteration: 267
Loss: 0.001967640195536301
Iteration: 268
Loss: 0.0019635962908742606
Iteration: 269
Loss: 0.0019568779310326517
Iteration: 270
Loss: 0.001961370353747941
Iteration: 271
Loss: 0.0019645762132526733
Iteration: 272
Loss: 0.001972637109079019
Iteration: 273
Loss: 0.001965512865062021
Iteration: 274
Loss: 0.001957592173525489
Iteration: 275
Loss: 0.0019595302658638467
Iteration: 276
Loss: 0.0019673733514975066
Iteration: 277
Loss: 0.001965231226216772
Iteration: 278
Loss: 0.001962119385738064
Iteration: 279
Loss: 0.001961554278458618
Iteration: 280
Loss: 0.0019468627036491662
Iteration: 281
Loss: 0.0019506282104110276
Iteration: 282
Loss: 0.0019580091947666656
Iteration: 283
Loss: 0.001960436664061782
Iteration: 284
Loss: 0.001958298673680811
Iteration: 285
Loss: 0.001953308448988806
Iteration: 286
Loss: 0.001955276209987516
Iteration: 287
Loss: 0.001954666900816431
Iteration: 288
Loss: 0.0019526922224480429
Iteration: 289
Loss: 0.0019504299105639444
Iteration: 290
Loss: 0.001944793189910275
Iteration: 291
Loss: 0.0019539690556372572
Iteration: 292
Loss: 0.0019512669611607253
Iteration: 293
Loss: 0.0019503272466223549
Iteration: 294
Loss: 0.0019403827350793614
Iteration: 295
Loss: 0.0019456391973007056
Iteration: 296
Loss: 0.0019426994086070746
Iteration: 297
Loss: 0.0019430025368583975
Iteration: 298
Loss: 0.0019500274527358052
Iteration: 299
Loss: 0.0019405049611066962
Iteration: 300
Loss: 0.0019497285844229254
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.2740524781341108
accuracy: 0.9672368421052632
confusion: 47 144 105 7304
precision: 0.24607329842931938
recall: 0.3092105263157895
Processing fold: ../../../kg_constructor/output/salmonella/folds/fold_3
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 84531
Starting training...
Iteration: 1
Loss: 0.7018730920038105
Iteration: 2
Loss: 0.6948114523181209
Iteration: 3
Loss: 0.6758480300138026
Iteration: 4
Loss: 0.6378615454391197
Iteration: 5
Loss: 0.5820293581044232
Iteration: 6
Loss: 0.5227339017538377
Iteration: 7
Loss: 0.4710986551679211
Iteration: 8
Loss: 0.43042521197118877
Iteration: 9
Loss: 0.3978213575887091
Iteration: 10
Loss: 0.3731065734669014
Iteration: 11
Loss: 0.3519155438299532
Iteration: 12
Loss: 0.334405654742394
Iteration: 13
Loss: 0.31733649637964034
Iteration: 14
Loss: 0.3023033348130591
Iteration: 15
Loss: 0.2875595732971474
Iteration: 16
Loss: 0.2732959985733032
Iteration: 17
Loss: 0.25898507531778314
Iteration: 18
Loss: 0.24506591536380626
Iteration: 19
Loss: 0.2306658136255947
Iteration: 20
Loss: 0.217676877423569
Iteration: 21
Loss: 0.2041732394768868
Iteration: 22
Loss: 0.19066662810466908
Iteration: 23
Loss: 0.17815080321865318
Iteration: 24
Loss: 0.16505558236881537
Iteration: 25
Loss: 0.1541613976896545
Iteration: 26
Loss: 0.14282835100168062
Iteration: 27
Loss: 0.13182609215562727
Iteration: 28
Loss: 0.12228703655210542
Iteration: 29
Loss: 0.11242022586089594
Iteration: 30
Loss: 0.10402477578616437
Iteration: 31
Loss: 0.09585757479991441
Iteration: 32
Loss: 0.0882079867117199
Iteration: 33
Loss: 0.0814542360143897
Iteration: 34
Loss: 0.07496934853218219
Iteration: 35
Loss: 0.06913824048307207
Iteration: 36
Loss: 0.06390227382013827
Iteration: 37
Loss: 0.058873766320355146
Iteration: 38
Loss: 0.054737379061587065
Iteration: 39
Loss: 0.05079205191613716
Iteration: 40
Loss: 0.0472229653854429
Iteration: 41
Loss: 0.04378630069118959
Iteration: 42
Loss: 0.04052603833469344
Iteration: 43
Loss: 0.03784629233457424
Iteration: 44
Loss: 0.035298407905631594
Iteration: 45
Loss: 0.03282515502270357
Iteration: 46
Loss: 0.0307443085597989
Iteration: 47
Loss: 0.028909688539527082
Iteration: 48
Loss: 0.027305929289187913
Iteration: 49
Loss: 0.025508141352070704
Iteration: 50
Loss: 0.024062136418105645
Iteration: 51
Loss: 0.022589272164084292
Iteration: 52
Loss: 0.021420974997274668
Iteration: 53
Loss: 0.02036346805592378
Iteration: 54
Loss: 0.01912274043777107
Iteration: 55
Loss: 0.01828388655903163
Iteration: 56
Loss: 0.01737823755836781
Iteration: 57
Loss: 0.01648302625772762
Iteration: 58
Loss: 0.015785343450620586
Iteration: 59
Loss: 0.014982812417050203
Iteration: 60
Loss: 0.0143523979012245
Iteration: 61
Loss: 0.013658081261832038
Iteration: 62
Loss: 0.013093553736437986
Iteration: 63
Loss: 0.012572588525528524
Iteration: 64
Loss: 0.012070003666995484
Iteration: 65
Loss: 0.0115109605277762
Iteration: 66
Loss: 0.01113176353873662
Iteration: 67
Loss: 0.010718590591424777
Iteration: 68
Loss: 0.010342811498744988
Iteration: 69
Loss: 0.009906910727789372
Iteration: 70
Loss: 0.009578947518250825
Iteration: 71
Loss: 0.009278615729676353
Iteration: 72
Loss: 0.008902045645187666
Iteration: 73
Loss: 0.008574136090177445
Iteration: 74
Loss: 0.0083298890069587
Iteration: 75
Loss: 0.008056794172497812
Iteration: 76
Loss: 0.007793735310343312
Iteration: 77
Loss: 0.0075947159935754755
Iteration: 78
Loss: 0.0073850448013363795
Iteration: 79
Loss: 0.007099504996505048
Iteration: 80
Loss: 0.006929009629666437
Iteration: 81
Loss: 0.006726220353609986
Iteration: 82
Loss: 0.006503709919612717
Iteration: 83
Loss: 0.0063602344282431365
Iteration: 84
Loss: 0.006202836061057485
Iteration: 85
Loss: 0.006066258197627318
Iteration: 86
Loss: 0.0058885491683067365
Iteration: 87
Loss: 0.005733931879800411
Iteration: 88
Loss: 0.005611458632313175
Iteration: 89
Loss: 0.005487383119072075
Iteration: 90
Loss: 0.005361494953157725
Iteration: 91
Loss: 0.0052514793523759755
Iteration: 92
Loss: 0.005132607540782587
Iteration: 93
Loss: 0.005033281689256798
Iteration: 94
Loss: 0.004909639637687324
Iteration: 95
Loss: 0.004803130955055908
Iteration: 96
Loss: 0.004723222012183181
Iteration: 97
Loss: 0.004603675307912959
Iteration: 98
Loss: 0.004516666553868556
Iteration: 99
Loss: 0.004434901002196618
Iteration: 100
Loss: 0.004340440859834169
Iteration: 101
Loss: 0.004273735104634622
Iteration: 102
Loss: 0.004187474380833683
Iteration: 103
Loss: 0.004117056026042979
Iteration: 104
Loss: 0.004068670210853955
Iteration: 105
Loss: 0.0039743608680128314
Iteration: 106
Loss: 0.0038903544766161545
Iteration: 107
Loss: 0.003831660915396096
Iteration: 108
Loss: 0.0037679070388369354
Iteration: 109
Loss: 0.0037069313895003297
Iteration: 110
Loss: 0.0036374281252515906
Iteration: 111
Loss: 0.0035974051786103736
Iteration: 112
Loss: 0.0035362571167449155
Iteration: 113
Loss: 0.003467558026543738
Iteration: 114
Loss: 0.0034242824645552
Iteration: 115
Loss: 0.0033763106308739494
Iteration: 116
Loss: 0.0033214300882393197
Iteration: 117
Loss: 0.00325918012517102
Iteration: 118
Loss: 0.0032255838331938895
Iteration: 119
Loss: 0.003177235141557492
Iteration: 120
Loss: 0.00314189400523901
Iteration: 121
Loss: 0.0030940218201987906
Iteration: 122
Loss: 0.0030426873371327
Iteration: 123
Loss: 0.0030006417993134185
Iteration: 124
Loss: 0.0029776881443175636
Iteration: 125
Loss: 0.0029499488192851896
Iteration: 126
Loss: 0.0029067986782778193
Iteration: 127
Loss: 0.0028794043047790543
Iteration: 128
Loss: 0.0028504318310476747
Iteration: 129
Loss: 0.0028072258423048036
Iteration: 130
Loss: 0.0027891082177513545
Iteration: 131
Loss: 0.0027696938213696816
Iteration: 132
Loss: 0.002716451176200752
Iteration: 133
Loss: 0.0026899413953040854
Iteration: 134
Loss: 0.0026695920976545708
Iteration: 135
Loss: 0.0026471963336254342
Iteration: 136
Loss: 0.00262079374105842
Iteration: 137
Loss: 0.00260421335639685
Iteration: 138
Loss: 0.0025806057577331862
Iteration: 139
Loss: 0.0025703610082007484
Iteration: 140
Loss: 0.0025436567361064163
Iteration: 141
Loss: 0.002514289536823829
Iteration: 142
Loss: 0.002496203965288989
Iteration: 143
Loss: 0.0024820296237544146
Iteration: 144
Loss: 0.0024740776438036084
Iteration: 145
Loss: 0.0024489666088081804
Iteration: 146
Loss: 0.002427097404214703
Iteration: 147
Loss: 0.0024097413399521215
Iteration: 148
Loss: 0.002399790848110929
Iteration: 149
Loss: 0.002392970422213828
Iteration: 150
Loss: 0.002371559747391277
Iteration: 151
Loss: 0.0023548262090318734
Iteration: 152
Loss: 0.0023449969069779288
Iteration: 153
Loss: 0.002331024320382211
Iteration: 154
Loss: 0.0023143317602160904
Iteration: 155
Loss: 0.002319670759463384
Iteration: 156
Loss: 0.0023116675107797357
Iteration: 157
Loss: 0.00228913953056398
Iteration: 158
Loss: 0.0022840618276623666
Iteration: 159
Loss: 0.0022810124016056457
Iteration: 160
Loss: 0.0022646909361176285
Iteration: 161
Loss: 0.00224979479946656
Iteration: 162
Loss: 0.002250365889918657
Iteration: 163
Loss: 0.0022430028938860806
Iteration: 164
Loss: 0.0022350387564388876
Iteration: 165
Loss: 0.0022308891955303187
Iteration: 166
Loss: 0.00220864766024421
Iteration: 167
Loss: 0.0021986509960742646
Iteration: 168
Loss: 0.0021988480052922243
Iteration: 169
Loss: 0.002192565823479751
Iteration: 170
Loss: 0.0021873355101518057
Iteration: 171
Loss: 0.0021755836263046037
Iteration: 172
Loss: 0.002183585329970092
Iteration: 173
Loss: 0.002165956056193897
Iteration: 174
Loss: 0.0021641682330403983
Iteration: 175
Loss: 0.0021604396558056274
Iteration: 176
Loss: 0.0021434038774006898
Iteration: 177
Loss: 0.0021554606860319588
Iteration: 178
Loss: 0.002138622271839851
Iteration: 179
Loss: 0.002127361337274865
Iteration: 180
Loss: 0.0021229116950345077
Iteration: 181
Loss: 0.0021332382270491416
Iteration: 182
Loss: 0.00211778784279976
Iteration: 183
Loss: 0.0021092614308459525
Iteration: 184
Loss: 0.0021089725153244756
Iteration: 185
Loss: 0.002106304222970833
Iteration: 186
Loss: 0.0021103745941532617
Iteration: 187
Loss: 0.0021027860427160323
Iteration: 188
Loss: 0.002092344946526911
Iteration: 189
Loss: 0.0020892600712860807
Iteration: 190
Loss: 0.0020764052499959496
Iteration: 191
Loss: 0.002079927603955622
Iteration: 192
Loss: 0.0020875154137841344
Iteration: 193
Loss: 0.0020811228271297834
Iteration: 194
Loss: 0.0020793080085566564
Iteration: 195
Loss: 0.002075519726071277
Iteration: 196
Loss: 0.0020669364287046557
Iteration: 197
Loss: 0.00205947237411215
Iteration: 198
Loss: 0.0020638082297173917
Iteration: 199
Loss: 0.002056540040310794
Iteration: 200
Loss: 0.002056451123437764
Iteration: 201
Loss: 0.0020687542712016973
Iteration: 202
Loss: 0.002063040992383052
Iteration: 203
Loss: 0.0020493316984401995
Iteration: 204
Loss: 0.002044301909785488
Iteration: 205
Loss: 0.0020429878136327055
Iteration: 206
Loss: 0.002048468086466101
Iteration: 207
Loss: 0.0020416873064536003
Iteration: 208
Loss: 0.0020318552722326582
Iteration: 209
Loss: 0.002049586655181131
Iteration: 210
Loss: 0.0020404587151988604
Iteration: 211
Loss: 0.0020235582272073735
Iteration: 212
Loss: 0.002040614963244693
Iteration: 213
Loss: 0.0020305754007472667
Iteration: 214
Loss: 0.002032672880033468
Iteration: 215
Loss: 0.0020215576715813376
Iteration: 216
Loss: 0.0020066113958572165
Iteration: 217
Loss: 0.002018857699887896
Iteration: 218
Loss: 0.0020193254264692464
Iteration: 219
Loss: 0.002014227717378993
Iteration: 220
Loss: 0.0020167544961297587
Iteration: 221
Loss: 0.0020118470556666087
Iteration: 222
Loss: 0.0020139967594410718
Iteration: 223
Loss: 0.002009033977146042
Iteration: 224
Loss: 0.002012193202972412
Iteration: 225
Loss: 0.002003874082152766
Iteration: 226
Loss: 0.0019992125841478505
Iteration: 227
Loss: 0.002009906620649552
Iteration: 228
Loss: 0.002010060515956102
Iteration: 229
Loss: 0.002005894840867431
Iteration: 230
Loss: 0.002011915884429106
Iteration: 231
Loss: 0.0019990111152170067
Iteration: 232
Loss: 0.0019850879778257674
Iteration: 233
Loss: 0.001995079473730315
Iteration: 234
Loss: 0.00199163149766348
Iteration: 235
Loss: 0.0019995305330757006
Iteration: 236
Loss: 0.0019947102531951704
Iteration: 237
Loss: 0.001984590940453388
Iteration: 238
Loss: 0.001984437783881102
Iteration: 239
Loss: 0.0019892655229655867
Iteration: 240
Loss: 0.0019940410864274995
Iteration: 241
Loss: 0.001988818192061175
Iteration: 242
Loss: 0.0019845180213451385
Iteration: 243
Loss: 0.0019846137362980734
Iteration: 244
Loss: 0.001980516629833958
Iteration: 245
Loss: 0.0019930093347980283
Iteration: 246
Loss: 0.0019910411627526267
Iteration: 247
Loss: 0.001978352469001195
Iteration: 248
Loss: 0.0019915817868838338
Iteration: 249
Loss: 0.0019769348997484754
Iteration: 250
Loss: 0.0019757300414099
Iteration: 251
Loss: 0.001988002744411337
Iteration: 252
Loss: 0.0019790427256260574
Iteration: 253
Loss: 0.0019700095577763373
Iteration: 254
Loss: 0.001965556010592407
Iteration: 255
Loss: 0.001967577955018683
Iteration: 256
Loss: 0.001969477870317613
Iteration: 257
Loss: 0.001967818877245817
Iteration: 258
Loss: 0.001980347974214381
Iteration: 259
Loss: 0.001979079603413005
Iteration: 260
Loss: 0.0019736895567657037
Iteration: 261
Loss: 0.0019589906319814883
Iteration: 262
Loss: 0.0019709031531633236
Iteration: 263
Loss: 0.0019775571108991167
Iteration: 264
Loss: 0.00196401282777021
Iteration: 265
Loss: 0.0019651567204108026
Iteration: 266
Loss: 0.0019617696424923193
Iteration: 267
Loss: 0.0019711594077195097
Iteration: 268
Loss: 0.0019716094693536927
Iteration: 269
Loss: 0.0019696350856166747
Iteration: 270
Loss: 0.001954613660209249
Iteration: 271
Loss: 0.0019619915530913407
Iteration: 272
Loss: 0.001955196653768696
Iteration: 273
Loss: 0.001962031128551857
Iteration: 274
Loss: 0.0019606555018711973
Iteration: 275
Loss: 0.0019527380584658663
Iteration: 276
Loss: 0.001943983797294398
Iteration: 277
Loss: 0.001958919552522769
Iteration: 278
Loss: 0.0019591191444362018
Iteration: 279
Loss: 0.001957103732202984
Iteration: 280
Loss: 0.001961710072340973
Iteration: 281
Loss: 0.0019542196460850076
Iteration: 282
Loss: 0.0019530378178589875
Iteration: 283
Loss: 0.001947830025865524
Iteration: 284
Loss: 0.0019483712435711498
Iteration: 285
Loss: 0.001954102062298284
Iteration: 286
Loss: 0.001948154956838231
Iteration: 287
Loss: 0.0019481685300024203
Iteration: 288
Loss: 0.0019429671552239193
Iteration: 289
Loss: 0.0019476672671850264
Iteration: 290
Loss: 0.001944084690573315
Iteration: 291
Loss: 0.0019512575300824312
Iteration: 292
Loss: 0.001955502989908887
Iteration: 293
Loss: 0.001951051163084713
Iteration: 294
Loss: 0.0019421202323204022
Iteration: 295
Loss: 0.001943345345487749
Iteration: 296
Loss: 0.0019386937715297127
Iteration: 297
Loss: 0.0019433240831634145
Iteration: 298
Loss: 0.0019392950106757107
Iteration: 299
Loss: 0.0019349303214385369
Iteration: 300
Loss: 0.0019408600220893635
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.1784037558685446
accuracy: 0.930921052631579
confusion: 57 430 95 7018
precision: 0.11704312114989733
recall: 0.375
Processing fold: ../../../kg_constructor/output/salmonella/folds/fold_4
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 84531
Starting training...
Iteration: 1
Loss: 0.7018454052783825
Iteration: 2
Loss: 0.6942352144806473
Iteration: 3
Loss: 0.6752126761424688
Iteration: 4
Loss: 0.635519023294802
Iteration: 5
Loss: 0.5798941681414475
Iteration: 6
Loss: 0.5212445560796761
Iteration: 7
Loss: 0.46937807621779265
Iteration: 8
Loss: 0.42802932380158226
Iteration: 9
Loss: 0.3988266314989255
Iteration: 10
Loss: 0.37322718493732404
Iteration: 11
Loss: 0.3517814241809609
Iteration: 12
Loss: 0.3348614043659634
Iteration: 13
Loss: 0.3187288222489534
Iteration: 14
Loss: 0.30301594071918064
Iteration: 15
Loss: 0.28865594849174403
Iteration: 16
Loss: 0.274364506020958
Iteration: 17
Loss: 0.2600970290325306
Iteration: 18
Loss: 0.24564440971539345
Iteration: 19
Loss: 0.23216506949177496
Iteration: 20
Loss: 0.21867405945136223
Iteration: 21
Loss: 0.20498256164568443
Iteration: 22
Loss: 0.19219280926533688
Iteration: 23
Loss: 0.17906447877118617
Iteration: 24
Loss: 0.1664536527277511
Iteration: 25
Loss: 0.15492647941465731
Iteration: 26
Loss: 0.14328279870527763
Iteration: 27
Loss: 0.1326323992308275
Iteration: 28
Loss: 0.1227212510543105
Iteration: 29
Loss: 0.11280044002665414
Iteration: 30
Loss: 0.10416759109055554
Iteration: 31
Loss: 0.09604561843989808
Iteration: 32
Loss: 0.08885795531081564
Iteration: 33
Loss: 0.08171378204851974
Iteration: 34
Loss: 0.07510015074117685
Iteration: 35
Loss: 0.06960906989780473
Iteration: 36
Loss: 0.06438599145154894
Iteration: 37
Loss: 0.059091114795502324
Iteration: 38
Loss: 0.05453795291207455
Iteration: 39
Loss: 0.05093649563230114
Iteration: 40
Loss: 0.047229918432824404
Iteration: 41
Loss: 0.04369886237897991
Iteration: 42
Loss: 0.04053459886783435
Iteration: 43
Loss: 0.037966080101919764
Iteration: 44
Loss: 0.03538202756532916
Iteration: 45
Loss: 0.032937101720843784
Iteration: 46
Loss: 0.031038022864563964
Iteration: 47
Loss: 0.028958042048745684
Iteration: 48
Loss: 0.027114063455366794
Iteration: 49
Loss: 0.025602063118123713
Iteration: 50
Loss: 0.02416530246903867
Iteration: 51
Loss: 0.02263330675109669
Iteration: 52
Loss: 0.021532147869835667
Iteration: 53
Loss: 0.02020114517690223
Iteration: 54
Loss: 0.019088694526825423
Iteration: 55
Loss: 0.01817988388148355
Iteration: 56
Loss: 0.01738657547091996
Iteration: 57
Loss: 0.016513110655877326
Iteration: 58
Loss: 0.015677724930423277
Iteration: 59
Loss: 0.014936733209056618
Iteration: 60
Loss: 0.0142100893397942
Iteration: 61
Loss: 0.013691667007443346
Iteration: 62
Loss: 0.013098978150038071
Iteration: 63
Loss: 0.012486845346880548
Iteration: 64
Loss: 0.012007685458678523
Iteration: 65
Loss: 0.011572272516787052
Iteration: 66
Loss: 0.011105837691345332
Iteration: 67
Loss: 0.010671345068624726
Iteration: 68
Loss: 0.01029540876639478
Iteration: 69
Loss: 0.009900641489636016
Iteration: 70
Loss: 0.009611257144974338
Iteration: 71
Loss: 0.009237652747995324
Iteration: 72
Loss: 0.008913239958569592
Iteration: 73
Loss: 0.00867245267547759
Iteration: 74
Loss: 0.0083502300520554
Iteration: 75
Loss: 0.008085414313101842
Iteration: 76
Loss: 0.007832676666662281
Iteration: 77
Loss: 0.007595358767121294
Iteration: 78
Loss: 0.007343065154589253
Iteration: 79
Loss: 0.007142472799499462
Iteration: 80
Loss: 0.006931063535128847
Iteration: 81
Loss: 0.006770773026172394
Iteration: 82
Loss: 0.00657726965131767
Iteration: 83
Loss: 0.006407466906778238
Iteration: 84
Loss: 0.00621306206341143
Iteration: 85
Loss: 0.006064303259560724
Iteration: 86
Loss: 0.005948042870717652
Iteration: 87
Loss: 0.005781871410385694
Iteration: 88
Loss: 0.005650785878116702
Iteration: 89
Loss: 0.005500740193428449
Iteration: 90
Loss: 0.0053850272900344415
Iteration: 91
Loss: 0.005270157590002557
Iteration: 92
Loss: 0.005165610433682615
Iteration: 93
Loss: 0.005039074705384764
Iteration: 94
Loss: 0.004920214880257845
Iteration: 95
Loss: 0.004825077864720866
Iteration: 96
Loss: 0.004732065348322561
Iteration: 97
Loss: 0.0046398851008694845
Iteration: 98
Loss: 0.004549931861276244
Iteration: 99
Loss: 0.004436324888058466
Iteration: 100
Loss: 0.004372855904224662
Iteration: 101
Loss: 0.004283091559270282
Iteration: 102
Loss: 0.004195540809980881
Iteration: 103
Loss: 0.004132274472166175
Iteration: 104
Loss: 0.004042295808041537
Iteration: 105
Loss: 0.0039823552288896875
Iteration: 106
Loss: 0.003909366156468972
Iteration: 107
Loss: 0.0038566652650910394
Iteration: 108
Loss: 0.0037633723314897515
Iteration: 109
Loss: 0.0037119297891349337
Iteration: 110
Loss: 0.0036429087746382496
Iteration: 111
Loss: 0.0036160143402715525
Iteration: 112
Loss: 0.003558599999473419
Iteration: 113
Loss: 0.003463374229469969
Iteration: 114
Loss: 0.003428154484150402
Iteration: 115
Loss: 0.003372915408194617
Iteration: 116
Loss: 0.003325427594928094
Iteration: 117
Loss: 0.0032718559399385143
Iteration: 118
Loss: 0.0032315017816461162
Iteration: 119
Loss: 0.0031816402283485657
Iteration: 120
Loss: 0.0031310329184993917
Iteration: 121
Loss: 0.0030986583492730135
Iteration: 122
Loss: 0.003068230899602727
Iteration: 123
Loss: 0.003027482487169313
Iteration: 124
Loss: 0.0029924798291176558
Iteration: 125
Loss: 0.0029414363326159894
Iteration: 126
Loss: 0.0029397095168400326
Iteration: 127
Loss: 0.0028876176471879455
Iteration: 128
Loss: 0.0028617472997234192
Iteration: 129
Loss: 0.002826468656874365
Iteration: 130
Loss: 0.0027955034661486192
Iteration: 131
Loss: 0.0027637033504836353
Iteration: 132
Loss: 0.0027407348115733006
Iteration: 133
Loss: 0.0026938673252538767
Iteration: 134
Loss: 0.0026961703020159477
Iteration: 135
Loss: 0.00266767108100064
Iteration: 136
Loss: 0.0026305905416791823
Iteration: 137
Loss: 0.0026416209624091414
Iteration: 138
Loss: 0.0025841775062827414
Iteration: 139
Loss: 0.0025636203943487303
Iteration: 140
Loss: 0.0025459179844431303
Iteration: 141
Loss: 0.0025304524532668753
Iteration: 142
Loss: 0.0025168902263689188
Iteration: 143
Loss: 0.002495599876123446
Iteration: 144
Loss: 0.0024701777851370015
Iteration: 145
Loss: 0.002463461046486541
Iteration: 146
Loss: 0.002440198195270366
Iteration: 147
Loss: 0.002429339543191923
Iteration: 148
Loss: 0.0023919942898008926
Iteration: 149
Loss: 0.002403346013944642
Iteration: 150
Loss: 0.00238591969806563
Iteration: 151
Loss: 0.002364957883356162
Iteration: 152
Loss: 0.0023610434713371006
Iteration: 153
Loss: 0.002335316530302351
Iteration: 154
Loss: 0.0023455433260051554
Iteration: 155
Loss: 0.002324461137852919
Iteration: 156
Loss: 0.0023146180829815106
Iteration: 157
Loss: 0.0022853437845629676
Iteration: 158
Loss: 0.002287581068590099
Iteration: 159
Loss: 0.0022830164813103133
Iteration: 160
Loss: 0.0022665281136186773
Iteration: 161
Loss: 0.002253608467678229
Iteration: 162
Loss: 0.00225721922542118
Iteration: 163
Loss: 0.002231993393420621
Iteration: 164
Loss: 0.002235228172790857
Iteration: 165
Loss: 0.002245567195738355
Iteration: 166
Loss: 0.0021958257614945373
Iteration: 167
Loss: 0.0022078434703871608
Iteration: 168
Loss: 0.002209178358891312
Iteration: 169
Loss: 0.0021996025131141516
Iteration: 170
Loss: 0.002194246285836454
Iteration: 171
Loss: 0.0021727897778705315
Iteration: 172
Loss: 0.002171686137623993
Iteration: 173
Loss: 0.002169158191845557
Iteration: 174
Loss: 0.0021660639861322663
Iteration: 175
Loss: 0.0021625137269887843
Iteration: 176
Loss: 0.0021534552352135013
Iteration: 177
Loss: 0.0021534380704210498
Iteration: 178
Loss: 0.0021417630077512177
Iteration: 179
Loss: 0.002140113756198574
Iteration: 180
Loss: 0.0021341269703604926
Iteration: 181
Loss: 0.0021316149648577896
Iteration: 182
Loss: 0.0021192937856540084
Iteration: 183
Loss: 0.0021188604059042752
Iteration: 184
Loss: 0.002118146146164724
Iteration: 185
Loss: 0.0021123002343638626
Iteration: 186
Loss: 0.0021025776923461643
Iteration: 187
Loss: 0.002105895475840863
Iteration: 188
Loss: 0.002101057470649665
Iteration: 189
Loss: 0.002091526854380873
Iteration: 190
Loss: 0.0020886830350292135
Iteration: 191
Loss: 0.0020880801631942576
Iteration: 192
Loss: 0.002088359794234512
Iteration: 193
Loss: 0.0020798676055108693
Iteration: 194
Loss: 0.002073014811842622
Iteration: 195
Loss: 0.0020728804326305785
Iteration: 196
Loss: 0.002076372778744517
Iteration: 197
Loss: 0.002068642979591257
Iteration: 198
Loss: 0.0020749659786276796
Iteration: 199
Loss: 0.0020676597760254404
Iteration: 200
Loss: 0.002063130334675036
Iteration: 201
Loss: 0.002072675790304295
Iteration: 202
Loss: 0.0020579907131944727
Iteration: 203
Loss: 0.0020519959061969946
Iteration: 204
Loss: 0.002048734464843608
Iteration: 205
Loss: 0.0020472550963017123
Iteration: 206
Loss: 0.002041286037211525
Iteration: 207
Loss: 0.002050895022550299
Iteration: 208
Loss: 0.0020334851413528307
Iteration: 209
Loss: 0.0020252184778520904
Iteration: 210
Loss: 0.002044619890332314
Iteration: 211
Loss: 0.0020372880174329986
Iteration: 212
Loss: 0.00203321752552357
Iteration: 213
Loss: 0.0020325453693254125
Iteration: 214
Loss: 0.002025059477518094
Iteration: 215
Loss: 0.002025728785133932
Iteration: 216
Loss: 0.0020176278884281164
Iteration: 217
Loss: 0.0020241105618576207
Iteration: 218
Loss: 0.0020262673784645253
Iteration: 219
Loss: 0.0020326151480944254
Iteration: 220
Loss: 0.00200932104870631
Iteration: 221
Loss: 0.0020112055799360443
Iteration: 222
Loss: 0.002008906298282522
Iteration: 223
Loss: 0.0020034804848241216
Iteration: 224
Loss: 0.0020089175589497626
Iteration: 225
Loss: 0.0020029516272637763
Iteration: 226
Loss: 0.0020110496293960344
Iteration: 227
Loss: 0.0020164646809046837
Iteration: 228
Loss: 0.0020160810033112396
Iteration: 229
Loss: 0.0020030333048284606
Iteration: 230
Loss: 0.001996132257727929
Iteration: 231
Loss: 0.0020008163446169577
Iteration: 232
Loss: 0.002009259900480601
Iteration: 233
Loss: 0.0020037817504302956
Iteration: 234
Loss: 0.0019993244894538765
Iteration: 235
Loss: 0.001993778028042504
Iteration: 236
Loss: 0.0019855247724249406
Iteration: 237
Loss: 0.0019939408513981803
Iteration: 238
Loss: 0.001985810219919608
Iteration: 239
Loss: 0.0019960469885349826
Iteration: 240
Loss: 0.001987813437288558
Iteration: 241
Loss: 0.0019863436909760405
Iteration: 242
Loss: 0.0019930035183436156
Iteration: 243
Loss: 0.001995185161595819
Iteration: 244
Loss: 0.001995573783247375
Iteration: 245
Loss: 0.0019892219103366873
Iteration: 246
Loss: 0.0019745913572770395
Iteration: 247
Loss: 0.001984234446556204
Iteration: 248
Loss: 0.001986358376553305
Iteration: 249
Loss: 0.0019823087761638527
Iteration: 250
Loss: 0.0019859895512553643
Iteration: 251
Loss: 0.0019857186973554855
Iteration: 252
Loss: 0.001972041358699861
Iteration: 253
Loss: 0.0019808269830213655
Iteration: 254
Loss: 0.001971796964134239
Iteration: 255
Loss: 0.0019862623813412623
Iteration: 256
Loss: 0.001968328991939348
Iteration: 257
Loss: 0.001982876586865772
Iteration: 258
Loss: 0.0019751951360017245
Iteration: 259
Loss: 0.0019612554618858814
Iteration: 260
Loss: 0.0019749934471752725
Iteration: 261
Loss: 0.0019723234545830407
Iteration: 262
Loss: 0.001983876695086099
Iteration: 263
Loss: 0.00196788403964429
Iteration: 264
Loss: 0.001969459423519395
Iteration: 265
Loss: 0.0019648770045742024
Iteration: 266
Loss: 0.0019652362478850983
Iteration: 267
Loss: 0.0019794088805577268
Iteration: 268
Loss: 0.001964752848352087
Iteration: 269
Loss: 0.0019681175055419223
Iteration: 270
Loss: 0.001960204151045117
Iteration: 271
Loss: 0.0019612331805681743
Iteration: 272
Loss: 0.001960827605396417
Iteration: 273
Loss: 0.0019494137111591336
Iteration: 274
Loss: 0.0019727174945773536
Iteration: 275
Loss: 0.001955609511365585
Iteration: 276
Loss: 0.0019522634504454924
Iteration: 277
Loss: 0.0019553757767066544
Iteration: 278
Loss: 0.0019653474058335028
Iteration: 279
Loss: 0.0019579902161947554
Iteration: 280
Loss: 0.001952073667600843
Iteration: 281
Loss: 0.001945133470458749
Iteration: 282
Loss: 0.0019516623162170066
Iteration: 283
Loss: 0.001958207920032703
Iteration: 284
Loss: 0.0019524874996370923
Iteration: 285
Loss: 0.0019548177420173163
Iteration: 286
Loss: 0.001952305426073755
Iteration: 287
Loss: 0.0019485476731472178
Iteration: 288
Loss: 0.0019531025965180662
Iteration: 289
Loss: 0.0019486274348893835
Iteration: 290
Loss: 0.001953180466870559
Iteration: 291
Loss: 0.0019435501415972357
Iteration: 292
Loss: 0.0019442108675354608
Iteration: 293
Loss: 0.0019511154631474687
Iteration: 294
Loss: 0.001946855950123274
Iteration: 295
Loss: 0.0019367149654476914
Iteration: 296
Loss: 0.0019445836408937603
Iteration: 297
Loss: 0.0019453108945953442
Iteration: 298
Loss: 0.001945437239712955
Iteration: 299
Loss: 0.0019318729360231462
Iteration: 300
Loss: 0.001934254011656675
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.19631901840490795
accuracy: 0.9482894736842106
confusion: 48 289 104 7159
precision: 0.142433234421365
recall: 0.3157894736842105
*******************************
Processing 2/2...
Hyperparameters: (300, 128, 0.0002, 1.0, 200, 30, 0.2, 0.4, 0.5, 0.1)
Processing fold: ../../../kg_constructor/output/salmonella/folds/fold_0
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 84531
Starting training...
Iteration: 1
Loss: 0.7015903871736409
Iteration: 2
Loss: 0.6938939955499437
Iteration: 3
Loss: 0.6745458385090769
Iteration: 4
Loss: 0.6357109487792592
Iteration: 5
Loss: 0.5821926637932107
Iteration: 6
Loss: 0.5233045481605294
Iteration: 7
Loss: 0.4705114655288649
Iteration: 8
Loss: 0.42991329453609606
Iteration: 9
Loss: 0.39615732063481834
Iteration: 10
Loss: 0.3710107170505288
Iteration: 11
Loss: 0.3503615878246449
Iteration: 12
Loss: 0.33097386691305375
Iteration: 13
Loss: 0.315242389102041
Iteration: 14
Loss: 0.29840202574376706
Iteration: 15
Loss: 0.28420666412070944
Iteration: 16
Loss: 0.26939647764335445
Iteration: 17
Loss: 0.25539255675710276
Iteration: 18
Loss: 0.2410383936431673
Iteration: 19
Loss: 0.22716188338803656
Iteration: 20
Loss: 0.21399825204301764
Iteration: 21
Loss: 0.20059343970116275
Iteration: 22
Loss: 0.1879395619954592
Iteration: 23
Loss: 0.17456602498337073
Iteration: 24
Loss: 0.16370043802408524
Iteration: 25
Loss: 0.15194593222788821
Iteration: 26
Loss: 0.14160690484223543
Iteration: 27
Loss: 0.13034422116147149
Iteration: 28
Loss: 0.12152153777855414
Iteration: 29
Loss: 0.11210718327834282
Iteration: 30
Loss: 0.10337564358372747
Iteration: 31
Loss: 0.09620193447227832
Iteration: 32
Loss: 0.08822965437983289
Iteration: 33
Loss: 0.0815020975323371
Iteration: 34
Loss: 0.07500187592741883
Iteration: 35
Loss: 0.06999121974279851
Iteration: 36
Loss: 0.06427069068138982
Iteration: 37
Loss: 0.05944983348434354
Iteration: 38
Loss: 0.05503907762927773
Iteration: 39
Loss: 0.05095411892290468
Iteration: 40
Loss: 0.04773792504896352
Iteration: 41
Loss: 0.0441034796797199
Iteration: 42
Loss: 0.04108076312659699
Iteration: 43
Loss: 0.03834983628289199
Iteration: 44
Loss: 0.03551015496989827
Iteration: 45
Loss: 0.03344324132266604
Iteration: 46
Loss: 0.031245905813979513
Iteration: 47
Loss: 0.029096528220875762
Iteration: 48
Loss: 0.02747489470574591
Iteration: 49
Loss: 0.025964482376972835
Iteration: 50
Loss: 0.024264922342550607
Iteration: 51
Loss: 0.022887916867563755
Iteration: 52
Loss: 0.021550607764058642
Iteration: 53
Loss: 0.020403767901438254
Iteration: 54
Loss: 0.019368607205925165
Iteration: 55
Loss: 0.018326313568302142
Iteration: 56
Loss: 0.017447838996663505
Iteration: 57
Loss: 0.0166442936723247
Iteration: 58
Loss: 0.01582474627152637
Iteration: 59
Loss: 0.014989889606281562
Iteration: 60
Loss: 0.01444668006611827
Iteration: 61
Loss: 0.013732843871377869
Iteration: 62
Loss: 0.013177371744848327
Iteration: 63
Loss: 0.012583282143797404
Iteration: 64
Loss: 0.012129950003675472
Iteration: 65
Loss: 0.011658667713219736
Iteration: 66
Loss: 0.011124229759989697
Iteration: 67
Loss: 0.010751993429881555
Iteration: 68
Loss: 0.010319589890171716
Iteration: 69
Loss: 0.009976212705634995
Iteration: 70
Loss: 0.009587731085901644
Iteration: 71
Loss: 0.009280550811025832
Iteration: 72
Loss: 0.008966345294022265
Iteration: 73
Loss: 0.008640652189008247
Iteration: 74
Loss: 0.008350488430831903
Iteration: 75
Loss: 0.008125348189086825
Iteration: 76
Loss: 0.00788968962467747
Iteration: 77
Loss: 0.0076097801477176905
Iteration: 78
Loss: 0.007380776136055773
Iteration: 79
Loss: 0.007208528342070403
Iteration: 80
Loss: 0.0069755594774988695
Iteration: 81
Loss: 0.006800055699307977
Iteration: 82
Loss: 0.006595546333694163
Iteration: 83
Loss: 0.006456891000040888
Iteration: 84
Loss: 0.006272096664035394
Iteration: 85
Loss: 0.006073945546867671
Iteration: 86
Loss: 0.005928325219608751
Iteration: 87
Loss: 0.0057826643223287885
Iteration: 88
Loss: 0.005634949513837511
Iteration: 89
Loss: 0.005499353028695892
Iteration: 90
Loss: 0.005365229188752027
Iteration: 91
Loss: 0.005268282371998569
Iteration: 92
Loss: 0.005164332101466479
Iteration: 93
Loss: 0.005026265317689122
Iteration: 94
Loss: 0.004927872185721809
Iteration: 95
Loss: 0.0048241539179910845
Iteration: 96
Loss: 0.004711086890533751
Iteration: 97
Loss: 0.004618514547653404
Iteration: 98
Loss: 0.004511466726982667
Iteration: 99
Loss: 0.004435129788485758
Iteration: 100
Loss: 0.004351192579409222
Iteration: 101
Loss: 0.0043050882688229096
Iteration: 102
Loss: 0.0042076973664217894
Iteration: 103
Loss: 0.0041189504482633905
Iteration: 104
Loss: 0.004035709204474165
Iteration: 105
Loss: 0.003974271406431073
Iteration: 106
Loss: 0.003901155406816138
Iteration: 107
Loss: 0.0038159474655755877
Iteration: 108
Loss: 0.0037595635929639323
Iteration: 109
Loss: 0.003691866922042804
Iteration: 110
Loss: 0.0036310297896924577
Iteration: 111
Loss: 0.0035817957954642214
Iteration: 112
Loss: 0.003522833590798172
Iteration: 113
Loss: 0.003480011643266972
Iteration: 114
Loss: 0.0034186458445073648
Iteration: 115
Loss: 0.0033526301188509405
Iteration: 116
Loss: 0.00332098434690708
Iteration: 117
Loss: 0.003265869064992409
Iteration: 118
Loss: 0.0032225052371943072
Iteration: 119
Loss: 0.0031832906066865466
Iteration: 120
Loss: 0.003158384019991866
Iteration: 121
Loss: 0.0030916827719336673
Iteration: 122
Loss: 0.003062595800140206
Iteration: 123
Loss: 0.0030114414647543506
Iteration: 124
Loss: 0.0029887896243850757
Iteration: 125
Loss: 0.00294056578171382
Iteration: 126
Loss: 0.002917984238952214
Iteration: 127
Loss: 0.0028932066663041895
Iteration: 128
Loss: 0.0028484145719787956
Iteration: 129
Loss: 0.0028314764186004064
Iteration: 130
Loss: 0.002801018801552278
Iteration: 131
Loss: 0.0027751738211301007
Iteration: 132
Loss: 0.002761419974612417
Iteration: 133
Loss: 0.0027274906830747187
Iteration: 134
Loss: 0.002689754958322019
Iteration: 135
Loss: 0.0026702359031287974
Iteration: 136
Loss: 0.0026502757143505194
Iteration: 137
Loss: 0.002624516688678184
Iteration: 138
Loss: 0.0026023398601898442
Iteration: 139
Loss: 0.0025772338209926714
Iteration: 140
Loss: 0.002569599309936166
Iteration: 141
Loss: 0.002546085878332824
Iteration: 142
Loss: 0.002530849113324542
Iteration: 143
Loss: 0.0025284182349288903
Iteration: 144
Loss: 0.0024996152072914958
Iteration: 145
Loss: 0.0024678505422478475
Iteration: 146
Loss: 0.002460932977106652
Iteration: 147
Loss: 0.0024609796409667643
Iteration: 148
Loss: 0.0024387981357443848
Iteration: 149
Loss: 0.0024288779808737245
Iteration: 150
Loss: 0.0024181902293621757
Iteration: 151
Loss: 0.0024056064804303056
Iteration: 152
Loss: 0.002381787589394752
Iteration: 153
Loss: 0.0023729801982825187
Iteration: 154
Loss: 0.0023492475724376646
Iteration: 155
Loss: 0.0023423519422426636
Iteration: 156
Loss: 0.002335108928727331
Iteration: 157
Loss: 0.0023128607448328425
Iteration: 158
Loss: 0.002309963681999548
Iteration: 159
Loss: 0.002315191773375795
Iteration: 160
Loss: 0.002295731267903322
Iteration: 161
Loss: 0.0022899327069566943
Iteration: 162
Loss: 0.002282372503368943
Iteration: 163
Loss: 0.0022795973963644586
Iteration: 164
Loss: 0.002262641478870294
Iteration: 165
Loss: 0.002260129671704806
Iteration: 166
Loss: 0.0022669734870210106
Iteration: 167
Loss: 0.0022412626299846135
Iteration: 168
Loss: 0.00224203893477902
Iteration: 169
Loss: 0.002224124568503028
Iteration: 170
Loss: 0.002214924941115357
Iteration: 171
Loss: 0.00222001851386862
Iteration: 172
Loss: 0.0022086866018879744
Iteration: 173
Loss: 0.002215305351893659
Iteration: 174
Loss: 0.002203526111883054
Iteration: 175
Loss: 0.002188871492470764
Iteration: 176
Loss: 0.0021949055644879005
Iteration: 177
Loss: 0.002180977163974334
Iteration: 178
Loss: 0.002175964055766846
Iteration: 179
Loss: 0.0021756595190338514
Iteration: 180
Loss: 0.0021591362023128217
Iteration: 181
Loss: 0.002168363356893813
Iteration: 182
Loss: 0.002162020654923478
Iteration: 183
Loss: 0.002155045564803812
Iteration: 184
Loss: 0.0021581035653520145
Iteration: 185
Loss: 0.0021459759905773364
Iteration: 186
Loss: 0.0021395807867340836
Iteration: 187
Loss: 0.0021387881191194426
Iteration: 188
Loss: 0.002127412054112848
Iteration: 189
Loss: 0.002121015348368221
Iteration: 190
Loss: 0.002133716169434289
Iteration: 191
Loss: 0.002118090199260616
Iteration: 192
Loss: 0.002117839327116531
Iteration: 193
Loss: 0.0021164025378379004
Iteration: 194
Loss: 0.002111854904078315
Iteration: 195
Loss: 0.0021075229979706585
Iteration: 196
Loss: 0.0021004688891547696
Iteration: 197
Loss: 0.0021151073052789325
Iteration: 198
Loss: 0.002095586687820837
Iteration: 199
Loss: 0.002094946021248621
Iteration: 200
Loss: 0.002091391503603922
Iteration: 201
Loss: 0.0020879479297609242
Iteration: 202
Loss: 0.0020818503768999636
Iteration: 203
Loss: 0.0020874016760147095
Iteration: 204
Loss: 0.002091293697486873
Iteration: 205
Loss: 0.0020796670520931114
Iteration: 206
Loss: 0.002078323048700429
Iteration: 207
Loss: 0.0020749237155541778
Iteration: 208
Loss: 0.002064945860363451
Iteration: 209
Loss: 0.0020695609675812315
Iteration: 210
Loss: 0.002075341893762442
Iteration: 211
Loss: 0.0020650452632388033
Iteration: 212
Loss: 0.0020569649433004266
Iteration: 213
Loss: 0.002061159748089626
Iteration: 214
Loss: 0.002063852285011959
Iteration: 215
Loss: 0.002058704334805593
Iteration: 216
Loss: 0.0020601485775392733
Iteration: 217
Loss: 0.002055937592457566
Iteration: 218
Loss: 0.0020584829574174903
Iteration: 219
Loss: 0.0020538640731692683
Iteration: 220
Loss: 0.002056849686382913
Iteration: 221
Loss: 0.0020537564206155544
Iteration: 222
Loss: 0.002037999826701887
Iteration: 223
Loss: 0.0020328366677679214
Iteration: 224
Loss: 0.0020508769264352728
Iteration: 225
Loss: 0.002043910553092482
Iteration: 226
Loss: 0.002032883669376189
Iteration: 227
Loss: 0.0020363860660129124
Iteration: 228
Loss: 0.002028534384329378
Iteration: 229
Loss: 0.0020216966858478606
Iteration: 230
Loss: 0.002032538857253521
Iteration: 231
Loss: 0.002029667944750852
Iteration: 232
Loss: 0.002036630502258094
Iteration: 233
Loss: 0.002023364608406009
Iteration: 234
Loss: 0.002031649727328324
Iteration: 235
Loss: 0.0020299376014413107
Iteration: 236
Loss: 0.002014656043841423
Iteration: 237
Loss: 0.002021715126897174
Iteration: 238
Loss: 0.002015442716785603
Iteration: 239
Loss: 0.0020215119304204427
Iteration: 240
Loss: 0.002020708943521719
Iteration: 241
Loss: 0.0020178721421455713
Iteration: 242
Loss: 0.0020161826942506947
Iteration: 243
Loss: 0.0020046604015393024
Iteration: 244
Loss: 0.002008784522106986
Iteration: 245
Loss: 0.0020041640669705325
Iteration: 246
Loss: 0.002016066559187976
Iteration: 247
Loss: 0.002012093080046736
Iteration: 248
Loss: 0.002006441968447172
Iteration: 249
Loss: 0.002004717787106832
Iteration: 250
Loss: 0.0020045703348853154
Iteration: 251
Loss: 0.0020149279411675203
Iteration: 252
Loss: 0.002001581219151432
Iteration: 253
Loss: 0.0019981803151169495
Iteration: 254
Loss: 0.002005361418676689
Iteration: 255
Loss: 0.0019883462874632743
Iteration: 256
Loss: 0.0019946880825438803
Iteration: 257
Loss: 0.0019955757364377748
Iteration: 258
Loss: 0.0019949556868753314
Iteration: 259
Loss: 0.0019932528928980047
Iteration: 260
Loss: 0.0019832480264781616
Iteration: 261
Loss: 0.0019887473727403966
Iteration: 262
Loss: 0.001979262246117916
Iteration: 263
Loss: 0.002007979165222266
Iteration: 264
Loss: 0.001993974038388258
Iteration: 265
Loss: 0.0019847412599411644
Iteration: 266
Loss: 0.001983375669411027
Iteration: 267
Loss: 0.0019919163199647524
Iteration: 268
Loss: 0.00198636242809395
Iteration: 269
Loss: 0.00199281376424349
Iteration: 270
Loss: 0.0019917029062469986
Iteration: 271
Loss: 0.0019805400523092644
Iteration: 272
Loss: 0.001986599840614715
Iteration: 273
Loss: 0.0019965317821774033
Iteration: 274
Loss: 0.0019889403174826762
Iteration: 275
Loss: 0.001990662243815116
Iteration: 276
Loss: 0.0019948357345398377
Iteration: 277
Loss: 0.0019665135941640648
Iteration: 278
Loss: 0.0019780731670857397
Iteration: 279
Loss: 0.0019863370553026964
Iteration: 280
Loss: 0.001972623673888544
Iteration: 281
Loss: 0.001980770145037016
Iteration: 282
Loss: 0.001981065792525992
Iteration: 283
Loss: 0.001987977309819357
Iteration: 284
Loss: 0.0019862878978551355
Iteration: 285
Loss: 0.0019821259236239173
Iteration: 286
Loss: 0.0019732059545837987
Iteration: 287
Loss: 0.0019684866499040782
Iteration: 288
Loss: 0.001974560836341554
Iteration: 289
Loss: 0.001970948619814015
Iteration: 290
Loss: 0.0019761295299287195
Iteration: 291
Loss: 0.0019601948924339663
Iteration: 292
Loss: 0.0019751207940395415
Iteration: 293
Loss: 0.0019652592578764867
Iteration: 294
Loss: 0.0019637819531914077
Iteration: 295
Loss: 0.0019651088061639005
Iteration: 296
Loss: 0.001964023658706818
Iteration: 297
Loss: 0.0019701302215387978
Iteration: 298
Loss: 0.001962567318840251
Iteration: 299
Loss: 0.0019631716853904502
Iteration: 300
Loss: 0.001955979904677305
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.12395709177592373
accuracy: 0.9032894736842105
confusion: 52 635 100 6813
precision: 0.07569141193595343
recall: 0.34210526315789475
Processing fold: ../../../kg_constructor/output/salmonella/folds/fold_1
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 84531
Starting training...
Iteration: 1
Loss: 0.7018576521932343
Iteration: 2
Loss: 0.694415384604607
Iteration: 3
Loss: 0.6752784391980112
Iteration: 4
Loss: 0.6359105051299672
Iteration: 5
Loss: 0.5802638619034378
Iteration: 6
Loss: 0.52375657396552
Iteration: 7
Loss: 0.4718328096248485
Iteration: 8
Loss: 0.4294147237583443
Iteration: 9
Loss: 0.3975511307333722
Iteration: 10
Loss: 0.3730792568789588
Iteration: 11
Loss: 0.35177973391097267
Iteration: 12
Loss: 0.3338109652201335
Iteration: 13
Loss: 0.3171070490354373
Iteration: 14
Loss: 0.30097484183900153
Iteration: 15
Loss: 0.28599624979643173
Iteration: 16
Loss: 0.27143384277084726
Iteration: 17
Loss: 0.25765823306124885
Iteration: 18
Loss: 0.2437917665566927
Iteration: 19
Loss: 0.2305081648591124
Iteration: 20
Loss: 0.21639288078855584
Iteration: 21
Loss: 0.2030725869131677
Iteration: 22
Loss: 0.19023129104832073
Iteration: 23
Loss: 0.1769357680538554
Iteration: 24
Loss: 0.16538903963418655
Iteration: 25
Loss: 0.15337259975480444
Iteration: 26
Loss: 0.14245438115832246
Iteration: 27
Loss: 0.13156474768011658
Iteration: 28
Loss: 0.12138633825528769
Iteration: 29
Loss: 0.11242311070730657
Iteration: 30
Loss: 0.10385188846676438
Iteration: 31
Loss: 0.09536754468708862
Iteration: 32
Loss: 0.08825560557989427
Iteration: 33
Loss: 0.08166251763885404
Iteration: 34
Loss: 0.07500448280278547
Iteration: 35
Loss: 0.06935917374528484
Iteration: 36
Loss: 0.0641565990668756
Iteration: 37
Loss: 0.05918114595575097
Iteration: 38
Loss: 0.054971881992655035
Iteration: 39
Loss: 0.05082535292999244
Iteration: 40
Loss: 0.047001262459857966
Iteration: 41
Loss: 0.04385333388675878
Iteration: 42
Loss: 0.040815549822133264
Iteration: 43
Loss: 0.037969713685689147
Iteration: 44
Loss: 0.03552572847700414
Iteration: 45
Loss: 0.03298715026014381
Iteration: 46
Loss: 0.031003752355406314
Iteration: 47
Loss: 0.028979020262206043
Iteration: 48
Loss: 0.027212383915428764
Iteration: 49
Loss: 0.025658702109882862
Iteration: 50
Loss: 0.024078999887462014
Iteration: 51
Loss: 0.022739236047606408
Iteration: 52
Loss: 0.02139043235392482
Iteration: 53
Loss: 0.020298794532815616
Iteration: 54
Loss: 0.01918115671494125
Iteration: 55
Loss: 0.01824579300519861
Iteration: 56
Loss: 0.017307362737663
Iteration: 57
Loss: 0.016553070292704634
Iteration: 58
Loss: 0.015630704534734474
Iteration: 59
Loss: 0.015032890505720804
Iteration: 60
Loss: 0.014331692001885839
Iteration: 61
Loss: 0.013710920124050275
Iteration: 62
Loss: 0.013012854382395744
Iteration: 63
Loss: 0.012569806768478435
Iteration: 64
Loss: 0.011987612916179646
Iteration: 65
Loss: 0.011507272490380723
Iteration: 66
Loss: 0.011113812169635001
Iteration: 67
Loss: 0.010710225360077104
Iteration: 68
Loss: 0.010259010955507372
Iteration: 69
Loss: 0.009882458451169508
Iteration: 70
Loss: 0.00952337786877229
Iteration: 71
Loss: 0.009264920481745108
Iteration: 72
Loss: 0.008910886006268822
Iteration: 73
Loss: 0.00859637481999802
Iteration: 74
Loss: 0.008376985886076719
Iteration: 75
Loss: 0.008078461972835623
Iteration: 76
Loss: 0.00780101287190193
Iteration: 77
Loss: 0.007582068207593244
Iteration: 78
Loss: 0.007390232446293036
Iteration: 79
Loss: 0.0071420760273381515
Iteration: 80
Loss: 0.006903668233181959
Iteration: 81
Loss: 0.006739183955676394
Iteration: 82
Loss: 0.006560565139178509
Iteration: 83
Loss: 0.006369098049393038
Iteration: 84
Loss: 0.006163286551097293
Iteration: 85
Loss: 0.006036987884637014
Iteration: 86
Loss: 0.005875920428446046
Iteration: 87
Loss: 0.005751089261913741
Iteration: 88
Loss: 0.005614206344349149
Iteration: 89
Loss: 0.005470114616378222
Iteration: 90
Loss: 0.005378873484140193
Iteration: 91
Loss: 0.005241046176740417
Iteration: 92
Loss: 0.00511245011570461
Iteration: 93
Loss: 0.005023282463949771
Iteration: 94
Loss: 0.004895237717133613
Iteration: 95
Loss: 0.004822202377527216
Iteration: 96
Loss: 0.004690033119217849
Iteration: 97
Loss: 0.004587179846072822
Iteration: 98
Loss: 0.004520549145699652
Iteration: 99
Loss: 0.004420338371009739
Iteration: 100
Loss: 0.004341063782411777
Iteration: 101
Loss: 0.004272118650583757
Iteration: 102
Loss: 0.004185366323562684
Iteration: 103
Loss: 0.004105563384745224
Iteration: 104
Loss: 0.0040221780015408624
Iteration: 105
Loss: 0.003942907023255104
Iteration: 106
Loss: 0.0038898778176739996
Iteration: 107
Loss: 0.0038249668331793796
Iteration: 108
Loss: 0.0037718435438970723
Iteration: 109
Loss: 0.003711385776024358
Iteration: 110
Loss: 0.0036401058357484915
Iteration: 111
Loss: 0.003595759178063384
Iteration: 112
Loss: 0.0035162448860061022
Iteration: 113
Loss: 0.003497853691195264
Iteration: 114
Loss: 0.0034306134028291263
Iteration: 115
Loss: 0.0033971276392953265
Iteration: 116
Loss: 0.003327264378629165
Iteration: 117
Loss: 0.00328202066184194
Iteration: 118
Loss: 0.003216414456543179
Iteration: 119
Loss: 0.003184114275270222
Iteration: 120
Loss: 0.003135279901785615
Iteration: 121
Loss: 0.0031138952465061052
Iteration: 122
Loss: 0.0031009294052412846
Iteration: 123
Loss: 0.0030399193918263473
Iteration: 124
Loss: 0.0030047484087171376
Iteration: 125
Loss: 0.0029551180275042116
Iteration: 126
Loss: 0.002921361809619415
Iteration: 127
Loss: 0.0028963307332661417
Iteration: 128
Loss: 0.0028614460197449836
Iteration: 129
Loss: 0.002835054309854721
Iteration: 130
Loss: 0.0027926639728478074
Iteration: 131
Loss: 0.0027820548289857897
Iteration: 132
Loss: 0.0027446031374971807
Iteration: 133
Loss: 0.002723491017672199
Iteration: 134
Loss: 0.0026937647820392875
Iteration: 135
Loss: 0.0026684878020328874
Iteration: 136
Loss: 0.002654383585638838
Iteration: 137
Loss: 0.002642817723415332
Iteration: 138
Loss: 0.0026171560928133536
Iteration: 139
Loss: 0.002581814240756226
Iteration: 140
Loss: 0.002563293695587803
Iteration: 141
Loss: 0.0025591339174382112
Iteration: 142
Loss: 0.0025437378157850033
Iteration: 143
Loss: 0.002512668242195138
Iteration: 144
Loss: 0.002492142979193617
Iteration: 145
Loss: 0.002485311564265026
Iteration: 146
Loss: 0.0024719991244053765
Iteration: 147
Loss: 0.002449161174741608
Iteration: 148
Loss: 0.002429818021662441
Iteration: 149
Loss: 0.002411464416065518
Iteration: 150
Loss: 0.0024090605230289107
Iteration: 151
Loss: 0.0023908869127662463
Iteration: 152
Loss: 0.002390616392302844
Iteration: 153
Loss: 0.002374715942107601
Iteration: 154
Loss: 0.0023576973414301504
Iteration: 155
Loss: 0.0023505190462103963
Iteration: 156
Loss: 0.002339994192790286
Iteration: 157
Loss: 0.002323840207270818
Iteration: 158
Loss: 0.0023208201437452695
Iteration: 159
Loss: 0.002314497233816871
Iteration: 160
Loss: 0.0023129932556508314
Iteration: 161
Loss: 0.0023038361054833656
Iteration: 162
Loss: 0.0022797154889292555
Iteration: 163
Loss: 0.0022765626659455858
Iteration: 164
Loss: 0.0022677917818159418
Iteration: 165
Loss: 0.0022602643024314333
Iteration: 166
Loss: 0.0022483669554837693
Iteration: 167
Loss: 0.002261311382078278
Iteration: 168
Loss: 0.0022251019886706347
Iteration: 169
Loss: 0.0022266368370181247
Iteration: 170
Loss: 0.0022209365737199047
Iteration: 171
Loss: 0.002216862787688036
Iteration: 172
Loss: 0.0022155150834379006
Iteration: 173
Loss: 0.002201149567823719
Iteration: 174
Loss: 0.0021982349406490906
Iteration: 175
Loss: 0.0021995491431566114
Iteration: 176
Loss: 0.0021822856793777996
Iteration: 177
Loss: 0.002185046090744436
Iteration: 178
Loss: 0.002175533090557121
Iteration: 179
Loss: 0.002178484395744256
Iteration: 180
Loss: 0.0021664032390640107
Iteration: 181
Loss: 0.002166086763293011
Iteration: 182
Loss: 0.0021666912750030556
Iteration: 183
Loss: 0.0021620779743786026
Iteration: 184
Loss: 0.0021462637821879284
Iteration: 185
Loss: 0.0021467451581064566
Iteration: 186
Loss: 0.00214108681725913
Iteration: 187
Loss: 0.002136956931977176
Iteration: 188
Loss: 0.002133892672308893
Iteration: 189
Loss: 0.0021304792773415462
Iteration: 190
Loss: 0.0021343543352329254
Iteration: 191
Loss: 0.0021357340781294086
Iteration: 192
Loss: 0.0021233724898739176
Iteration: 193
Loss: 0.002122962266712645
Iteration: 194
Loss: 0.002124809839670765
Iteration: 195
Loss: 0.002110783653161316
Iteration: 196
Loss: 0.002102130352818396
Iteration: 197
Loss: 0.002103128778046848
Iteration: 198
Loss: 0.0020956384423361334
Iteration: 199
Loss: 0.0021014787475176063
Iteration: 200
Loss: 0.002087488415490054
Iteration: 201
Loss: 0.002097197085400514
Iteration: 202
Loss: 0.0020832952807576938
Iteration: 203
Loss: 0.002086783874274036
Iteration: 204
Loss: 0.002094188360152421
Iteration: 205
Loss: 0.002082337174035701
Iteration: 206
Loss: 0.002083519636078473
Iteration: 207
Loss: 0.0020683130053918302
Iteration: 208
Loss: 0.002081462333013338
Iteration: 209
Loss: 0.002061266923484243
Iteration: 210
Loss: 0.0020715404477223386
Iteration: 211
Loss: 0.0020571759870321848
Iteration: 212
Loss: 0.00206900137781315
Iteration: 213
Loss: 0.0020658955938056296
Iteration: 214
Loss: 0.002060469886701968
Iteration: 215
Loss: 0.0020514908634747067
Iteration: 216
Loss: 0.002062410301597858
Iteration: 217
Loss: 0.0020528003188241043
Iteration: 218
Loss: 0.002049519608579116
Iteration: 219
Loss: 0.0020494182496389123
Iteration: 220
Loss: 0.0020440559385827292
Iteration: 221
Loss: 0.0020407867045889112
Iteration: 222
Loss: 0.002046023603287284
Iteration: 223
Loss: 0.0020371450163010093
Iteration: 224
Loss: 0.0020355571688003378
Iteration: 225
Loss: 0.0020378800295868223
Iteration: 226
Loss: 0.00204286445075945
Iteration: 227
Loss: 0.0020414878668861443
Iteration: 228
Loss: 0.0020329461757806534
Iteration: 229
Loss: 0.0020350554906245735
Iteration: 230
Loss: 0.00202214489777193
Iteration: 231
Loss: 0.0020298837040217572
Iteration: 232
Loss: 0.0020178867026841565
Iteration: 233
Loss: 0.0020278270103405286
Iteration: 234
Loss: 0.002021594810943453
Iteration: 235
Loss: 0.002022971295648151
Iteration: 236
Loss: 0.002023958626927601
Iteration: 237
Loss: 0.002017346233773379
Iteration: 238
Loss: 0.0020187487911984877
Iteration: 239
Loss: 0.0020284732619737403
Iteration: 240
Loss: 0.0020154974434846715
Iteration: 241
Loss: 0.002016475938697472
Iteration: 242
Loss: 0.0020111259245486172
Iteration: 243
Loss: 0.00200677503986905
Iteration: 244
Loss: 0.0020128770538801214
Iteration: 245
Loss: 0.002011165637982848
Iteration: 246
Loss: 0.002017507453184621
Iteration: 247
Loss: 0.002014470295290704
Iteration: 248
Loss: 0.0020005147541976636
Iteration: 249
Loss: 0.0020033585131858235
Iteration: 250
Loss: 0.0019997060730087535
Iteration: 251
Loss: 0.0020085657446610706
Iteration: 252
Loss: 0.0019937265667213516
Iteration: 253
Loss: 0.002024798937153393
Iteration: 254
Loss: 0.0019865531379494954
Iteration: 255
Loss: 0.0019936810109626364
Iteration: 256
Loss: 0.00199576300126213
Iteration: 257
Loss: 0.002005293961027982
Iteration: 258
Loss: 0.0020038673228779695
Iteration: 259
Loss: 0.001997242021995286
Iteration: 260
Loss: 0.0019869768983443027
Iteration: 261
Loss: 0.001983129172183481
Iteration: 262
Loss: 0.0019962617848643368
Iteration: 263
Loss: 0.0019975969277575244
Iteration: 264
Loss: 0.001995157929033869
Iteration: 265
Loss: 0.0019890285574221686
Iteration: 266
Loss: 0.001988459525077983
Iteration: 267
Loss: 0.0019880380268772076
Iteration: 268
Loss: 0.001990227619427498
Iteration: 269
Loss: 0.0019939282153054703
Iteration: 270
Loss: 0.001986203697958478
Iteration: 271
Loss: 0.001978419123240459
Iteration: 272
Loss: 0.0019878234576295923
Iteration: 273
Loss: 0.001991245178448289
Iteration: 274
Loss: 0.0019898290392356337
Iteration: 275
Loss: 0.0019889162841862365
Iteration: 276
Loss: 0.0019809809458775467
Iteration: 277
Loss: 0.001982643491772498
Iteration: 278
Loss: 0.001984248525624014
Iteration: 279
Loss: 0.0019785722467565428
Iteration: 280
Loss: 0.001976736517979499
Iteration: 281
Loss: 0.00197442831977466
Iteration: 282
Loss: 0.0019625724411144116
Iteration: 283
Loss: 0.0019762842688870468
Iteration: 284
Loss: 0.0019771644937587375
Iteration: 285
Loss: 0.0019863030821498898
Iteration: 286
Loss: 0.001978292959213349
Iteration: 287
Loss: 0.001968544074276715
Iteration: 288
Loss: 0.001968039402358786
Iteration: 289
Loss: 0.001970776759180022
Iteration: 290
Loss: 0.0019718695498060113
Iteration: 291
Loss: 0.0019622127465144905
Iteration: 292
Loss: 0.0019679771072265728
Iteration: 293
Loss: 0.001979453218985855
Iteration: 294
Loss: 0.00198225653299165
Iteration: 295
Loss: 0.001970166663846208
Iteration: 296
Loss: 0.00197355768982505
Iteration: 297
Loss: 0.001963318860227311
Iteration: 298
Loss: 0.0019676079987950715
Iteration: 299
Loss: 0.0019666795866035017
Iteration: 300
Loss: 0.001967587488140037
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.11224489795918367
accuracy: 0.8626315789473684
confusion: 66 958 86 6490
precision: 0.064453125
recall: 0.4342105263157895
Processing fold: ../../../kg_constructor/output/salmonella/folds/fold_2
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 84531
Starting training...
Iteration: 1
Loss: 0.7016214198536344
Iteration: 2
Loss: 0.6940147655981558
Iteration: 3
Loss: 0.6746138753714385
Iteration: 4
Loss: 0.6365157189192595
Iteration: 5
Loss: 0.582459623430982
Iteration: 6
Loss: 0.5247371733924489
Iteration: 7
Loss: 0.4725522432062361
Iteration: 8
Loss: 0.43097392975548166
Iteration: 9
Loss: 0.39980179917665176
Iteration: 10
Loss: 0.3753858639134301
Iteration: 11
Loss: 0.3531978851483192
Iteration: 12
Loss: 0.3357732211366112
Iteration: 13
Loss: 0.3195474787994667
Iteration: 14
Loss: 0.304229094290439
Iteration: 15
Loss: 0.2892899892212432
Iteration: 16
Loss: 0.27521155976954803
Iteration: 17
Loss: 0.2606964475578732
Iteration: 18
Loss: 0.2473090754614936
Iteration: 19
Loss: 0.23409627359590413
Iteration: 20
Loss: 0.21980024211936527
Iteration: 21
Loss: 0.20603553969183086
Iteration: 22
Loss: 0.1933733449306017
Iteration: 23
Loss: 0.18030262177373158
Iteration: 24
Loss: 0.16751134984287214
Iteration: 25
Loss: 0.15582515648853631
Iteration: 26
Loss: 0.14457036554813385
Iteration: 27
Loss: 0.13372625005833896
Iteration: 28
Loss: 0.12368794070717729
Iteration: 29
Loss: 0.11393908788392573
Iteration: 30
Loss: 0.10554734185154055
Iteration: 31
Loss: 0.0972412176154278
Iteration: 32
Loss: 0.08967763764990701
Iteration: 33
Loss: 0.08244041519032584
Iteration: 34
Loss: 0.07612389169725371
Iteration: 35
Loss: 0.07028181262222337
Iteration: 36
Loss: 0.06529169603262419
Iteration: 37
Loss: 0.059846186803446874
Iteration: 38
Loss: 0.055308337380856644
Iteration: 39
Loss: 0.05130088379905547
Iteration: 40
Loss: 0.047593270554954624
Iteration: 41
Loss: 0.044095700399743185
Iteration: 42
Loss: 0.04113268962612859
Iteration: 43
Loss: 0.03831172017035661
Iteration: 44
Loss: 0.035606693384456045
Iteration: 45
Loss: 0.03321484956936336
Iteration: 46
Loss: 0.0311835533996791
Iteration: 47
Loss: 0.029357764249046642
Iteration: 48
Loss: 0.02758122877114349
Iteration: 49
Loss: 0.025844883472647195
Iteration: 50
Loss: 0.02424635160944344
Iteration: 51
Loss: 0.0229488481985934
Iteration: 52
Loss: 0.021662226084757735
Iteration: 53
Loss: 0.020527829910502022
Iteration: 54
Loss: 0.01951393669034228
Iteration: 55
Loss: 0.018372571555736624
Iteration: 56
Loss: 0.017585947948657435
Iteration: 57
Loss: 0.016559591896280094
Iteration: 58
Loss: 0.015851877767730643
Iteration: 59
Loss: 0.015097591853528112
Iteration: 60
Loss: 0.014474394788712631
Iteration: 61
Loss: 0.013792905048179774
Iteration: 62
Loss: 0.01318606390491312
Iteration: 63
Loss: 0.012724764242676306
Iteration: 64
Loss: 0.012166431553109928
Iteration: 65
Loss: 0.011658061537201758
Iteration: 66
Loss: 0.01127626907862263
Iteration: 67
Loss: 0.010802623712354235
Iteration: 68
Loss: 0.010446808381397048
Iteration: 69
Loss: 0.010048921817890655
Iteration: 70
Loss: 0.009735439708571375
Iteration: 71
Loss: 0.009385697428642967
Iteration: 72
Loss: 0.00907541657395569
Iteration: 73
Loss: 0.008782323041677843
Iteration: 74
Loss: 0.008484694336391525
Iteration: 75
Loss: 0.008251348286177273
Iteration: 76
Loss: 0.008014993977813441
Iteration: 77
Loss: 0.007730675849741624
Iteration: 78
Loss: 0.00753513279221492
Iteration: 79
Loss: 0.0073249887640553495
Iteration: 80
Loss: 0.007105929869008653
Iteration: 81
Loss: 0.006914039274654639
Iteration: 82
Loss: 0.0067351229868277355
Iteration: 83
Loss: 0.006522401264742201
Iteration: 84
Loss: 0.0063802752022941904
Iteration: 85
Loss: 0.006243166720701588
Iteration: 86
Loss: 0.006067501231568095
Iteration: 87
Loss: 0.0059242977841216845
Iteration: 88
Loss: 0.0057874046105109616
Iteration: 89
Loss: 0.005656102626228038
Iteration: 90
Loss: 0.005521038696629766
Iteration: 91
Loss: 0.005399901034701385
Iteration: 92
Loss: 0.0052658486708906695
Iteration: 93
Loss: 0.005153366542754717
Iteration: 94
Loss: 0.0050494698635130016
Iteration: 95
Loss: 0.00494292475582089
Iteration: 96
Loss: 0.004817796784832522
Iteration: 97
Loss: 0.0047349363283748615
Iteration: 98
Loss: 0.004623511196746502
Iteration: 99
Loss: 0.004549597427363933
Iteration: 100
Loss: 0.004445705554206614
Iteration: 101
Loss: 0.004359450929993649
Iteration: 102
Loss: 0.004261808173822952
Iteration: 103
Loss: 0.004200302417976437
Iteration: 104
Loss: 0.0041082289133128926
Iteration: 105
Loss: 0.004036523772541572
Iteration: 106
Loss: 0.0039859531522027135
Iteration: 107
Loss: 0.0038879119020737247
Iteration: 108
Loss: 0.0038346927756742564
Iteration: 109
Loss: 0.003766906257994749
Iteration: 110
Loss: 0.0037111380700895814
Iteration: 111
Loss: 0.003652774427017128
Iteration: 112
Loss: 0.0035766880947774573
Iteration: 113
Loss: 0.0035169630133213454
Iteration: 114
Loss: 0.0034695528763817782
Iteration: 115
Loss: 0.003417425832055785
Iteration: 116
Loss: 0.0033583272921128404
Iteration: 117
Loss: 0.0032834140526383747
Iteration: 118
Loss: 0.0032631389158605424
Iteration: 119
Loss: 0.003215465591185623
Iteration: 120
Loss: 0.0031662709050938303
Iteration: 121
Loss: 0.003118168519533895
Iteration: 122
Loss: 0.0030839308337480935
Iteration: 123
Loss: 0.00303267866924957
Iteration: 124
Loss: 0.0029924959145532348
Iteration: 125
Loss: 0.002967743412878962
Iteration: 126
Loss: 0.0029235861672341455
Iteration: 127
Loss: 0.0029045979831551698
Iteration: 128
Loss: 0.0028708216009868514
Iteration: 129
Loss: 0.002828205432627856
Iteration: 130
Loss: 0.002803900986219998
Iteration: 131
Loss: 0.0027741795537970316
Iteration: 132
Loss: 0.002753456732557144
Iteration: 133
Loss: 0.002731755723670861
Iteration: 134
Loss: 0.0027062553297268386
Iteration: 135
Loss: 0.0026698682865380872
Iteration: 136
Loss: 0.0026480537166612018
Iteration: 137
Loss: 0.0026295802780185585
Iteration: 138
Loss: 0.0026052734803086444
Iteration: 139
Loss: 0.0025985902491497035
Iteration: 140
Loss: 0.0025528482891573213
Iteration: 141
Loss: 0.0025365436363413377
Iteration: 142
Loss: 0.002528733387014564
Iteration: 143
Loss: 0.0025122716999900195
Iteration: 144
Loss: 0.002483216631743643
Iteration: 145
Loss: 0.0024749632568363054
Iteration: 146
Loss: 0.002468647458303122
Iteration: 147
Loss: 0.0024414860850407016
Iteration: 148
Loss: 0.0024361835999621283
Iteration: 149
Loss: 0.002411964792114349
Iteration: 150
Loss: 0.0024051676119137325
Iteration: 151
Loss: 0.002390980142755089
Iteration: 152
Loss: 0.0023692811459854798
Iteration: 153
Loss: 0.0023589654348468705
Iteration: 154
Loss: 0.002354401198250276
Iteration: 155
Loss: 0.002338347131570364
Iteration: 156
Loss: 0.002334739837542544
Iteration: 157
Loss: 0.002323169945336786
Iteration: 158
Loss: 0.002315966101984183
Iteration: 159
Loss: 0.002301491526771843
Iteration: 160
Loss: 0.002294139100100707
Iteration: 161
Loss: 0.0022827437273974034
Iteration: 162
Loss: 0.0022732831830917684
Iteration: 163
Loss: 0.00228142676829004
Iteration: 164
Loss: 0.0022649901292436285
Iteration: 165
Loss: 0.002250736329221615
Iteration: 166
Loss: 0.0022477721622972575
Iteration: 167
Loss: 0.002229836056537834
Iteration: 168
Loss: 0.002243597258779186
Iteration: 169
Loss: 0.0022296452963793717
Iteration: 170
Loss: 0.0022215578463618403
Iteration: 171
Loss: 0.0022079573676886933
Iteration: 172
Loss: 0.0022135929399986327
Iteration: 173
Loss: 0.0021899381543245213
Iteration: 174
Loss: 0.00219147639165138
Iteration: 175
Loss: 0.002184190360518793
Iteration: 176
Loss: 0.002179872861166519
Iteration: 177
Loss: 0.002180391452620151
Iteration: 178
Loss: 0.002183772212492279
Iteration: 179
Loss: 0.002173146435611502
Iteration: 180
Loss: 0.0021520372189834346
Iteration: 181
Loss: 0.0021534573134425797
Iteration: 182
Loss: 0.0021473885812785154
Iteration: 183
Loss: 0.0021552337437040275
Iteration: 184
Loss: 0.002152353811169756
Iteration: 185
Loss: 0.0021394608918876377
Iteration: 186
Loss: 0.0021405184848441016
Iteration: 187
Loss: 0.002130326839382358
Iteration: 188
Loss: 0.0021082190753616116
Iteration: 189
Loss: 0.0021238566267039673
Iteration: 190
Loss: 0.0021186403392664628
Iteration: 191
Loss: 0.002114821084556572
Iteration: 192
Loss: 0.0021132800345780487
Iteration: 193
Loss: 0.002117451349342311
Iteration: 194
Loss: 0.0021026784763958904
Iteration: 195
Loss: 0.0021069003398426706
Iteration: 196
Loss: 0.0020907370554697183
Iteration: 197
Loss: 0.0020975696647332776
Iteration: 198
Loss: 0.002092974401297577
Iteration: 199
Loss: 0.0020857320733391393
Iteration: 200
Loss: 0.002085737270349062
Iteration: 201
Loss: 0.002100166363101223
Iteration: 202
Loss: 0.002082999196351954
Iteration: 203
Loss: 0.0020813021972706475
Iteration: 204
Loss: 0.002074489772411776
Iteration: 205
Loss: 0.0020654412406140273
Iteration: 206
Loss: 0.002077370642017527
Iteration: 207
Loss: 0.002063188264951294
Iteration: 208
Loss: 0.002069395285582653
Iteration: 209
Loss: 0.0020749474930243546
Iteration: 210
Loss: 0.002059377004093502
Iteration: 211
Loss: 0.0020524624959324246
Iteration: 212
Loss: 0.0020524932525730058
Iteration: 213
Loss: 0.0020595577625948706
Iteration: 214
Loss: 0.0020543014714034433
Iteration: 215
Loss: 0.00204628960798598
Iteration: 216
Loss: 0.0020408899635421455
Iteration: 217
Loss: 0.0020406509370163636
Iteration: 218
Loss: 0.0020459005839110894
Iteration: 219
Loss: 0.002043966931164449
Iteration: 220
Loss: 0.0020514475152954275
Iteration: 221
Loss: 0.0020480129945402346
Iteration: 222
Loss: 0.0020403953838265603
Iteration: 223
Loss: 0.0020296071040915484
Iteration: 224
Loss: 0.0020274174080609723
Iteration: 225
Loss: 0.002029183560717897
Iteration: 226
Loss: 0.0020346460220070535
Iteration: 227
Loss: 0.0020335099780587136
Iteration: 228
Loss: 0.002021962890320998
Iteration: 229
Loss: 0.0020393360805709237
Iteration: 230
Loss: 0.002027662025396655
Iteration: 231
Loss: 0.002028572023846209
Iteration: 232
Loss: 0.0020190985085741606
Iteration: 233
Loss: 0.00201434948924285
Iteration: 234
Loss: 0.002017417668226968
Iteration: 235
Loss: 0.0020251594480977936
Iteration: 236
Loss: 0.0020168817308903846
Iteration: 237
Loss: 0.002017560048477241
Iteration: 238
Loss: 0.0020089849260532195
Iteration: 239
Loss: 0.0020115248812937444
Iteration: 240
Loss: 0.0020182616692496305
Iteration: 241
Loss: 0.0020077616022324855
Iteration: 242
Loss: 0.002005811905729826
Iteration: 243
Loss: 0.002014355541402359
Iteration: 244
Loss: 0.0019950883428531663
Iteration: 245
Loss: 0.002009272773715633
Iteration: 246
Loss: 0.002009248812280503
Iteration: 247
Loss: 0.002003560558444372
Iteration: 248
Loss: 0.0019936893583723794
Iteration: 249
Loss: 0.0020004774854851907
Iteration: 250
Loss: 0.001998207743141662
Iteration: 251
Loss: 0.0020021852781352254
Iteration: 252
Loss: 0.0019848421187266523
Iteration: 253
Loss: 0.0020036136193982796
Iteration: 254
Loss: 0.0019985798984926977
Iteration: 255
Loss: 0.0019926677090058354
Iteration: 256
Loss: 0.0020003794062951647
Iteration: 257
Loss: 0.001998087862667478
Iteration: 258
Loss: 0.0020019265486755306
Iteration: 259
Loss: 0.001993895642010978
Iteration: 260
Loss: 0.0019951773991371377
Iteration: 261
Loss: 0.001978861659558283
Iteration: 262
Loss: 0.0019868743235107373
Iteration: 263
Loss: 0.0019947406792737267
Iteration: 264
Loss: 0.0020080165014843697
Iteration: 265
Loss: 0.0019884083971933083
Iteration: 266
Loss: 0.0019857021807520477
Iteration: 267
Loss: 0.001979809001456072
Iteration: 268
Loss: 0.0019762997636226594
Iteration: 269
Loss: 0.0019993426588674388
Iteration: 270
Loss: 0.0019851204821334026
Iteration: 271
Loss: 0.0019834448804756924
Iteration: 272
Loss: 0.0019840034526876276
Iteration: 273
Loss: 0.0019871967235281143
Iteration: 274
Loss: 0.0019824097743602813
Iteration: 275
Loss: 0.0019820760116326035
Iteration: 276
Loss: 0.001986243651409484
Iteration: 277
Loss: 0.0019802851818019043
Iteration: 278
Loss: 0.0019763739014987225
Iteration: 279
Loss: 0.00197197535696129
Iteration: 280
Loss: 0.0019755550346877656
Iteration: 281
Loss: 0.001974251515082555
Iteration: 282
Loss: 0.0019788829305059747
Iteration: 283
Loss: 0.0019763384551890655
Iteration: 284
Loss: 0.0019851601510136214
Iteration: 285
Loss: 0.0019699975123835933
Iteration: 286
Loss: 0.0019745763425749762
Iteration: 287
Loss: 0.0019691137835544385
Iteration: 288
Loss: 0.0019709394991765787
Iteration: 289
Loss: 0.0019709318933755528
Iteration: 290
Loss: 0.0019641859977172667
Iteration: 291
Loss: 0.0019800797777457368
Iteration: 292
Loss: 0.0019670233136203923
Iteration: 293
Loss: 0.001980051790640034
Iteration: 294
Loss: 0.0019570807868867744
Iteration: 295
Loss: 0.0019680997744823494
Iteration: 296
Loss: 0.001971143409954729
Iteration: 297
Loss: 0.0019717428439446254
Iteration: 298
Loss: 0.0019704250210643183
Iteration: 299
Loss: 0.0019554043415772877
Iteration: 300
Loss: 0.001967540088420113
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.21693121693121695
accuracy: 0.9610526315789474
confusion: 41 185 111 7263
precision: 0.18141592920353983
recall: 0.26973684210526316
Processing fold: ../../../kg_constructor/output/salmonella/folds/fold_3
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 84531
Starting training...
Iteration: 1
Loss: 0.7018665413797638
Iteration: 2
Loss: 0.6939603568595133
Iteration: 3
Loss: 0.6743395600789859
Iteration: 4
Loss: 0.6350473018340123
Iteration: 5
Loss: 0.5818962314982473
Iteration: 6
Loss: 0.521945184763567
Iteration: 7
Loss: 0.4714944495095147
Iteration: 8
Loss: 0.4296229360280214
Iteration: 9
Loss: 0.39808125775537373
Iteration: 10
Loss: 0.3720288633564372
Iteration: 11
Loss: 0.34972474310133195
Iteration: 12
Loss: 0.3327813582655824
Iteration: 13
Loss: 0.31600547388747885
Iteration: 14
Loss: 0.2999894015582991
Iteration: 15
Loss: 0.2848143364176338
Iteration: 16
Loss: 0.2703401472097562
Iteration: 17
Loss: 0.25609428463158784
Iteration: 18
Loss: 0.24249911749804462
Iteration: 19
Loss: 0.22892050316304335
Iteration: 20
Loss: 0.21537246803442636
Iteration: 21
Loss: 0.20176692803700766
Iteration: 22
Loss: 0.18916694194446376
Iteration: 23
Loss: 0.17674999049416296
Iteration: 24
Loss: 0.16449096191812446
Iteration: 25
Loss: 0.1530545824839745
Iteration: 26
Loss: 0.14219691043282734
Iteration: 27
Loss: 0.1315427784014631
Iteration: 28
Loss: 0.12195789813995361
Iteration: 29
Loss: 0.11257201709129193
Iteration: 30
Loss: 0.10409272618499803
Iteration: 31
Loss: 0.0959208122006169
Iteration: 32
Loss: 0.08868985705905491
Iteration: 33
Loss: 0.08183139912140222
Iteration: 34
Loss: 0.07567748390598061
Iteration: 35
Loss: 0.06981506575772792
Iteration: 36
Loss: 0.06467162785522731
Iteration: 37
Loss: 0.05952125318624355
Iteration: 38
Loss: 0.05508808672060201
Iteration: 39
Loss: 0.05104922728590024
Iteration: 40
Loss: 0.04748816305288562
Iteration: 41
Loss: 0.044147192787976915
Iteration: 42
Loss: 0.04097979483597072
Iteration: 43
Loss: 0.03806830436727147
Iteration: 44
Loss: 0.03554260289227521
Iteration: 45
Loss: 0.03336991516896236
Iteration: 46
Loss: 0.031158299817715163
Iteration: 47
Loss: 0.028996084713273577
Iteration: 48
Loss: 0.027269168032540217
Iteration: 49
Loss: 0.02571970518724418
Iteration: 50
Loss: 0.02423872754989583
Iteration: 51
Loss: 0.02280287819420114
Iteration: 52
Loss: 0.021556379457498775
Iteration: 53
Loss: 0.020400214986300763
Iteration: 54
Loss: 0.019353516560829717
Iteration: 55
Loss: 0.018248534933836373
Iteration: 56
Loss: 0.017347297964640605
Iteration: 57
Loss: 0.016557942776952262
Iteration: 58
Loss: 0.015724375418582816
Iteration: 59
Loss: 0.014966450838579072
Iteration: 60
Loss: 0.014376907241473228
Iteration: 61
Loss: 0.013737593191089454
Iteration: 62
Loss: 0.013159986861326077
Iteration: 63
Loss: 0.012576519994548074
Iteration: 64
Loss: 0.012022798915428144
Iteration: 65
Loss: 0.011585243345226771
Iteration: 66
Loss: 0.011046300009812838
Iteration: 67
Loss: 0.010660789182616605
Iteration: 68
Loss: 0.010292774503245766
Iteration: 69
Loss: 0.00994666038012063
Iteration: 70
Loss: 0.00955409833724484
Iteration: 71
Loss: 0.009233488356349644
Iteration: 72
Loss: 0.008947764074912778
Iteration: 73
Loss: 0.008581695399810503
Iteration: 74
Loss: 0.00835859718054165
Iteration: 75
Loss: 0.008084665207813183
Iteration: 76
Loss: 0.007817969163074905
Iteration: 77
Loss: 0.007588067666891916
Iteration: 78
Loss: 0.007353909986300601
Iteration: 79
Loss: 0.007162974129810378
Iteration: 80
Loss: 0.006930566668786385
Iteration: 81
Loss: 0.00675304799720093
Iteration: 82
Loss: 0.006545706813074189
Iteration: 83
Loss: 0.006414037962432628
Iteration: 84
Loss: 0.006216082595472718
Iteration: 85
Loss: 0.006058844628479378
Iteration: 86
Loss: 0.005939724446952711
Iteration: 87
Loss: 0.005789097680215851
Iteration: 88
Loss: 0.005669288210754777
Iteration: 89
Loss: 0.005539144274353245
Iteration: 90
Loss: 0.00539463494019008
Iteration: 91
Loss: 0.0053043945657986185
Iteration: 92
Loss: 0.005169479958253142
Iteration: 93
Loss: 0.005074380247359291
Iteration: 94
Loss: 0.004929405073692769
Iteration: 95
Loss: 0.00486559404152227
Iteration: 96
Loss: 0.004767110536771792
Iteration: 97
Loss: 0.004659834211508249
Iteration: 98
Loss: 0.004577095557096196
Iteration: 99
Loss: 0.004510741928063425
Iteration: 100
Loss: 0.004418920515749006
Iteration: 101
Loss: 0.0043336613086500655
Iteration: 102
Loss: 0.004267149259922681
Iteration: 103
Loss: 0.0041826837035011
Iteration: 104
Loss: 0.004115618975189181
Iteration: 105
Loss: 0.004019398279603065
Iteration: 106
Loss: 0.003964046772316467
Iteration: 107
Loss: 0.003885934879589412
Iteration: 108
Loss: 0.0038234481766222066
Iteration: 109
Loss: 0.003775875799840799
Iteration: 110
Loss: 0.003699884468252644
Iteration: 111
Loss: 0.003633152799864794
Iteration: 112
Loss: 0.003599622028155459
Iteration: 113
Loss: 0.003519626667746055
Iteration: 114
Loss: 0.0034755540459796234
Iteration: 115
Loss: 0.0034213989978817143
Iteration: 116
Loss: 0.003371793426067373
Iteration: 117
Loss: 0.003336263452599078
Iteration: 118
Loss: 0.003281931467584254
Iteration: 119
Loss: 0.0032500740238407878
Iteration: 120
Loss: 0.0031791842703558044
Iteration: 121
Loss: 0.0031534479844763322
Iteration: 122
Loss: 0.0031214924414216735
Iteration: 123
Loss: 0.0030555919094448106
Iteration: 124
Loss: 0.003041794106801167
Iteration: 125
Loss: 0.0030082893373881594
Iteration: 126
Loss: 0.0029554492104108686
Iteration: 127
Loss: 0.002917390359841563
Iteration: 128
Loss: 0.0029014405121819842
Iteration: 129
Loss: 0.0028582607211982028
Iteration: 130
Loss: 0.0028199409163430517
Iteration: 131
Loss: 0.002794540492978729
Iteration: 132
Loss: 0.00278064439556113
Iteration: 133
Loss: 0.0027410389602552225
Iteration: 134
Loss: 0.0027162258133844094
Iteration: 135
Loss: 0.0027067279356911226
Iteration: 136
Loss: 0.002664498205837092
Iteration: 137
Loss: 0.002665093007646961
Iteration: 138
Loss: 0.0026354078565620715
Iteration: 139
Loss: 0.002610416976551031
Iteration: 140
Loss: 0.0025803758992733045
Iteration: 141
Loss: 0.0025544736022704547
Iteration: 142
Loss: 0.0025425744242966175
Iteration: 143
Loss: 0.0025347779263500815
Iteration: 144
Loss: 0.002491567286743242
Iteration: 145
Loss: 0.0024938855670898416
Iteration: 146
Loss: 0.0024839007887805316
Iteration: 147
Loss: 0.002454684808689319
Iteration: 148
Loss: 0.0024585877803878654
Iteration: 149
Loss: 0.0024362510346152165
Iteration: 150
Loss: 0.002426223929994452
Iteration: 151
Loss: 0.0023907756168441274
Iteration: 152
Loss: 0.002384008928834472
Iteration: 153
Loss: 0.0023798461785011086
Iteration: 154
Loss: 0.002366219825444766
Iteration: 155
Loss: 0.0023544887626934566
Iteration: 156
Loss: 0.0023470615255243984
Iteration: 157
Loss: 0.002324480474294152
Iteration: 158
Loss: 0.002325518013633512
Iteration: 159
Loss: 0.002302494555435799
Iteration: 160
Loss: 0.0023022265328715243
Iteration: 161
Loss: 0.002293528641148666
Iteration: 162
Loss: 0.0022912493613721045
Iteration: 163
Loss: 0.002271689421405303
Iteration: 164
Loss: 0.00226061181222767
Iteration: 165
Loss: 0.0022693894125153254
Iteration: 166
Loss: 0.002261803068657532
Iteration: 167
Loss: 0.0022361886423127152
Iteration: 168
Loss: 0.0022414213845529307
Iteration: 169
Loss: 0.0022321175482055105
Iteration: 170
Loss: 0.00221847445577567
Iteration: 171
Loss: 0.002219048274083086
Iteration: 172
Loss: 0.0022011475499581407
Iteration: 173
Loss: 0.002211382256153925
Iteration: 174
Loss: 0.00221890091240682
Iteration: 175
Loss: 0.0021962775937913927
Iteration: 176
Loss: 0.002195936903633453
Iteration: 177
Loss: 0.0021857441328235984
Iteration: 178
Loss: 0.0021858552979581334
Iteration: 179
Loss: 0.002172790223410652
Iteration: 180
Loss: 0.002167552850427635
Iteration: 181
Loss: 0.0021596234809193346
Iteration: 182
Loss: 0.00215596435620323
Iteration: 183
Loss: 0.002149514337990111
Iteration: 184
Loss: 0.0021512608653233376
Iteration: 185
Loss: 0.002143733736622021
Iteration: 186
Loss: 0.0021363910974612757
Iteration: 187
Loss: 0.002140928962394411
Iteration: 188
Loss: 0.0021241778927498763
Iteration: 189
Loss: 0.002131237836704118
Iteration: 190
Loss: 0.002136668769562226
Iteration: 191
Loss: 0.0021241613531508196
Iteration: 192
Loss: 0.002118171940063253
Iteration: 193
Loss: 0.0021147702843594698
Iteration: 194
Loss: 0.0021138484446032915
Iteration: 195
Loss: 0.0021052503797375126
Iteration: 196
Loss: 0.002089994571482142
Iteration: 197
Loss: 0.0020959952452369495
Iteration: 198
Loss: 0.0020919809689929274
Iteration: 199
Loss: 0.00209328728687754
Iteration: 200
Loss: 0.0020888050641565594
Iteration: 201
Loss: 0.0020750671967197164
Iteration: 202
Loss: 0.0020842667824278274
Iteration: 203
Loss: 0.0020751871720508294
Iteration: 204
Loss: 0.0020930514854873403
Iteration: 205
Loss: 0.002074190319147835
Iteration: 206
Loss: 0.002084667241074329
Iteration: 207
Loss: 0.0020720990702371906
Iteration: 208
Loss: 0.002065204885891742
Iteration: 209
Loss: 0.0020642922989978097
Iteration: 210
Loss: 0.0020613298409353988
Iteration: 211
Loss: 0.0020580144504223157
Iteration: 212
Loss: 0.0020690479554396905
Iteration: 213
Loss: 0.0020502271765935015
Iteration: 214
Loss: 0.002069338289503422
Iteration: 215
Loss: 0.0020571904412160316
Iteration: 216
Loss: 0.002046583010527639
Iteration: 217
Loss: 0.002046018459454731
Iteration: 218
Loss: 0.002054330282039756
Iteration: 219
Loss: 0.0020374491577968
Iteration: 220
Loss: 0.002049507360537479
Iteration: 221
Loss: 0.002045114911276341
Iteration: 222
Loss: 0.002039986612263745
Iteration: 223
Loss: 0.0020299543638104276
Iteration: 224
Loss: 0.002048320105345345
Iteration: 225
Loss: 0.0020251103596370897
Iteration: 226
Loss: 0.0020400897217454548
Iteration: 227
Loss: 0.00203311648995926
Iteration: 228
Loss: 0.0020259741412038787
Iteration: 229
Loss: 0.002039507004133437
Iteration: 230
Loss: 0.002024939405892826
Iteration: 231
Loss: 0.0020274790751253382
Iteration: 232
Loss: 0.002029190391854004
Iteration: 233
Loss: 0.002023079136478496
Iteration: 234
Loss: 0.0020247650028056936
Iteration: 235
Loss: 0.002016606690350598
Iteration: 236
Loss: 0.002017399460008299
Iteration: 237
Loss: 0.0020166214176074223
Iteration: 238
Loss: 0.002016398097089503
Iteration: 239
Loss: 0.002007490884869095
Iteration: 240
Loss: 0.0019954192167561916
Iteration: 241
Loss: 0.002009261159490748
Iteration: 242
Loss: 0.0020116446985299755
Iteration: 243
Loss: 0.0020076377622001334
Iteration: 244
Loss: 0.002012880927204718
Iteration: 245
Loss: 0.002005236594144393
Iteration: 246
Loss: 0.002020558943098158
Iteration: 247
Loss: 0.0020134074406491387
Iteration: 248
Loss: 0.0019950992657722516
Iteration: 249
Loss: 0.001997838650519649
Iteration: 250
Loss: 0.0019951314452667664
Iteration: 251
Loss: 0.0019942616402849923
Iteration: 252
Loss: 0.0020045361748931034
Iteration: 253
Loss: 0.0019992610862202666
Iteration: 254
Loss: 0.0019966434632761425
Iteration: 255
Loss: 0.0020082451066830093
Iteration: 256
Loss: 0.001988299040089327
Iteration: 257
Loss: 0.001991580967664903
Iteration: 258
Loss: 0.002000437738994757
Iteration: 259
Loss: 0.0019903340532133975
Iteration: 260
Loss: 0.001987633131513441
Iteration: 261
Loss: 0.0019968398171189574
Iteration: 262
Loss: 0.0019796843120230383
Iteration: 263
Loss: 0.002001984448770038
Iteration: 264
Loss: 0.0019832305469331736
Iteration: 265
Loss: 0.0019800948384390386
Iteration: 266
Loss: 0.0019873535737717227
Iteration: 267
Loss: 0.001988804874723248
Iteration: 268
Loss: 0.001982006386646794
Iteration: 269
Loss: 0.0019718051750074934
Iteration: 270
Loss: 0.0019793879128647625
Iteration: 271
Loss: 0.0019800742214292656
Iteration: 272
Loss: 0.0019774713918543707
Iteration: 273
Loss: 0.0019815730744466922
Iteration: 274
Loss: 0.0019829252125396404
Iteration: 275
Loss: 0.00198367119329855
Iteration: 276
Loss: 0.0019762380376320194
Iteration: 277
Loss: 0.0019791756561140955
Iteration: 278
Loss: 0.0019826484530773244
Iteration: 279
Loss: 0.0019829495735236525
Iteration: 280
Loss: 0.0019683532839365027
Iteration: 281
Loss: 0.001973221227986577
Iteration: 282
Loss: 0.0019764882457776016
Iteration: 283
Loss: 0.0019817964337697186
Iteration: 284
Loss: 0.0019772192621373653
Iteration: 285
Loss: 0.0019731137723328525
Iteration: 286
Loss: 0.0019734585513993177
Iteration: 287
Loss: 0.001967465446077655
Iteration: 288
Loss: 0.001965990620607763
Iteration: 289
Loss: 0.001961355431028354
Iteration: 290
Loss: 0.001967513226662521
Iteration: 291
Loss: 0.001965564677066365
Iteration: 292
Loss: 0.001968942249170792
Iteration: 293
Loss: 0.0019678451784851925
Iteration: 294
Loss: 0.0019683110381097154
Iteration: 295
Loss: 0.0019771747713632605
Iteration: 296
Loss: 0.0019778969444702436
Iteration: 297
Loss: 0.001970946202399554
Iteration: 298
Loss: 0.001967418230322684
Iteration: 299
Loss: 0.001965670589139156
Iteration: 300
Loss: 0.0019611063367330735
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.16547788873038516
accuracy: 0.9230263157894737
confusion: 58 491 94 6957
precision: 0.10564663023679417
recall: 0.3815789473684211
Processing fold: ../../../kg_constructor/output/salmonella/folds/fold_4
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 84531
Starting training...
Iteration: 1
Loss: 0.7017435519783585
Iteration: 2
Loss: 0.6943133539623685
Iteration: 3
Loss: 0.6750649728892762
Iteration: 4
Loss: 0.6373953694178734
Iteration: 5
Loss: 0.5838570028175543
Iteration: 6
Loss: 0.5214809146192338
Iteration: 7
Loss: 0.4707769008330357
Iteration: 8
Loss: 0.43051239480206993
Iteration: 9
Loss: 0.398136521195188
Iteration: 10
Loss: 0.3721492695219723
Iteration: 11
Loss: 0.35114587751435644
Iteration: 12
Loss: 0.3332529292430407
Iteration: 13
Loss: 0.31657791836762134
Iteration: 14
Loss: 0.3007166098665308
Iteration: 15
Loss: 0.2861804859137829
Iteration: 16
Loss: 0.27147789685814466
Iteration: 17
Loss: 0.2572532229953342
Iteration: 18
Loss: 0.24300803391285886
Iteration: 19
Loss: 0.228659627246268
Iteration: 20
Loss: 0.21561854948968062
Iteration: 21
Loss: 0.20204733514491421
Iteration: 22
Loss: 0.18906119760171866
Iteration: 23
Loss: 0.17658503997473068
Iteration: 24
Loss: 0.1641034765375985
Iteration: 25
Loss: 0.15247754016776144
Iteration: 26
Loss: 0.14144731395774418
Iteration: 27
Loss: 0.13128481023473504
Iteration: 28
Loss: 0.12080532064040501
Iteration: 29
Loss: 0.11187963711994665
Iteration: 30
Loss: 0.10336080009554639
Iteration: 31
Loss: 0.09515092429923422
Iteration: 32
Loss: 0.08762271913849277
Iteration: 33
Loss: 0.08133182564267406
Iteration: 34
Loss: 0.07467443146455435
Iteration: 35
Loss: 0.06900135022990497
Iteration: 36
Loss: 0.06381314013291288
Iteration: 37
Loss: 0.058833591318056905
Iteration: 38
Loss: 0.05456568098362581
Iteration: 39
Loss: 0.05059442471390889
Iteration: 40
Loss: 0.04691366791541194
Iteration: 41
Loss: 0.04378653681020678
Iteration: 42
Loss: 0.040721001402463444
Iteration: 43
Loss: 0.037842500816892694
Iteration: 44
Loss: 0.035263058028103395
Iteration: 45
Loss: 0.0328664924075574
Iteration: 46
Loss: 0.030798898135990273
Iteration: 47
Loss: 0.028907588878531516
Iteration: 48
Loss: 0.027230345264629082
Iteration: 49
Loss: 0.025504915181685378
Iteration: 50
Loss: 0.023955296725034714
Iteration: 51
Loss: 0.02263213604412697
Iteration: 52
Loss: 0.021342236594653424
Iteration: 53
Loss: 0.020298338138762816
Iteration: 54
Loss: 0.019079772732508035
Iteration: 55
Loss: 0.01809203831685914
Iteration: 56
Loss: 0.017128769952205965
Iteration: 57
Loss: 0.016363784866292537
Iteration: 58
Loss: 0.015610931970087099
Iteration: 59
Loss: 0.014898941323253107
Iteration: 60
Loss: 0.014209555174557515
Iteration: 61
Loss: 0.013531174993625394
Iteration: 62
Loss: 0.01301206067710379
Iteration: 63
Loss: 0.012462621865173181
Iteration: 64
Loss: 0.011933335080098958
Iteration: 65
Loss: 0.011464585550129414
Iteration: 66
Loss: 0.010962886793285977
Iteration: 67
Loss: 0.010611937933222012
Iteration: 68
Loss: 0.010258572118609775
Iteration: 69
Loss: 0.009827501542958212
Iteration: 70
Loss: 0.009488357506968357
Iteration: 71
Loss: 0.00918034736044061
Iteration: 72
Loss: 0.008852960334883796
Iteration: 73
Loss: 0.008570491773202831
Iteration: 74
Loss: 0.008271988987186809
Iteration: 75
Loss: 0.008032813145100702
Iteration: 76
Loss: 0.00781259779461924
Iteration: 77
Loss: 0.007585303472745933
Iteration: 78
Loss: 0.00735407859592894
Iteration: 79
Loss: 0.007100382566819956
Iteration: 80
Loss: 0.006925894436139016
Iteration: 81
Loss: 0.006717059996990878
Iteration: 82
Loss: 0.006585458816707503
Iteration: 83
Loss: 0.006387317501422431
Iteration: 84
Loss: 0.006207646347904279
Iteration: 85
Loss: 0.006095145489836549
Iteration: 86
Loss: 0.0059303936410557345
Iteration: 87
Loss: 0.005784916444279161
Iteration: 88
Loss: 0.005654329899698496
Iteration: 89
Loss: 0.005509775402139367
Iteration: 90
Loss: 0.005396271808601824
Iteration: 91
Loss: 0.005283255584216044
Iteration: 92
Loss: 0.005176297659169377
Iteration: 93
Loss: 0.0050275212176788
Iteration: 94
Loss: 0.004929979239808925
Iteration: 95
Loss: 0.004862292474250735
Iteration: 96
Loss: 0.004754375511159499
Iteration: 97
Loss: 0.004639984614411254
Iteration: 98
Loss: 0.004568697140195671
Iteration: 99
Loss: 0.004476546479481054
Iteration: 100
Loss: 0.004386512275187321
Iteration: 101
Loss: 0.004309233919039
Iteration: 102
Loss: 0.004220031244956233
Iteration: 103
Loss: 0.004161270254831991
Iteration: 104
Loss: 0.004067161324560458
Iteration: 105
Loss: 0.0040082417595211734
Iteration: 106
Loss: 0.00390455912925119
Iteration: 107
Loss: 0.0038567198193229643
Iteration: 108
Loss: 0.0037889469495434084
Iteration: 109
Loss: 0.003740419929755139
Iteration: 110
Loss: 0.0036791737858252026
Iteration: 111
Loss: 0.0036113124720200344
Iteration: 112
Loss: 0.0035529196906236953
Iteration: 113
Loss: 0.003502498090520133
Iteration: 114
Loss: 0.0034419212800761065
Iteration: 115
Loss: 0.0033920553317408502
Iteration: 116
Loss: 0.0033363486686146552
Iteration: 117
Loss: 0.0033073477780469407
Iteration: 118
Loss: 0.0032560167443237186
Iteration: 119
Loss: 0.003181550257988734
Iteration: 120
Loss: 0.0031834530534889595
Iteration: 121
Loss: 0.0031208505072159534
Iteration: 122
Loss: 0.0030954191288738338
Iteration: 123
Loss: 0.003047627028951674
Iteration: 124
Loss: 0.0030036586865690757
Iteration: 125
Loss: 0.0029715522175171863
Iteration: 126
Loss: 0.002934833433778014
Iteration: 127
Loss: 0.002894838271593606
Iteration: 128
Loss: 0.002870157297792626
Iteration: 129
Loss: 0.002834712632322385
Iteration: 130
Loss: 0.0028023130898536357
Iteration: 131
Loss: 0.0027692941028946714
Iteration: 132
Loss: 0.002751972739021351
Iteration: 133
Loss: 0.0027223336999017147
Iteration: 134
Loss: 0.0027056644227999595
Iteration: 135
Loss: 0.00267277171915788
Iteration: 136
Loss: 0.002657694946169669
Iteration: 137
Loss: 0.0026336559324444814
Iteration: 138
Loss: 0.002609630434394435
Iteration: 139
Loss: 0.0025856297658641398
Iteration: 140
Loss: 0.0025723014877718172
Iteration: 141
Loss: 0.002547869349565403
Iteration: 142
Loss: 0.0025302878161317403
Iteration: 143
Loss: 0.002502978144121575
Iteration: 144
Loss: 0.0025053441777457427
Iteration: 145
Loss: 0.002484471836115843
Iteration: 146
Loss: 0.0024509713122690163
Iteration: 147
Loss: 0.0024580394441008936
Iteration: 148
Loss: 0.0024247265989995664
Iteration: 149
Loss: 0.00241593883752271
Iteration: 150
Loss: 0.0023891695050729644
Iteration: 151
Loss: 0.002389442471695351
Iteration: 152
Loss: 0.0023944513343366574
Iteration: 153
Loss: 0.0023701573020698113
Iteration: 154
Loss: 0.002366708651943891
Iteration: 155
Loss: 0.002341595055811383
Iteration: 156
Loss: 0.002342093958703364
Iteration: 157
Loss: 0.00232072109873924
Iteration: 158
Loss: 0.0023163121688053196
Iteration: 159
Loss: 0.0023153148028125735
Iteration: 160
Loss: 0.0023107854505701932
Iteration: 161
Loss: 0.0022880944975272374
Iteration: 162
Loss: 0.002286932301811046
Iteration: 163
Loss: 0.002261918805608595
Iteration: 164
Loss: 0.0022602992988892913
Iteration: 165
Loss: 0.0022534902155813244
Iteration: 166
Loss: 0.002253492829895774
Iteration: 167
Loss: 0.002241362051543906
Iteration: 168
Loss: 0.0022305814221639324
Iteration: 169
Loss: 0.0022273647072897466
Iteration: 170
Loss: 0.0022137171220908193
Iteration: 171
Loss: 0.0022243913924206555
Iteration: 172
Loss: 0.0022088473656985126
Iteration: 173
Loss: 0.00219773424697327
Iteration: 174
Loss: 0.0021983886994191527
Iteration: 175
Loss: 0.0022020613469965295
Iteration: 176
Loss: 0.0021833412257241613
Iteration: 177
Loss: 0.002187614148068759
Iteration: 178
Loss: 0.0021691813584946004
Iteration: 179
Loss: 0.0021764413284045864
Iteration: 180
Loss: 0.0021623015880906657
Iteration: 181
Loss: 0.002155096955700881
Iteration: 182
Loss: 0.002156468055119025
Iteration: 183
Loss: 0.0021485510040764456
Iteration: 184
Loss: 0.002141348175405536
Iteration: 185
Loss: 0.0021368847084864053
Iteration: 186
Loss: 0.002147716228608732
Iteration: 187
Loss: 0.002130445493902597
Iteration: 188
Loss: 0.0021267758716863616
Iteration: 189
Loss: 0.0021358885080841037
Iteration: 190
Loss: 0.002124876248144349
Iteration: 191
Loss: 0.0021257762133178337
Iteration: 192
Loss: 0.002118388147748731
Iteration: 193
Loss: 0.0021129275879098307
Iteration: 194
Loss: 0.002098553631466204
Iteration: 195
Loss: 0.002108577429029493
Iteration: 196
Loss: 0.002112833696796938
Iteration: 197
Loss: 0.002109101681717108
Iteration: 198
Loss: 0.0021012330220805276
Iteration: 199
Loss: 0.0020858204081355605
Iteration: 200
Loss: 0.002084645778975553
Iteration: 201
Loss: 0.0020887523251413194
Iteration: 202
Loss: 0.0020816449656576655
Iteration: 203
Loss: 0.00208379019247253
Iteration: 204
Loss: 0.00208686003289013
Iteration: 205
Loss: 0.002078114502867799
Iteration: 206
Loss: 0.0020771637346227597
Iteration: 207
Loss: 0.0020702391673155405
Iteration: 208
Loss: 0.0020837937093650303
Iteration: 209
Loss: 0.0020687407023491865
Iteration: 210
Loss: 0.002072831147269886
Iteration: 211
Loss: 0.002062621016204817
Iteration: 212
Loss: 0.0020567415696051386
Iteration: 213
Loss: 0.002062925358912275
Iteration: 214
Loss: 0.002061508903509857
Iteration: 215
Loss: 0.002068431484570474
Iteration: 216
Loss: 0.0020570407181771267
Iteration: 217
Loss: 0.0020612548421608443
Iteration: 218
Loss: 0.0020466250896361876
Iteration: 219
Loss: 0.0020470386442877813
Iteration: 220
Loss: 0.0020586773192647983
Iteration: 221
Loss: 0.0020370565363447423
Iteration: 222
Loss: 0.0020336045187978463
Iteration: 223
Loss: 0.0020434772983814278
Iteration: 224
Loss: 0.002048581125743595
Iteration: 225
Loss: 0.0020305509492180044
Iteration: 226
Loss: 0.0020302439519537634
Iteration: 227
Loss: 0.002039667586853475
Iteration: 228
Loss: 0.0020323295381931977
Iteration: 229
Loss: 0.0020138122224720354
Iteration: 230
Loss: 0.002028383994417518
Iteration: 231
Loss: 0.002024592039819209
Iteration: 232
Loss: 0.002033385856330027
Iteration: 233
Loss: 0.0020246591855898315
Iteration: 234
Loss: 0.002033077661856365
Iteration: 235
Loss: 0.002024554408925735
Iteration: 236
Loss: 0.002014267538605189
Iteration: 237
Loss: 0.0020277855822954465
Iteration: 238
Loss: 0.002020498516359999
Iteration: 239
Loss: 0.0020301548238084825
Iteration: 240
Loss: 0.002012873850302932
Iteration: 241
Loss: 0.0020149287891309754
Iteration: 242
Loss: 0.0020274725371833753
Iteration: 243
Loss: 0.002019041381707346
Iteration: 244
Loss: 0.002006509224884212
Iteration: 245
Loss: 0.002010606528248316
Iteration: 246
Loss: 0.002013318897454919
Iteration: 247
Loss: 0.002008712062911119
Iteration: 248
Loss: 0.002010141341245653
Iteration: 249
Loss: 0.00200507154452534
Iteration: 250
Loss: 0.0019940127012101406
Iteration: 251
Loss: 0.0019958929049518005
Iteration: 252
Loss: 0.002002906351764169
Iteration: 253
Loss: 0.0020065316197427885
Iteration: 254
Loss: 0.002001224194917782
Iteration: 255
Loss: 0.0020067379134419707
Iteration: 256
Loss: 0.0020040281628614957
Iteration: 257
Loss: 0.0019962733315395903
Iteration: 258
Loss: 0.0019771174806526596
Iteration: 259
Loss: 0.001996919008282324
Iteration: 260
Loss: 0.0019910347254164977
Iteration: 261
Loss: 0.0019937420312752144
Iteration: 262
Loss: 0.0019876863519994563
Iteration: 263
Loss: 0.0019932154603417456
Iteration: 264
Loss: 0.0019939798447820875
Iteration: 265
Loss: 0.0019869329709628664
Iteration: 266
Loss: 0.0019874212383810016
Iteration: 267
Loss: 0.0019997723736903733
Iteration: 268
Loss: 0.001993019490238325
Iteration: 269
Loss: 0.0019819746009522568
Iteration: 270
Loss: 0.0019913022909428418
Iteration: 271
Loss: 0.0019724093935233945
Iteration: 272
Loss: 0.0019869640049881036
Iteration: 273
Loss: 0.0019793776826887034
Iteration: 274
Loss: 0.0019790654999123497
Iteration: 275
Loss: 0.001969579486521306
Iteration: 276
Loss: 0.001977028162793521
Iteration: 277
Loss: 0.0019767954758724863
Iteration: 278
Loss: 0.001985995179433146
Iteration: 279
Loss: 0.001976194113125036
Iteration: 280
Loss: 0.001980577453246547
Iteration: 281
Loss: 0.0019809615533844926
Iteration: 282
Loss: 0.001968778852361864
Iteration: 283
Loss: 0.00197228346520138
Iteration: 284
Loss: 0.0019798354377947103
Iteration: 285
Loss: 0.001970873411204436
Iteration: 286
Loss: 0.0019777025165105305
Iteration: 287
Loss: 0.0019761908463098938
Iteration: 288
Loss: 0.001960241848050996
Iteration: 289
Loss: 0.001961412315003942
Iteration: 290
Loss: 0.001968563926682152
Iteration: 291
Loss: 0.0019576812916101867
Iteration: 292
Loss: 0.0019646319431356257
Iteration: 293
Loss: 0.0019602210873186036
Iteration: 294
Loss: 0.001964105863733516
Iteration: 295
Loss: 0.0019742641813570152
Iteration: 296
Loss: 0.001966551000850252
Iteration: 297
Loss: 0.0019609184251569304
Iteration: 298
Loss: 0.001963793202360839
Iteration: 299
Loss: 0.0019582728093581986
Iteration: 300
Loss: 0.0019625379577463057
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.15230460921843686
accuracy: 0.9443421052631579
confusion: 38 309 114 7139
precision: 0.10951008645533142
recall: 0.25
Finding results in directory: ../../output/ecoli/tucker
Found 1 files in the directory.
Hyperparameters search result sorted by f1:
                                     hyperparameters        f1
0  (3, 128, 0.0002, 1.0, 200, 100, 0.2, 0.4, 0.5,...  0.042807
1  (3, 128, 0.0002, 1.0, 200, 30, 0.2, 0.4, 0.5, ...  0.040777
num_iterations: [300]
batch_size: [128]
learning_rate: [0.0002]
decay_rate: [1.0]
ent_vec_dim: [200]
rel_vec_dim: [30]
input_dropout: [0.2]
hidden_dropout1: [0.4]
hidden_dropout2: [0.5]
label_smoothing: [0.1]
Loading train data...
Loading val data...
Loading test data...
Training the TuckER model...
Number of training data points: 84683
Starting training...
Iteration: 1
Loss: 0.7017439552923528
Iteration: 2
Loss: 0.6940881624454405
Iteration: 3
Loss: 0.6745423972606659
Iteration: 4
Loss: 0.6344567333779684
Iteration: 5
Loss: 0.578591014553861
Iteration: 6
Loss: 0.5193008442477483
Iteration: 7
Loss: 0.46934218086847446
Iteration: 8
Loss: 0.4274947098115595
Iteration: 9
Loss: 0.3959887976326594
Iteration: 10
Loss: 0.3711576803428371
Iteration: 11
Loss: 0.3497200390187705
Iteration: 12
Loss: 0.33109715435563064
Iteration: 13
Loss: 0.3167146357094369
Iteration: 14
Loss: 0.3001584759572657
Iteration: 15
Loss: 0.2847579819400136
Iteration: 16
Loss: 0.26984025247213317
Iteration: 17
Loss: 0.2559118985039432
Iteration: 18
Loss: 0.24164348804369207
Iteration: 19
Loss: 0.22797666835348782
Iteration: 20
Loss: 0.21433797624053025
Iteration: 21
Loss: 0.2009552057559897
Iteration: 22
Loss: 0.18757567409335113
Iteration: 23
Loss: 0.174311183938166
Iteration: 24
Loss: 0.16259226275653374
Iteration: 25
Loss: 0.15087372446205558
Iteration: 26
Loss: 0.13962662656132768
Iteration: 27
Loss: 0.1288971578384318
Iteration: 28
Loss: 0.11882216992174707
Iteration: 29
Loss: 0.10990891287603029
Iteration: 30
Loss: 0.10162254068546178
Iteration: 31
Loss: 0.09362611637973203
Iteration: 32
Loss: 0.08575073257088661
Iteration: 33
Loss: 0.07930938417955143
Iteration: 34
Loss: 0.07295845149130356
Iteration: 35
Loss: 0.06723424151721524
Iteration: 36
Loss: 0.06199359521269798
Iteration: 37
Loss: 0.057469388424623305
Iteration: 38
Loss: 0.05300471890808606
Iteration: 39
Loss: 0.04936838354461077
Iteration: 40
Loss: 0.04557671307063684
Iteration: 41
Loss: 0.04229826489236296
Iteration: 42
Loss: 0.039461359109093506
Iteration: 43
Loss: 0.03664357028901577
Iteration: 44
Loss: 0.03419385387039766
Iteration: 45
Loss: 0.03213454035633221
Iteration: 46
Loss: 0.029943438905586556
Iteration: 47
Loss: 0.02796986612786607
Iteration: 48
Loss: 0.026377597642017574
Iteration: 49
Loss: 0.02463719264672297
Iteration: 50
Loss: 0.023243236809787227
Iteration: 51
Loss: 0.02198954635277027
Iteration: 52
Loss: 0.02089625156325538
Iteration: 53
Loss: 0.019638826071125704
Iteration: 54
Loss: 0.01855518096467344
Iteration: 55
Loss: 0.01765661380003865
Iteration: 56
Loss: 0.01673507104377921
Iteration: 57
Loss: 0.015999972456839026
Iteration: 58
Loss: 0.01525552490180949
Iteration: 59
Loss: 0.014509972162181285
Iteration: 60
Loss: 0.013881905629049713
Iteration: 61
Loss: 0.013239677961370567
Iteration: 62
Loss: 0.012700324798593433
Iteration: 63
Loss: 0.012210601929393484
Iteration: 64
Loss: 0.011702418156967657
Iteration: 65
Loss: 0.011150351827737034
Iteration: 66
Loss: 0.01077966911082224
Iteration: 67
Loss: 0.010321060581723364
Iteration: 68
Loss: 0.009938343666584753
Iteration: 69
Loss: 0.009587249003078153
Iteration: 70
Loss: 0.009254140336429927
Iteration: 71
Loss: 0.008953086770616653
Iteration: 72
Loss: 0.008618640225017217
Iteration: 73
Loss: 0.008365677385704547
Iteration: 74
Loss: 0.008076510925936262
Iteration: 75
Loss: 0.007828766622058139
Iteration: 76
Loss: 0.00760672525957045
Iteration: 77
Loss: 0.007393308830015906
Iteration: 78
Loss: 0.007200821308509969
Iteration: 79
Loss: 0.0069646760083098965
Iteration: 80
Loss: 0.006778002239582016
Iteration: 81
Loss: 0.006582882380249296
Iteration: 82
Loss: 0.006405448955597311
Iteration: 83
Loss: 0.006285583549292713
Iteration: 84
Loss: 0.006075166177186297
Iteration: 85
Loss: 0.005938077038835462
Iteration: 86
Loss: 0.005802300366822903
Iteration: 87
Loss: 0.005625517476668082
Iteration: 88
Loss: 0.005509630129559011
Iteration: 89
Loss: 0.0053709715309484705
Iteration: 90
Loss: 0.005274341719951935
Iteration: 91
Loss: 0.005155071167547892
Iteration: 92
Loss: 0.0050412968948210886
Iteration: 93
Loss: 0.0049458337981799026
Iteration: 94
Loss: 0.00484764511787855
Iteration: 95
Loss: 0.004729665253629408
Iteration: 96
Loss: 0.004648894730319337
Iteration: 97
Loss: 0.004543185129012094
Iteration: 98
Loss: 0.004463287970482758
Iteration: 99
Loss: 0.004378093275945724
Iteration: 100
Loss: 0.004264149045367248
Iteration: 101
Loss: 0.004189542298795792
Iteration: 102
Loss: 0.004116275918488278
Iteration: 103
Loss: 0.004053895762076647
Iteration: 104
Loss: 0.003984399733315335
Iteration: 105
Loss: 0.003908136530165992
Iteration: 106
Loss: 0.003855118522329665
Iteration: 107
Loss: 0.0037742121167844388
Iteration: 108
Loss: 0.0037232139261394012
Iteration: 109
Loss: 0.003653476858034548
Iteration: 110
Loss: 0.003587761726343959
Iteration: 111
Loss: 0.0035464393752967804
Iteration: 112
Loss: 0.0034870149158849947
Iteration: 113
Loss: 0.0034338185796514153
Iteration: 114
Loss: 0.0033752763780151925
Iteration: 115
Loss: 0.0033355699795320996
Iteration: 116
Loss: 0.0032898191754456335
Iteration: 117
Loss: 0.0032509232916664785
Iteration: 118
Loss: 0.003188227226662381
Iteration: 119
Loss: 0.003153420115889209
Iteration: 120
Loss: 0.003119046853787107
Iteration: 121
Loss: 0.0030565097524852653
Iteration: 122
Loss: 0.003035258734598756
Iteration: 123
Loss: 0.002996618680587811
Iteration: 124
Loss: 0.002952744264337348
Iteration: 125
Loss: 0.002950848250581724
Iteration: 126
Loss: 0.002880236116896679
Iteration: 127
Loss: 0.002858717940043567
Iteration: 128
Loss: 0.002847759339890284
Iteration: 129
Loss: 0.002809639352873513
Iteration: 130
Loss: 0.0027817694793977753
Iteration: 131
Loss: 0.002735776338362839
Iteration: 132
Loss: 0.002743652067715075
Iteration: 133
Loss: 0.0027202121152474384
Iteration: 134
Loss: 0.0026911925609655134
Iteration: 135
Loss: 0.0026582495633103863
Iteration: 136
Loss: 0.002625837382610615
Iteration: 137
Loss: 0.0026030699898511537
Iteration: 138
Loss: 0.0025973664653464786
Iteration: 139
Loss: 0.0025743720116002894
Iteration: 140
Loss: 0.0025539774612364607
Iteration: 141
Loss: 0.0025210520394555315
Iteration: 142
Loss: 0.002515178835360197
Iteration: 143
Loss: 0.0024882387455053083
Iteration: 144
Loss: 0.0024777261154135554
Iteration: 145
Loss: 0.0024605609516317887
Iteration: 146
Loss: 0.0024525095572377124
Iteration: 147
Loss: 0.0024427420618694
Iteration: 148
Loss: 0.0024038634955792166
Iteration: 149
Loss: 0.0023938771564422584
Iteration: 150
Loss: 0.0023935677216774445
Iteration: 151
Loss: 0.002385609110294864
Iteration: 152
Loss: 0.002371199502859537
Iteration: 153
Loss: 0.002359299858032567
Iteration: 154
Loss: 0.002360692128493655
Iteration: 155
Loss: 0.0023380967805434654
Iteration: 156
Loss: 0.002321288496448954
Iteration: 157
Loss: 0.0023367785559058555
Iteration: 158
Loss: 0.002300593459683402
Iteration: 159
Loss: 0.002287950997081835
Iteration: 160
Loss: 0.002281911409937027
Iteration: 161
Loss: 0.0022935013119244905
Iteration: 162
Loss: 0.0022756391767094412
Iteration: 163
Loss: 0.0022704724830052837
Iteration: 164
Loss: 0.0022620177237180675
Iteration: 165
Loss: 0.002256572164208969
Iteration: 166
Loss: 0.002226139337857958
Iteration: 167
Loss: 0.0022304124772412385
Iteration: 168
Loss: 0.0022140548027847415
Iteration: 169
Loss: 0.002238081343665083
Iteration: 170
Loss: 0.0022136063383119864
Iteration: 171
Loss: 0.0021936424280993823
Iteration: 172
Loss: 0.002200212467336891
Iteration: 173
Loss: 0.002185606244360892
Iteration: 174
Loss: 0.0021925468562829604
Iteration: 175
Loss: 0.002190454735238923
Iteration: 176
Loss: 0.0021698345052555386
Iteration: 177
Loss: 0.002167808735266146
Iteration: 178
Loss: 0.002159330093801567
Iteration: 179
Loss: 0.0021671401223213207
Iteration: 180
Loss: 0.0021507985926823825
Iteration: 181
Loss: 0.0021627245189781053
Iteration: 182
Loss: 0.002143155121835085
Iteration: 183
Loss: 0.0021469576749950647
Iteration: 184
Loss: 0.002151015071232417
Iteration: 185
Loss: 0.002125686196405895
Iteration: 186
Loss: 0.0021162557816019325
Iteration: 187
Loss: 0.0021297110644986897
Iteration: 188
Loss: 0.0021298221247157125
Iteration: 189
Loss: 0.0021339049463432918
Iteration: 190
Loss: 0.0021143256013690516
Iteration: 191
Loss: 0.002111062550883195
Iteration: 192
Loss: 0.0021100526195723655
Iteration: 193
Loss: 0.0021112191905381114
Iteration: 194
Loss: 0.00211671621697705
Iteration: 195
Loss: 0.0021049901303585345
Iteration: 196
Loss: 0.0021078619614765958
Iteration: 197
Loss: 0.002096896569483073
Iteration: 198
Loss: 0.0020900310312465926
Iteration: 199
Loss: 0.0020814532663945744
Iteration: 200
Loss: 0.002096652108664829
Iteration: 201
Loss: 0.0020858999469499217
Iteration: 202
Loss: 0.0020723445156430142
Iteration: 203
Loss: 0.002071435590240559
Iteration: 204
Loss: 0.0020753940519104464
Iteration: 205
Loss: 0.0020597483352303688
Iteration: 206
Loss: 0.0020621960994037913
Iteration: 207
Loss: 0.002068705809950011
Iteration: 208
Loss: 0.0020568180962738285
Iteration: 209
Loss: 0.002053884929897854
Iteration: 210
Loss: 0.002059568703549392
Iteration: 211
Loss: 0.00205652470694764
Iteration: 212
Loss: 0.002049555181434787
Iteration: 213
Loss: 0.002045318413321383
Iteration: 214
Loss: 0.002060278141360003
Iteration: 215
Loss: 0.0020430729477439167
Iteration: 216
Loss: 0.0020421259076811556
Iteration: 217
Loss: 0.002038385995902212
Iteration: 218
Loss: 0.002051327728223419
Iteration: 219
Loss: 0.0020415003499493186
Iteration: 220
Loss: 0.0020495805060263815
Iteration: 221
Loss: 0.0020498379301166205
Iteration: 222
Loss: 0.0020343584601949082
Iteration: 223
Loss: 0.0020322828417325893
Iteration: 224
Loss: 0.0020372157280401488
Iteration: 225
Loss: 0.0020304579903916795
Iteration: 226
Loss: 0.00202868594617651
Iteration: 227
Loss: 0.0020369325790031835
Iteration: 228
Loss: 0.0020167881838704755
Iteration: 229
Loss: 0.00203021137872938
Iteration: 230
Loss: 0.0020339582314169626
Iteration: 231
Loss: 0.0020198323153436367
Iteration: 232
Loss: 0.0020239830780887933
Iteration: 233
Loss: 0.002025739044212259
Iteration: 234
Loss: 0.002017861357095038
Iteration: 235
Loss: 0.002017488196299116
Iteration: 236
Loss: 0.002006229180729062
Iteration: 237
Loss: 0.002010047475512071
Iteration: 238
Loss: 0.0020107997910732904
Iteration: 239
Loss: 0.002011081774537338
Iteration: 240
Loss: 0.002013536607392314
Iteration: 241
Loss: 0.002003082139531105
Iteration: 242
Loss: 0.002012430534062044
Iteration: 243
Loss: 0.002005838806082199
Iteration: 244
Loss: 0.0019988969760583487
Iteration: 245
Loss: 0.0019958565485800003
Iteration: 246
Loss: 0.0020013476761693997
Iteration: 247
Loss: 0.002005289356996555
Iteration: 248
Loss: 0.001995196388523299
Iteration: 249
Loss: 0.0019912655087664906
Iteration: 250
Loss: 0.0019972996755003385
Iteration: 251
Loss: 0.00200449563591264
Iteration: 252
Loss: 0.0019994491552252597
Iteration: 253
Loss: 0.0019964876672730033
Iteration: 254
Loss: 0.0019925303158077707
Iteration: 255
Loss: 0.0019987929376728106
Iteration: 256
Loss: 0.0019926224784107835
Iteration: 257
Loss: 0.001993674938226255
Iteration: 258
Loss: 0.0019892502488091405
Iteration: 259
Loss: 0.0019853792193050427
Iteration: 260
Loss: 0.001986534850069935
Iteration: 261
Loss: 0.001987031449438868
Iteration: 262
Loss: 0.0019838362421113545
Iteration: 263
Loss: 0.0019786958185183568
Iteration: 264
Loss: 0.001993863083842415
Iteration: 265
Loss: 0.0019918701642692633
Iteration: 266
Loss: 0.0019853128654109997
Iteration: 267
Loss: 0.0019874947071756893
Iteration: 268
Loss: 0.001973404417030241
Iteration: 269
Loss: 0.001977170058015007
Iteration: 270
Loss: 0.0019783556387509877
Iteration: 271
Loss: 0.001981875715811351
Iteration: 272
Loss: 0.001974208455067128
Iteration: 273
Loss: 0.001980187110148552
Iteration: 274
Loss: 0.001972280260993213
Iteration: 275
Loss: 0.0019702734354678994
Iteration: 276
Loss: 0.001965336957321752
Iteration: 277
Loss: 0.0019630730157808923
Iteration: 278
Loss: 0.001971635836880745
Iteration: 279
Loss: 0.0019680363092026334
Iteration: 280
Loss: 0.00196650447806616
Iteration: 281
Loss: 0.001975257822778076
Iteration: 282
Loss: 0.0019731615065233556
Iteration: 283
Loss: 0.001965526220241062
Iteration: 284
Loss: 0.0019710975068780346
Iteration: 285
Loss: 0.0019631651570884193
Iteration: 286
Loss: 0.0019759047810533423
Iteration: 287
Loss: 0.001961716706296656
Iteration: 288
Loss: 0.0019723271082223554
Iteration: 289
Loss: 0.001966676194924952
Iteration: 290
Loss: 0.0019649358795637765
Iteration: 291
Loss: 0.0019538590227390026
Iteration: 292
Loss: 0.0019702011855637154
Iteration: 293
Loss: 0.0019587755214604662
Iteration: 294
Loss: 0.0019740975708927868
Iteration: 295
Loss: 0.0019511325446684368
Iteration: 296
Loss: 0.0019616061898243683
Iteration: 297
Loss: 0.0019578750603037273
Iteration: 298
Loss: 0.0019660031638766935
Iteration: 299
Loss: 0.0019645893594204653
Iteration: 300
Loss: 0.0019630189593199915
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.32112676056338024
accuracy: 0.9746315789473684
confusion: 57 108 133 9202
precision: 0.34545454545454546
recall: 0.3
Loading train data...
Loading val data...
Loading test data...
Training the TuckER model...
Number of training data points: 84683
Starting training...
Iteration: 1
Loss: 0.7015593945980072
Iteration: 2
Loss: 0.6942125042764152
Iteration: 3
Loss: 0.6746362643997844
Iteration: 4
Loss: 0.6356451707642253
Iteration: 5
Loss: 0.5812079092351402
Iteration: 6
Loss: 0.5212826140043212
Iteration: 7
Loss: 0.4689552678567607
Iteration: 8
Loss: 0.4297016670064228
Iteration: 9
Loss: 0.3985855146879103
Iteration: 10
Loss: 0.3735401179732346
Iteration: 11
Loss: 0.3537968727873593
Iteration: 12
Loss: 0.33472953991192145
Iteration: 13
Loss: 0.31851058362460716
Iteration: 14
Loss: 0.30371467841834554
Iteration: 15
Loss: 0.2888089498368705
Iteration: 16
Loss: 0.2749794874249435
Iteration: 17
Loss: 0.2602423825641958
Iteration: 18
Loss: 0.24609949239870396
Iteration: 19
Loss: 0.2325037536097736
Iteration: 20
Loss: 0.21841627613800327
Iteration: 21
Loss: 0.2055768336101276
Iteration: 22
Loss: 0.19205051787742755
Iteration: 23
Loss: 0.17844830198985775
Iteration: 24
Loss: 0.16575981494857045
Iteration: 25
Loss: 0.15403590951024032
Iteration: 26
Loss: 0.14271017327541258
Iteration: 27
Loss: 0.13190301307817784
Iteration: 28
Loss: 0.12161497480985595
Iteration: 29
Loss: 0.11143189686827543
Iteration: 30
Loss: 0.10313725589615542
Iteration: 31
Loss: 0.09519660272976248
Iteration: 32
Loss: 0.08740542347474796
Iteration: 33
Loss: 0.08089942575954809
Iteration: 34
Loss: 0.0742664537051829
Iteration: 35
Loss: 0.06797999189030833
Iteration: 36
Loss: 0.06294833196372521
Iteration: 37
Loss: 0.058107567905652815
Iteration: 38
Loss: 0.0541168400881494
Iteration: 39
Loss: 0.049827415359819806
Iteration: 40
Loss: 0.046241250752312384
Iteration: 41
Loss: 0.04291111939563984
Iteration: 42
Loss: 0.03979216497845766
Iteration: 43
Loss: 0.037062995680948584
Iteration: 44
Loss: 0.03461628834285387
Iteration: 45
Loss: 0.03239350602394197
Iteration: 46
Loss: 0.03037748111003056
Iteration: 47
Loss: 0.028352509320872587
Iteration: 48
Loss: 0.026742473539964454
Iteration: 49
Loss: 0.025056420666415516
Iteration: 50
Loss: 0.023503366971343028
Iteration: 51
Loss: 0.02213109970637938
Iteration: 52
Loss: 0.02107561313797061
Iteration: 53
Loss: 0.019974271539689566
Iteration: 54
Loss: 0.018895172105148073
Iteration: 55
Loss: 0.017903119802656697
Iteration: 56
Loss: 0.01699328438447016
Iteration: 57
Loss: 0.01612779153965232
Iteration: 58
Loss: 0.015447903117846424
Iteration: 59
Loss: 0.014708370413267759
Iteration: 60
Loss: 0.014126217674191406
Iteration: 61
Loss: 0.013471613376878383
Iteration: 62
Loss: 0.01295396887765425
Iteration: 63
Loss: 0.012403638204332532
Iteration: 64
Loss: 0.01187953000842798
Iteration: 65
Loss: 0.011462478898465633
Iteration: 66
Loss: 0.01106025178621455
Iteration: 67
Loss: 0.010599948772479122
Iteration: 68
Loss: 0.010219597075952262
Iteration: 69
Loss: 0.009872904751540684
Iteration: 70
Loss: 0.009530733998228864
Iteration: 71
Loss: 0.009239769665660655
Iteration: 72
Loss: 0.008894647426176362
Iteration: 73
Loss: 0.008603445992509767
Iteration: 74
Loss: 0.00833753565689776
Iteration: 75
Loss: 0.008100076300296478
Iteration: 76
Loss: 0.007846638350179645
Iteration: 77
Loss: 0.007609039340622541
Iteration: 78
Loss: 0.0074069631660765994
Iteration: 79
Loss: 0.007151164716427646
Iteration: 80
Loss: 0.006972412317536953
Iteration: 81
Loss: 0.006781682275553666
Iteration: 82
Loss: 0.006616065880601726
Iteration: 83
Loss: 0.006411169462541982
Iteration: 84
Loss: 0.006265092692224354
Iteration: 85
Loss: 0.006106451023142875
Iteration: 86
Loss: 0.005958192152675332
Iteration: 87
Loss: 0.005798716188930884
Iteration: 88
Loss: 0.005681572510383841
Iteration: 89
Loss: 0.005561201236914934
Iteration: 90
Loss: 0.005403005239758186
Iteration: 91
Loss: 0.005311833069909636
Iteration: 92
Loss: 0.0051930967332204665
Iteration: 93
Loss: 0.005078769413890635
Iteration: 94
Loss: 0.0049729668762444
Iteration: 95
Loss: 0.004880861259951461
Iteration: 96
Loss: 0.004797902049087897
Iteration: 97
Loss: 0.0047112596505207984
Iteration: 98
Loss: 0.004609013961559934
Iteration: 99
Loss: 0.004558835402926112
Iteration: 100
Loss: 0.004459327972698502
Iteration: 101
Loss: 0.004397042251828059
Iteration: 102
Loss: 0.00430116700803543
Iteration: 103
Loss: 0.004232966494405779
Iteration: 104
Loss: 0.0041335156562215675
Iteration: 105
Loss: 0.004080486654235822
Iteration: 106
Loss: 0.004021744513543459
Iteration: 107
Loss: 0.003951297621469854
Iteration: 108
Loss: 0.0038788067148544077
Iteration: 109
Loss: 0.0038109021374910343
Iteration: 110
Loss: 0.0037397422981107742
Iteration: 111
Loss: 0.0036891210347232296
Iteration: 112
Loss: 0.0036314015004147845
Iteration: 113
Loss: 0.003572533889560074
Iteration: 114
Loss: 0.0035184575244784355
Iteration: 115
Loss: 0.0034483325491636626
Iteration: 116
Loss: 0.003397015698549406
Iteration: 117
Loss: 0.0033563182564316
Iteration: 118
Loss: 0.003300526920456167
Iteration: 119
Loss: 0.0032356174821715528
Iteration: 120
Loss: 0.0031930492799047655
Iteration: 121
Loss: 0.0031586972025518375
Iteration: 122
Loss: 0.003098573512965586
Iteration: 123
Loss: 0.0030781315389748027
Iteration: 124
Loss: 0.003026096868646763
Iteration: 125
Loss: 0.002990072979251059
Iteration: 126
Loss: 0.0029614881112626414
Iteration: 127
Loss: 0.0029335256308180894
Iteration: 128
Loss: 0.0028762947427245175
Iteration: 129
Loss: 0.002852513082893338
Iteration: 130
Loss: 0.002833153612389252
Iteration: 131
Loss: 0.002811874498658609
Iteration: 132
Loss: 0.002775057670433165
Iteration: 133
Loss: 0.0027507493050950692
Iteration: 134
Loss: 0.002722110460148897
Iteration: 135
Loss: 0.00269094162064082
Iteration: 136
Loss: 0.0026641037519566898
Iteration: 137
Loss: 0.0026455050215079654
Iteration: 138
Loss: 0.0026270427014224414
Iteration: 139
Loss: 0.002597809419967234
Iteration: 140
Loss: 0.0025803993836545
Iteration: 141
Loss: 0.002569961710833013
Iteration: 142
Loss: 0.0025509033017087636
Iteration: 143
Loss: 0.002527280148436747
Iteration: 144
Loss: 0.0024864008371922666
Iteration: 145
Loss: 0.0024927484337240458
Iteration: 146
Loss: 0.0024697270596445334
Iteration: 147
Loss: 0.0024448166141377353
Iteration: 148
Loss: 0.0024374247182206046
Iteration: 149
Loss: 0.0024244130628819508
Iteration: 150
Loss: 0.0024208815274305824
Iteration: 151
Loss: 0.002395809574120837
Iteration: 152
Loss: 0.002382916969065441
Iteration: 153
Loss: 0.0023713056423996645
Iteration: 154
Loss: 0.0023592490214491037
Iteration: 155
Loss: 0.0023528266039381665
Iteration: 156
Loss: 0.0023392454533633297
Iteration: 157
Loss: 0.0023273182209444846
Iteration: 158
Loss: 0.002315966807101376
Iteration: 159
Loss: 0.002288586888803033
Iteration: 160
Loss: 0.0022954693626294413
Iteration: 161
Loss: 0.0022839296342259865
Iteration: 162
Loss: 0.002277145847939409
Iteration: 163
Loss: 0.0022619118624389537
Iteration: 164
Loss: 0.002251835928934558
Iteration: 165
Loss: 0.002245673867192392
Iteration: 166
Loss: 0.0022329611976335687
Iteration: 167
Loss: 0.0022325194029069345
Iteration: 168
Loss: 0.0022338054245110693
Iteration: 169
Loss: 0.00221947915624918
Iteration: 170
Loss: 0.0022136201619216036
Iteration: 171
Loss: 0.002195773498586765
Iteration: 172
Loss: 0.0022032927202128966
Iteration: 173
Loss: 0.0021965075979895163
Iteration: 174
Loss: 0.002187372369850736
Iteration: 175
Loss: 0.0021835976967406346
Iteration: 176
Loss: 0.0021623454422962556
Iteration: 177
Loss: 0.0021666151885988147
Iteration: 178
Loss: 0.0021520012197448167
Iteration: 179
Loss: 0.0021525144565668775
Iteration: 180
Loss: 0.002161227253254321
Iteration: 181
Loss: 0.0021454284532654395
Iteration: 182
Loss: 0.002143564121513741
Iteration: 183
Loss: 0.0021459318813867867
Iteration: 184
Loss: 0.0021402629024190145
Iteration: 185
Loss: 0.0021454188404831944
Iteration: 186
Loss: 0.002124046158647483
Iteration: 187
Loss: 0.002117805844513563
Iteration: 188
Loss: 0.002124567801507599
Iteration: 189
Loss: 0.0021226364486033053
Iteration: 190
Loss: 0.0021093641549757704
Iteration: 191
Loss: 0.002100178574370902
Iteration: 192
Loss: 0.0020955625435950735
Iteration: 193
Loss: 0.002103909774876495
Iteration: 194
Loss: 0.002102352215953898
Iteration: 195
Loss: 0.002088349677406524
Iteration: 196
Loss: 0.0020968831362906935
Iteration: 197
Loss: 0.002083702744041547
Iteration: 198
Loss: 0.002087486413784507
Iteration: 199
Loss: 0.002071029897040983
Iteration: 200
Loss: 0.002079983772823542
Iteration: 201
Loss: 0.002077973635140352
Iteration: 202
Loss: 0.0020755117605781048
Iteration: 203
Loss: 0.0020661259214848097
Iteration: 204
Loss: 0.002066428856867388
Iteration: 205
Loss: 0.002064695911163964
Iteration: 206
Loss: 0.002076497352568478
Iteration: 207
Loss: 0.0020575469811219813
Iteration: 208
Loss: 0.0020560139418216196
Iteration: 209
Loss: 0.0020490274111535853
Iteration: 210
Loss: 0.0020503107083552495
Iteration: 211
Loss: 0.002054456612890268
Iteration: 212
Loss: 0.0020569573545533163
Iteration: 213
Loss: 0.002050426226427279
Iteration: 214
Loss: 0.00204459898508858
Iteration: 215
Loss: 0.002050014251059421
Iteration: 216
Loss: 0.002041140505930454
Iteration: 217
Loss: 0.002036932570484989
Iteration: 218
Loss: 0.0020336638170680623
Iteration: 219
Loss: 0.002023971246316938
Iteration: 220
Loss: 0.0020371081333084985
Iteration: 221
Loss: 0.0020501123039937783
Iteration: 222
Loss: 0.0020306688284774015
Iteration: 223
Loss: 0.002023903712652987
Iteration: 224
Loss: 0.0020245158102181627
Iteration: 225
Loss: 0.0020268799328222505
Iteration: 226
Loss: 0.0020173816847978386
Iteration: 227
Loss: 0.002029239589053138
Iteration: 228
Loss: 0.002024788254724316
Iteration: 229
Loss: 0.0020199178564702957
Iteration: 230
Loss: 0.002012478210395429
Iteration: 231
Loss: 0.0020068547242639086
Iteration: 232
Loss: 0.0020109921688123084
Iteration: 233
Loss: 0.0020129736583884353
Iteration: 234
Loss: 0.0020164039792356695
Iteration: 235
Loss: 0.002011089227957333
Iteration: 236
Loss: 0.0020214110760517962
Iteration: 237
Loss: 0.0020113969732802814
Iteration: 238
Loss: 0.002008847825558538
Iteration: 239
Loss: 0.0020164933180572783
Iteration: 240
Loss: 0.0020078237568846016
Iteration: 241
Loss: 0.001997819905759903
Iteration: 242
Loss: 0.0020058817931496334
Iteration: 243
Loss: 0.0019942744955897514
Iteration: 244
Loss: 0.0020022366065304818
Iteration: 245
Loss: 0.0020193336815459697
Iteration: 246
Loss: 0.001988669001304249
Iteration: 247
Loss: 0.001996700535528362
Iteration: 248
Loss: 0.0019920599340211326
Iteration: 249
Loss: 0.0020115405233099874
Iteration: 250
Loss: 0.0019936879355710274
Iteration: 251
Loss: 0.0019948093700449822
Iteration: 252
Loss: 0.001997404317257971
Iteration: 253
Loss: 0.001990145002775712
Iteration: 254
Loss: 0.0019949925027038084
Iteration: 255
Loss: 0.0019844845533598123
Iteration: 256
Loss: 0.0019937064711617806
Iteration: 257
Loss: 0.0019839255951299537
Iteration: 258
Loss: 0.001982951580785306
Iteration: 259
Loss: 0.0019906568738472896
Iteration: 260
Loss: 0.001986843803556772
Iteration: 261
Loss: 0.001973004067577876
Iteration: 262
Loss: 0.001980020461615357
Iteration: 263
Loss: 0.0019762565597619226
Iteration: 264
Loss: 0.0019834279423443283
Iteration: 265
Loss: 0.0019759555608488437
Iteration: 266
Loss: 0.001969909675919065
Iteration: 267
Loss: 0.00198455329802705
Iteration: 268
Loss: 0.0019814748935992033
Iteration: 269
Loss: 0.0019713626285765046
Iteration: 270
Loss: 0.001980660258928632
Iteration: 271
Loss: 0.001976512916238479
Iteration: 272
Loss: 0.001976180892642133
Iteration: 273
Loss: 0.0019741163591899705
Iteration: 274
Loss: 0.0019782109912939187
Iteration: 275
Loss: 0.0019794447282215624
Iteration: 276
Loss: 0.0019757373119340983
Iteration: 277
Loss: 0.0019694513658334205
Iteration: 278
Loss: 0.0019782254551878064
Iteration: 279
Loss: 0.0019655981024431926
Iteration: 280
Loss: 0.0019742824838542175
Iteration: 281
Loss: 0.001961271291337453
Iteration: 282
Loss: 0.0019616738924325056
Iteration: 283
Loss: 0.001964669305513181
Iteration: 284
Loss: 0.00196944624355926
Iteration: 285
Loss: 0.001972586705871835
Iteration: 286
Loss: 0.0019603960965646474
Iteration: 287
Loss: 0.001959040316628156
Iteration: 288
Loss: 0.0019659882272230235
Iteration: 289
Loss: 0.0019658016021045425
Iteration: 290
Loss: 0.001962484560463941
Iteration: 291
Loss: 0.0019629956549601403
Iteration: 292
Loss: 0.0019615746398524542
Iteration: 293
Loss: 0.0019589522015871254
Iteration: 294
Loss: 0.0019578863497505465
Iteration: 295
Loss: 0.0019604792653742
Iteration: 296
Loss: 0.0019533396215836813
Iteration: 297
Loss: 0.001953128679859929
Iteration: 298
Loss: 0.001963232372739784
Iteration: 299
Loss: 0.0019514006690303908
Iteration: 300
Loss: 0.0019516416700420583
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.25061425061425063
accuracy: 0.9678947368421053
confusion: 51 166 139 9144
precision: 0.2350230414746544
recall: 0.26842105263157895
Loading train data...
Loading val data...
Loading test data...
Training the TuckER model...
Number of training data points: 84683
Starting training...
Iteration: 1
Loss: 0.7015871151191432
Iteration: 2
Loss: 0.6942849057476695
Iteration: 3
Loss: 0.6742782868990084
Iteration: 4
Loss: 0.6351323840094776
Iteration: 5
Loss: 0.5793941144536181
Iteration: 6
Loss: 0.5206804860655855
Iteration: 7
Loss: 0.4684083280039997
Iteration: 8
Loss: 0.42758642427804994
Iteration: 9
Loss: 0.3952711046468921
Iteration: 10
Loss: 0.37090764249243385
Iteration: 11
Loss: 0.34979083734314614
Iteration: 12
Loss: 0.3315211392757369
Iteration: 13
Loss: 0.3147174752340084
Iteration: 14
Loss: 0.2990117232973983
Iteration: 15
Loss: 0.28408225135105414
Iteration: 16
Loss: 0.26979184296072983
Iteration: 17
Loss: 0.25562333261094444
Iteration: 18
Loss: 0.24181451921055958
Iteration: 19
Loss: 0.2276712774140079
Iteration: 20
Loss: 0.21396398526139376
Iteration: 21
Loss: 0.20055967932794153
Iteration: 22
Loss: 0.18798028641357656
Iteration: 23
Loss: 0.17481849070002392
Iteration: 24
Loss: 0.16279066463069217
Iteration: 25
Loss: 0.1513419425705584
Iteration: 26
Loss: 0.14018072006179066
Iteration: 27
Loss: 0.1298301491613795
Iteration: 28
Loss: 0.11953530978502297
Iteration: 29
Loss: 0.11034782984998168
Iteration: 30
Loss: 0.10192911081561228
Iteration: 31
Loss: 0.09395131223448892
Iteration: 32
Loss: 0.08645142687530052
Iteration: 33
Loss: 0.07960327414841187
Iteration: 34
Loss: 0.07347457010934992
Iteration: 35
Loss: 0.06787459988419603
Iteration: 36
Loss: 0.06291130344133551
Iteration: 37
Loss: 0.05809857578175824
Iteration: 38
Loss: 0.05350079796299702
Iteration: 39
Loss: 0.049495552371187905
Iteration: 40
Loss: 0.046038229881626806
Iteration: 41
Loss: 0.04289281191077174
Iteration: 42
Loss: 0.039959043446110516
Iteration: 43
Loss: 0.036916894929074656
Iteration: 44
Loss: 0.03471394778206581
Iteration: 45
Loss: 0.03227635759224252
Iteration: 46
Loss: 0.030286459933693815
Iteration: 47
Loss: 0.02824420263854469
Iteration: 48
Loss: 0.0265583873658282
Iteration: 49
Loss: 0.02500703240313181
Iteration: 50
Loss: 0.023594340332215878
Iteration: 51
Loss: 0.02218960945653479
Iteration: 52
Loss: 0.02091997216750936
Iteration: 53
Loss: 0.01979649096454789
Iteration: 54
Loss: 0.01875253703172614
Iteration: 55
Loss: 0.017776725541164235
Iteration: 56
Loss: 0.017038855247381256
Iteration: 57
Loss: 0.01611739467466023
Iteration: 58
Loss: 0.01534527070002585
Iteration: 59
Loss: 0.014625264922293222
Iteration: 60
Loss: 0.014025693623031058
Iteration: 61
Loss: 0.013312575021168081
Iteration: 62
Loss: 0.012820674969655712
Iteration: 63
Loss: 0.012252331336551323
Iteration: 64
Loss: 0.011746677751766472
Iteration: 65
Loss: 0.01130728971031381
Iteration: 66
Loss: 0.010882757774486048
Iteration: 67
Loss: 0.01050622472766696
Iteration: 68
Loss: 0.010087005759975532
Iteration: 69
Loss: 0.009795783700921186
Iteration: 70
Loss: 0.009448804529156626
Iteration: 71
Loss: 0.009131054195189258
Iteration: 72
Loss: 0.008752484419723837
Iteration: 73
Loss: 0.008523173976644146
Iteration: 74
Loss: 0.008268579244386495
Iteration: 75
Loss: 0.00798036749815432
Iteration: 76
Loss: 0.007727828643443744
Iteration: 77
Loss: 0.007527862164396338
Iteration: 78
Loss: 0.007306152926303628
Iteration: 79
Loss: 0.007144046845141708
Iteration: 80
Loss: 0.006896868751316172
Iteration: 81
Loss: 0.0067268401248062526
Iteration: 82
Loss: 0.006548377293411915
Iteration: 83
Loss: 0.00640410383618096
Iteration: 84
Loss: 0.006241578801830367
Iteration: 85
Loss: 0.006090168381200694
Iteration: 86
Loss: 0.005906979059346202
Iteration: 87
Loss: 0.005750192851736778
Iteration: 88
Loss: 0.005658503604780246
Iteration: 89
Loss: 0.00549858631339015
Iteration: 90
Loss: 0.005377499023225249
Iteration: 91
Loss: 0.005276407541071133
Iteration: 92
Loss: 0.0051374701533194
Iteration: 93
Loss: 0.005055790392273083
Iteration: 94
Loss: 0.004932938529769095
Iteration: 95
Loss: 0.004843017875739351
Iteration: 96
Loss: 0.004751371239630006
Iteration: 97
Loss: 0.004651767763922491
Iteration: 98
Loss: 0.004546707902080947
Iteration: 99
Loss: 0.004436952283424212
Iteration: 100
Loss: 0.00436554592466209
Iteration: 101
Loss: 0.00428508412626731
Iteration: 102
Loss: 0.004204074880562541
Iteration: 103
Loss: 0.004106952973482449
Iteration: 104
Loss: 0.004030165830399932
Iteration: 105
Loss: 0.0039528259984785465
Iteration: 106
Loss: 0.003880962079231877
Iteration: 107
Loss: 0.003832852377033815
Iteration: 108
Loss: 0.0037665612649217975
Iteration: 109
Loss: 0.0036912042272799627
Iteration: 110
Loss: 0.003611533436924219
Iteration: 111
Loss: 0.0035495403870122464
Iteration: 112
Loss: 0.0035026055737966445
Iteration: 113
Loss: 0.0034502413629258913
Iteration: 114
Loss: 0.0033905024344964723
Iteration: 115
Loss: 0.00334498371713136
Iteration: 116
Loss: 0.0032876543956240865
Iteration: 117
Loss: 0.003257337506156324
Iteration: 118
Loss: 0.0031997014359548322
Iteration: 119
Loss: 0.0031638469024584063
Iteration: 120
Loss: 0.003126935306482199
Iteration: 121
Loss: 0.00308510761071996
Iteration: 122
Loss: 0.0030387576027750603
Iteration: 123
Loss: 0.002986105144728066
Iteration: 124
Loss: 0.002956181029198555
Iteration: 125
Loss: 0.002933739440333916
Iteration: 126
Loss: 0.0028939366016945823
Iteration: 127
Loss: 0.002867163780781372
Iteration: 128
Loss: 0.0028270954669385056
Iteration: 129
Loss: 0.002792871560015511
Iteration: 130
Loss: 0.002770174638277329
Iteration: 131
Loss: 0.002746812519390227
Iteration: 132
Loss: 0.0027132744974706596
Iteration: 133
Loss: 0.002677428567350456
Iteration: 134
Loss: 0.0026578956323380512
Iteration: 135
Loss: 0.002652713512184053
Iteration: 136
Loss: 0.0026400419003216593
Iteration: 137
Loss: 0.0025983023565126265
Iteration: 138
Loss: 0.002568946961063619
Iteration: 139
Loss: 0.002556677095637452
Iteration: 140
Loss: 0.0025538865437095123
Iteration: 141
Loss: 0.0025221588992990736
Iteration: 142
Loss: 0.002508082884227539
Iteration: 143
Loss: 0.002490195201528145
Iteration: 144
Loss: 0.0024674600217400528
Iteration: 145
Loss: 0.0024496902109737077
Iteration: 146
Loss: 0.0024415330206066736
Iteration: 147
Loss: 0.0024178380192053026
Iteration: 148
Loss: 0.0024047926942840584
Iteration: 149
Loss: 0.0024047823020870366
Iteration: 150
Loss: 0.00238678058064202
Iteration: 151
Loss: 0.0023710576692459787
Iteration: 152
Loss: 0.0023766306722990986
Iteration: 153
Loss: 0.0023389379011194518
Iteration: 154
Loss: 0.0023439406564958938
Iteration: 155
Loss: 0.002331499581043495
Iteration: 156
Loss: 0.00231194685542638
Iteration: 157
Loss: 0.002308188957480214
Iteration: 158
Loss: 0.0023028663950177227
Iteration: 159
Loss: 0.002290789852850139
Iteration: 160
Loss: 0.0022828863570239486
Iteration: 161
Loss: 0.00227283619105725
Iteration: 162
Loss: 0.0022486799635083937
Iteration: 163
Loss: 0.0022602933118256127
Iteration: 164
Loss: 0.002237508509296742
Iteration: 165
Loss: 0.002245295531593445
Iteration: 166
Loss: 0.002222327416103969
Iteration: 167
Loss: 0.0022189724429442386
Iteration: 168
Loss: 0.002204401133207195
Iteration: 169
Loss: 0.0022046206726283745
Iteration: 170
Loss: 0.002201172503324725
Iteration: 171
Loss: 0.002189009538397375
Iteration: 172
Loss: 0.0021961041493341327
Iteration: 173
Loss: 0.0021889979082227846
Iteration: 174
Loss: 0.0021785868746342094
Iteration: 175
Loss: 0.0021699071398981642
Iteration: 176
Loss: 0.002170244485946236
Iteration: 177
Loss: 0.0021634377488048704
Iteration: 178
Loss: 0.002160350292375902
Iteration: 179
Loss: 0.002151229775158643
Iteration: 180
Loss: 0.002163306880946748
Iteration: 181
Loss: 0.0021503282762019007
Iteration: 182
Loss: 0.0021402767061528454
Iteration: 183
Loss: 0.002139946272619432
Iteration: 184
Loss: 0.002128293228097169
Iteration: 185
Loss: 0.002117504022175037
Iteration: 186
Loss: 0.002119151318854675
Iteration: 187
Loss: 0.0021149367335388755
Iteration: 188
Loss: 0.002109826102330372
Iteration: 189
Loss: 0.0021028601785940005
Iteration: 190
Loss: 0.0021179066076534004
Iteration: 191
Loss: 0.002104160337561242
Iteration: 192
Loss: 0.002100678325471718
Iteration: 193
Loss: 0.002100733927984881
Iteration: 194
Loss: 0.0020952422282567657
Iteration: 195
Loss: 0.002100665241525304
Iteration: 196
Loss: 0.0020906906163251798
Iteration: 197
Loss: 0.0020827738605098934
Iteration: 198
Loss: 0.0020816934340429014
Iteration: 199
Loss: 0.0020886099962631197
Iteration: 200
Loss: 0.0020677085963032413
Iteration: 201
Loss: 0.0020707215753202213
Iteration: 202
Loss: 0.00206733862131198
Iteration: 203
Loss: 0.0020571523757719596
Iteration: 204
Loss: 0.00206991889877472
Iteration: 205
Loss: 0.002058569391587431
Iteration: 206
Loss: 0.002059701884097261
Iteration: 207
Loss: 0.002064180629975276
Iteration: 208
Loss: 0.0020463549185589683
Iteration: 209
Loss: 0.0020552290441672796
Iteration: 210
Loss: 0.0020532195638047488
Iteration: 211
Loss: 0.0020469115839748665
Iteration: 212
Loss: 0.0020567674044996683
Iteration: 213
Loss: 0.0020500004913362605
Iteration: 214
Loss: 0.002042715671960628
Iteration: 215
Loss: 0.002046665376926795
Iteration: 216
Loss: 0.0020426892357446797
Iteration: 217
Loss: 0.0020360685274481956
Iteration: 218
Loss: 0.002046329451997469
Iteration: 219
Loss: 0.002033343832519632
Iteration: 220
Loss: 0.002033027780537562
Iteration: 221
Loss: 0.002029613814623345
Iteration: 222
Loss: 0.0020442275903982724
Iteration: 223
Loss: 0.0020249778257183187
Iteration: 224
Loss: 0.0020194904560722955
Iteration: 225
Loss: 0.0020186914934595002
Iteration: 226
Loss: 0.0020198759796075157
Iteration: 227
Loss: 0.0020258570765713003
Iteration: 228
Loss: 0.0020203188490463286
Iteration: 229
Loss: 0.0020214985749434406
Iteration: 230
Loss: 0.0020117846774732376
Iteration: 231
Loss: 0.002023950674877752
Iteration: 232
Loss: 0.002014602799956664
Iteration: 233
Loss: 0.00201747842167118
Iteration: 234
Loss: 0.0020162058559746094
Iteration: 235
Loss: 0.0020179177432820745
Iteration: 236
Loss: 0.0020190728175249403
Iteration: 237
Loss: 0.0020159157433138207
Iteration: 238
Loss: 0.0020071990978222615
Iteration: 239
Loss: 0.002007737030309239
Iteration: 240
Loss: 0.0020128511951486693
Iteration: 241
Loss: 0.002002862164262319
Iteration: 242
Loss: 0.002003276614165615
Iteration: 243
Loss: 0.0020145582128884044
Iteration: 244
Loss: 0.0019972023934625633
Iteration: 245
Loss: 0.002006291959820906
Iteration: 246
Loss: 0.002006217840173077
Iteration: 247
Loss: 0.001999828053025029
Iteration: 248
Loss: 0.0019912969565200733
Iteration: 249
Loss: 0.0019968684518528058
Iteration: 250
Loss: 0.0019935962073959227
Iteration: 251
Loss: 0.0019952210870276136
Iteration: 252
Loss: 0.0019798459635661323
Iteration: 253
Loss: 0.001997685059905052
Iteration: 254
Loss: 0.0019902740307038693
Iteration: 255
Loss: 0.0019967308276469206
Iteration: 256
Loss: 0.0019924341823065244
Iteration: 257
Loss: 0.0019917308875336878
Iteration: 258
Loss: 0.0019849793653276453
Iteration: 259
Loss: 0.0019861530646023045
Iteration: 260
Loss: 0.0019761809025800265
Iteration: 261
Loss: 0.0019787106132021217
Iteration: 262
Loss: 0.0019811961172353024
Iteration: 263
Loss: 0.001983391815262688
Iteration: 264
Loss: 0.001984856883631792
Iteration: 265
Loss: 0.001989906119955022
Iteration: 266
Loss: 0.001984251562290166
Iteration: 267
Loss: 0.0019769798084048598
Iteration: 268
Loss: 0.001987795244686578
Iteration: 269
Loss: 0.001980255653218525
Iteration: 270
Loss: 0.0019709309917965495
Iteration: 271
Loss: 0.001976813743367973
Iteration: 272
Loss: 0.0019692409238437326
Iteration: 273
Loss: 0.0019740448049383194
Iteration: 274
Loss: 0.001978306145202823
Iteration: 275
Loss: 0.0019773286568546076
Iteration: 276
Loss: 0.0019672181054086585
Iteration: 277
Loss: 0.001975422702366259
Iteration: 278
Loss: 0.001970271983115775
Iteration: 279
Loss: 0.001978080435769587
Iteration: 280
Loss: 0.0019724496481258696
Iteration: 281
Loss: 0.001965888834092766
Iteration: 282
Loss: 0.001969547140150808
Iteration: 283
Loss: 0.0019683720041407137
Iteration: 284
Loss: 0.001965723989997059
Iteration: 285
Loss: 0.0019699982268077025
Iteration: 286
Loss: 0.0019705401472293023
Iteration: 287
Loss: 0.0019581605234501383
Iteration: 288
Loss: 0.001957947175336502
Iteration: 289
Loss: 0.0019691256371826477
Iteration: 290
Loss: 0.0019654017026967756
Iteration: 291
Loss: 0.001975246502097878
Iteration: 292
Loss: 0.0019651660298156304
Iteration: 293
Loss: 0.0019711796804785546
Iteration: 294
Loss: 0.0019518009148447252
Iteration: 295
Loss: 0.0019475372491131833
Iteration: 296
Loss: 0.0019668058035100197
Iteration: 297
Loss: 0.001963437615791563
Iteration: 298
Loss: 0.001957958701873062
Iteration: 299
Loss: 0.001957081543656475
Iteration: 300
Loss: 0.0019492461607314463
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.17573221757322174
accuracy: 0.9377894736842105
confusion: 63 464 127 8846
precision: 0.11954459203036052
recall: 0.33157894736842103
Loading train data...
Loading val data...
Loading test data...
Training the TuckER model...
Number of training data points: 84683
Starting training...
Iteration: 1
Loss: 0.7016878346117531
Iteration: 2
Loss: 0.6939619975845989
Iteration: 3
Loss: 0.6734901332273716
Iteration: 4
Loss: 0.6345922314539189
Iteration: 5
Loss: 0.5785563762595014
Iteration: 6
Loss: 0.5176381250707115
Iteration: 7
Loss: 0.4674540427399845
Iteration: 8
Loss: 0.4248988101395165
Iteration: 9
Loss: 0.39356237867983374
Iteration: 10
Loss: 0.36748055822965575
Iteration: 11
Loss: 0.34569958870003864
Iteration: 12
Loss: 0.32814506641248375
Iteration: 13
Loss: 0.31055963766284106
Iteration: 14
Loss: 0.29446608954813425
Iteration: 15
Loss: 0.27942528121355104
Iteration: 16
Loss: 0.26421613009964545
Iteration: 17
Loss: 0.25037225881000846
Iteration: 18
Loss: 0.23626099181611362
Iteration: 19
Loss: 0.2228833720451448
Iteration: 20
Loss: 0.2096538907144128
Iteration: 21
Loss: 0.19646503939861204
Iteration: 22
Loss: 0.18347176836758125
Iteration: 23
Loss: 0.1715810751406158
Iteration: 24
Loss: 0.15913168813397244
Iteration: 25
Loss: 0.14863879905968178
Iteration: 26
Loss: 0.13777570480980525
Iteration: 27
Loss: 0.1273910147024364
Iteration: 28
Loss: 0.11793540972398549
Iteration: 29
Loss: 0.10936602804718948
Iteration: 30
Loss: 0.10062367369125529
Iteration: 31
Loss: 0.09304128823483862
Iteration: 32
Loss: 0.08577283580855626
Iteration: 33
Loss: 0.07923873940982469
Iteration: 34
Loss: 0.0732362928186975
Iteration: 35
Loss: 0.06748429404162778
Iteration: 36
Loss: 0.06251022178770566
Iteration: 37
Loss: 0.05750101191423288
Iteration: 38
Loss: 0.053429764763611116
Iteration: 39
Loss: 0.049383160408313684
Iteration: 40
Loss: 0.045874457124893256
Iteration: 41
Loss: 0.04249750604716743
Iteration: 42
Loss: 0.03987593589941176
Iteration: 43
Loss: 0.0370057755399768
Iteration: 44
Loss: 0.03455671499960306
Iteration: 45
Loss: 0.03227260336279869
Iteration: 46
Loss: 0.029965789488902907
Iteration: 47
Loss: 0.028141562895077032
Iteration: 48
Loss: 0.026369518918416848
Iteration: 49
Loss: 0.024796163390685872
Iteration: 50
Loss: 0.023493744205774332
Iteration: 51
Loss: 0.022069538807178418
Iteration: 52
Loss: 0.02094635144784683
Iteration: 53
Loss: 0.019718635895448488
Iteration: 54
Loss: 0.018618902941120834
Iteration: 55
Loss: 0.01768783407240379
Iteration: 56
Loss: 0.016724770525243224
Iteration: 57
Loss: 0.01594092796851949
Iteration: 58
Loss: 0.015294064347427793
Iteration: 59
Loss: 0.014520889829571655
Iteration: 60
Loss: 0.013847338240139367
Iteration: 61
Loss: 0.013267717858004133
Iteration: 62
Loss: 0.012649138242278883
Iteration: 63
Loss: 0.012087365919042652
Iteration: 64
Loss: 0.0115892557290996
Iteration: 65
Loss: 0.011155467207838849
Iteration: 66
Loss: 0.010696907164301814
Iteration: 67
Loss: 0.010243745123195211
Iteration: 68
Loss: 0.00992185256739215
Iteration: 69
Loss: 0.009536465539074526
Iteration: 70
Loss: 0.009205859256681145
Iteration: 71
Loss: 0.00886999625462766
Iteration: 72
Loss: 0.008619672317830165
Iteration: 73
Loss: 0.008287834968963048
Iteration: 74
Loss: 0.00803807673680528
Iteration: 75
Loss: 0.0077855161152689195
Iteration: 76
Loss: 0.0075922619278838
Iteration: 77
Loss: 0.007339972485901743
Iteration: 78
Loss: 0.007100896847366196
Iteration: 79
Loss: 0.006955280553594958
Iteration: 80
Loss: 0.006727824935962151
Iteration: 81
Loss: 0.006540143851009084
Iteration: 82
Loss: 0.0063361445862073
Iteration: 83
Loss: 0.00622356162270213
Iteration: 84
Loss: 0.006051516103580958
Iteration: 85
Loss: 0.00592438131570816
Iteration: 86
Loss: 0.005763145385128333
Iteration: 87
Loss: 0.005614050221061561
Iteration: 88
Loss: 0.0055007855691833465
Iteration: 89
Loss: 0.005351910537971956
Iteration: 90
Loss: 0.005243110771449964
Iteration: 91
Loss: 0.005128030648182441
Iteration: 92
Loss: 0.005009851446829554
Iteration: 93
Loss: 0.004932139500430445
Iteration: 94
Loss: 0.004805637015279655
Iteration: 95
Loss: 0.004706120209359541
Iteration: 96
Loss: 0.004638456921206742
Iteration: 97
Loss: 0.004531207349628392
Iteration: 98
Loss: 0.004447816700742739
Iteration: 99
Loss: 0.004357061453913225
Iteration: 100
Loss: 0.004281205735623655
Iteration: 101
Loss: 0.004201427525159244
Iteration: 102
Loss: 0.004132589208893478
Iteration: 103
Loss: 0.004061422520885017
Iteration: 104
Loss: 0.004004318954241349
Iteration: 105
Loss: 0.0039041599474574735
Iteration: 106
Loss: 0.0038313781189527815
Iteration: 107
Loss: 0.003773430297037632
Iteration: 108
Loss: 0.0037285755189680834
Iteration: 109
Loss: 0.00366201586794199
Iteration: 110
Loss: 0.003619583533713367
Iteration: 111
Loss: 0.003541674192358808
Iteration: 112
Loss: 0.003499861678858174
Iteration: 113
Loss: 0.003450721675507361
Iteration: 114
Loss: 0.0034052277801603806
Iteration: 115
Loss: 0.003350223573606189
Iteration: 116
Loss: 0.003301290928658734
Iteration: 117
Loss: 0.003267006866247734
Iteration: 118
Loss: 0.003221450526857885
Iteration: 119
Loss: 0.0031788330525159836
Iteration: 120
Loss: 0.0031202139088702275
Iteration: 121
Loss: 0.0030850056308980396
Iteration: 122
Loss: 0.0030474911512034697
Iteration: 123
Loss: 0.003019602779049154
Iteration: 124
Loss: 0.0029714577575782088
Iteration: 125
Loss: 0.002947417309326006
Iteration: 126
Loss: 0.0029120949358202334
Iteration: 127
Loss: 0.002879577714427397
Iteration: 128
Loss: 0.0028583215203181636
Iteration: 129
Loss: 0.00282037928814023
Iteration: 130
Loss: 0.0027909244143790226
Iteration: 131
Loss: 0.0027653917942832154
Iteration: 132
Loss: 0.0027488648533684815
Iteration: 133
Loss: 0.0027101357757091157
Iteration: 134
Loss: 0.002678142068339739
Iteration: 135
Loss: 0.002662419256322631
Iteration: 136
Loss: 0.002631010129911507
Iteration: 137
Loss: 0.0026257690865682756
Iteration: 138
Loss: 0.0026049270442255385
Iteration: 139
Loss: 0.0025759422555338683
Iteration: 140
Loss: 0.0025591292651370168
Iteration: 141
Loss: 0.0025309406379919225
Iteration: 142
Loss: 0.002515898602901072
Iteration: 143
Loss: 0.0025050440610091132
Iteration: 144
Loss: 0.0024858309927101177
Iteration: 145
Loss: 0.0024615032740346178
Iteration: 146
Loss: 0.002445237128995359
Iteration: 147
Loss: 0.002432873512322946
Iteration: 148
Loss: 0.0024424908574806844
Iteration: 149
Loss: 0.0024145532246088474
Iteration: 150
Loss: 0.002398077179904936
Iteration: 151
Loss: 0.002384171366873311
Iteration: 152
Loss: 0.0023844215916696845
Iteration: 153
Loss: 0.0023508613855328137
Iteration: 154
Loss: 0.0023437105658714002
Iteration: 155
Loss: 0.0023298868029264777
Iteration: 156
Loss: 0.0023388859089009647
Iteration: 157
Loss: 0.002313318554448282
Iteration: 158
Loss: 0.0022929480395893135
Iteration: 159
Loss: 0.0022932865526300015
Iteration: 160
Loss: 0.002279680497694488
Iteration: 161
Loss: 0.0022685952802620284
Iteration: 162
Loss: 0.0022718107083630635
Iteration: 163
Loss: 0.0022459468071735125
Iteration: 164
Loss: 0.002259059059547215
Iteration: 165
Loss: 0.0022421776524336056
Iteration: 166
Loss: 0.0022455304321583086
Iteration: 167
Loss: 0.0022226810929501747
Iteration: 168
Loss: 0.002203747490988817
Iteration: 169
Loss: 0.002209592905895012
Iteration: 170
Loss: 0.0022004453419912154
Iteration: 171
Loss: 0.0022076488111917748
Iteration: 172
Loss: 0.002198082978394246
Iteration: 173
Loss: 0.002182888104182827
Iteration: 174
Loss: 0.002186772835202424
Iteration: 175
Loss: 0.002172555244902558
Iteration: 176
Loss: 0.0021669454560274393
Iteration: 177
Loss: 0.0021724773729901487
Iteration: 178
Loss: 0.002167485884345341
Iteration: 179
Loss: 0.002156295103770567
Iteration: 180
Loss: 0.0021632823016971532
Iteration: 181
Loss: 0.002147562012485251
Iteration: 182
Loss: 0.0021499018270022623
Iteration: 183
Loss: 0.0021467456074499686
Iteration: 184
Loss: 0.0021359806201738736
Iteration: 185
Loss: 0.0021232395438568257
Iteration: 186
Loss: 0.00212685728059491
Iteration: 187
Loss: 0.0021269803102945952
Iteration: 188
Loss: 0.002122343661229512
Iteration: 189
Loss: 0.00212077894702372
Iteration: 190
Loss: 0.0021091479532670504
Iteration: 191
Loss: 0.0021129303310838776
Iteration: 192
Loss: 0.002100159960696701
Iteration: 193
Loss: 0.0021166959763277413
Iteration: 194
Loss: 0.0020935315902844615
Iteration: 195
Loss: 0.0020960523610616603
Iteration: 196
Loss: 0.00209851317510872
Iteration: 197
Loss: 0.0020861095342807837
Iteration: 198
Loss: 0.002097471513524197
Iteration: 199
Loss: 0.0020776524268113442
Iteration: 200
Loss: 0.0020760622985729177
Iteration: 201
Loss: 0.0020784154596806665
Iteration: 202
Loss: 0.0020778794608237905
Iteration: 203
Loss: 0.002072004798697535
Iteration: 204
Loss: 0.0020736244027275683
Iteration: 205
Loss: 0.002054942350141795
Iteration: 206
Loss: 0.0020550834881028204
Iteration: 207
Loss: 0.0020701453137983816
Iteration: 208
Loss: 0.002060612731772225
Iteration: 209
Loss: 0.002065695179115254
Iteration: 210
Loss: 0.0020511555400245436
Iteration: 211
Loss: 0.0020510419726190044
Iteration: 212
Loss: 0.002052152692624254
Iteration: 213
Loss: 0.00204942312954766
Iteration: 214
Loss: 0.0020572676823088307
Iteration: 215
Loss: 0.0020479758116757362
Iteration: 216
Loss: 0.002033654985120293
Iteration: 217
Loss: 0.002046375227353831
Iteration: 218
Loss: 0.0020479423635661968
Iteration: 219
Loss: 0.0020409870762150826
Iteration: 220
Loss: 0.00203412498074879
Iteration: 221
Loss: 0.0020344078756596258
Iteration: 222
Loss: 0.0020319295808582044
Iteration: 223
Loss: 0.002031967288064884
Iteration: 224
Loss: 0.0020189350158566744
Iteration: 225
Loss: 0.002022521637050753
Iteration: 226
Loss: 0.0020272444220963957
Iteration: 227
Loss: 0.0020294787401969475
Iteration: 228
Loss: 0.002021614730460342
Iteration: 229
Loss: 0.0020185377599285324
Iteration: 230
Loss: 0.0020178871771616
Iteration: 231
Loss: 0.0020169158914785197
Iteration: 232
Loss: 0.0020110148939349484
Iteration: 233
Loss: 0.0020158978806604153
Iteration: 234
Loss: 0.002011949284479204
Iteration: 235
Loss: 0.0020149671543593997
Iteration: 236
Loss: 0.0020063914012209308
Iteration: 237
Loss: 0.0020022216045706555
Iteration: 238
Loss: 0.002016292664892517
Iteration: 239
Loss: 0.00201824869500564
Iteration: 240
Loss: 0.0019981889409103953
Iteration: 241
Loss: 0.001997467006908775
Iteration: 242
Loss: 0.0020055246437195597
Iteration: 243
Loss: 0.002001721874765325
Iteration: 244
Loss: 0.0020045597069306164
Iteration: 245
Loss: 0.002004215529290732
Iteration: 246
Loss: 0.0019956847053679934
Iteration: 247
Loss: 0.0019974280816003136
Iteration: 248
Loss: 0.00199409181723462
Iteration: 249
Loss: 0.002002694795941707
Iteration: 250
Loss: 0.001996611144177888
Iteration: 251
Loss: 0.0019995192415080965
Iteration: 252
Loss: 0.0019951419246087712
Iteration: 253
Loss: 0.00198847013270119
Iteration: 254
Loss: 0.0019963014751627314
Iteration: 255
Loss: 0.001990776584290604
Iteration: 256
Loss: 0.002004020533626672
Iteration: 257
Loss: 0.0019912152755551223
Iteration: 258
Loss: 0.00198357927232481
Iteration: 259
Loss: 0.0019873801360429243
Iteration: 260
Loss: 0.0019856542135906837
Iteration: 261
Loss: 0.0019784549197250207
Iteration: 262
Loss: 0.0019869807108146387
Iteration: 263
Loss: 0.0019842869752631862
Iteration: 264
Loss: 0.0019811507422340716
Iteration: 265
Loss: 0.0019841379082829852
Iteration: 266
Loss: 0.0019891908258858433
Iteration: 267
Loss: 0.0019743367220365965
Iteration: 268
Loss: 0.001971528662380013
Iteration: 269
Loss: 0.001981769100672043
Iteration: 270
Loss: 0.001974461976404688
Iteration: 271
Loss: 0.0019823298164893216
Iteration: 272
Loss: 0.0019817788852378726
Iteration: 273
Loss: 0.0019754370384872325
Iteration: 274
Loss: 0.00197272103921533
Iteration: 275
Loss: 0.0019710971207198935
Iteration: 276
Loss: 0.0019821279253469915
Iteration: 277
Loss: 0.0019693523446643133
Iteration: 278
Loss: 0.0019750240906255276
Iteration: 279
Loss: 0.0019663842437538975
Iteration: 280
Loss: 0.0019730640640595884
Iteration: 281
Loss: 0.001960220437201603
Iteration: 282
Loss: 0.0019638424415550216
Iteration: 283
Loss: 0.0019666707617367004
Iteration: 284
Loss: 0.0019744920200759136
Iteration: 285
Loss: 0.0019580000335714075
Iteration: 286
Loss: 0.001953185403935339
Iteration: 287
Loss: 0.0019626804902899736
Iteration: 288
Loss: 0.001967164036170466
Iteration: 289
Loss: 0.0019685505270367353
Iteration: 290
Loss: 0.001965333822626257
Iteration: 291
Loss: 0.0019624338999231594
Iteration: 292
Loss: 0.001967340474948287
Iteration: 293
Loss: 0.0019737309463912756
Iteration: 294
Loss: 0.0019613659497713896
Iteration: 295
Loss: 0.0019771158652629977
Iteration: 296
Loss: 0.001962768443485313
Iteration: 297
Loss: 0.0019623740709659348
Iteration: 298
Loss: 0.001956397001688346
Iteration: 299
Loss: 0.0019529788817347186
Iteration: 300
Loss: 0.0019535173210542558
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.2470119521912351
accuracy: 0.9602105263157895
confusion: 62 250 128 9060
precision: 0.1987179487179487
recall: 0.3263157894736842
Loading train data...
Loading val data...
Loading test data...
Training the TuckER model...
Number of training data points: 84683
Starting training...
Iteration: 1
Loss: 0.7018597409492586
Iteration: 2
Loss: 0.6942529343977207
Iteration: 3
Loss: 0.6743225927759962
Iteration: 4
Loss: 0.6347207812274375
Iteration: 5
Loss: 0.5797503990371052
Iteration: 6
Loss: 0.5201928092212211
Iteration: 7
Loss: 0.46988638108823355
Iteration: 8
Loss: 0.42816515484961065
Iteration: 9
Loss: 0.39664013174975793
Iteration: 10
Loss: 0.37197827347894996
Iteration: 11
Loss: 0.35170203629063396
Iteration: 12
Loss: 0.3335572960900097
Iteration: 13
Loss: 0.3179041390738836
Iteration: 14
Loss: 0.3023978740703769
Iteration: 15
Loss: 0.2881720978312376
Iteration: 16
Loss: 0.27309013939485316
Iteration: 17
Loss: 0.25935800773341483
Iteration: 18
Loss: 0.24405202941923607
Iteration: 19
Loss: 0.2305712656276982
Iteration: 20
Loss: 0.2170508142288138
Iteration: 21
Loss: 0.203055343249949
Iteration: 22
Loss: 0.1899120635012301
Iteration: 23
Loss: 0.17671702038951037
Iteration: 24
Loss: 0.16409651608001896
Iteration: 25
Loss: 0.15236557038818918
Iteration: 26
Loss: 0.14100361942518047
Iteration: 27
Loss: 0.13063765408062353
Iteration: 28
Loss: 0.11981350319778047
Iteration: 29
Loss: 0.11055549170549323
Iteration: 30
Loss: 0.1016773500638764
Iteration: 31
Loss: 0.09415569756089187
Iteration: 32
Loss: 0.08679489146282034
Iteration: 33
Loss: 0.07963913561003964
Iteration: 34
Loss: 0.07296792108838152
Iteration: 35
Loss: 0.06774220224924205
Iteration: 36
Loss: 0.061918419127057235
Iteration: 37
Loss: 0.05749490171125749
Iteration: 38
Loss: 0.05317833242801631
Iteration: 39
Loss: 0.04916014177043263
Iteration: 40
Loss: 0.04564146833812318
Iteration: 41
Loss: 0.042394523622422686
Iteration: 42
Loss: 0.03935057656248895
Iteration: 43
Loss: 0.03662744382532632
Iteration: 44
Loss: 0.034067079879161785
Iteration: 45
Loss: 0.03181704706171664
Iteration: 46
Loss: 0.029976104318005282
Iteration: 47
Loss: 0.028157603063779634
Iteration: 48
Loss: 0.0263429814495328
Iteration: 49
Loss: 0.024884941192661842
Iteration: 50
Loss: 0.023195095133127237
Iteration: 51
Loss: 0.02215672297993811
Iteration: 52
Loss: 0.020804829963641924
Iteration: 53
Loss: 0.019617553550477434
Iteration: 54
Loss: 0.018611600009224762
Iteration: 55
Loss: 0.017645668579129185
Iteration: 56
Loss: 0.01676583353702615
Iteration: 57
Loss: 0.015995830808561748
Iteration: 58
Loss: 0.015220975605543793
Iteration: 59
Loss: 0.014559211232131573
Iteration: 60
Loss: 0.014054304363614902
Iteration: 61
Loss: 0.01328150295402582
Iteration: 62
Loss: 0.012757766140034286
Iteration: 63
Loss: 0.01227184973384549
Iteration: 64
Loss: 0.011710674297518847
Iteration: 65
Loss: 0.011298428789326331
Iteration: 66
Loss: 0.010842982861326962
Iteration: 67
Loss: 0.010483369322084799
Iteration: 68
Loss: 0.010084875882035348
Iteration: 69
Loss: 0.009694355653553473
Iteration: 70
Loss: 0.009389951575274875
Iteration: 71
Loss: 0.009047594250793137
Iteration: 72
Loss: 0.00877547416290859
Iteration: 73
Loss: 0.008465908598381935
Iteration: 74
Loss: 0.008177256228674839
Iteration: 75
Loss: 0.00791884888894856
Iteration: 76
Loss: 0.007705228885908316
Iteration: 77
Loss: 0.007457818892761701
Iteration: 78
Loss: 0.007232963726515087
Iteration: 79
Loss: 0.00702358076984926
Iteration: 80
Loss: 0.006848941852406758
Iteration: 81
Loss: 0.006627548389454804
Iteration: 82
Loss: 0.0064981714651988045
Iteration: 83
Loss: 0.00631919598615751
Iteration: 84
Loss: 0.006149666145353056
Iteration: 85
Loss: 0.005977063303495326
Iteration: 86
Loss: 0.005854564196452862
Iteration: 87
Loss: 0.005726984724765871
Iteration: 88
Loss: 0.0055621731231307115
Iteration: 89
Loss: 0.005448185361740066
Iteration: 90
Loss: 0.005311774283011512
Iteration: 91
Loss: 0.005190344862412752
Iteration: 92
Loss: 0.00507180325732362
Iteration: 93
Loss: 0.004947190530734454
Iteration: 94
Loss: 0.004894718050775005
Iteration: 95
Loss: 0.004770971346283104
Iteration: 96
Loss: 0.004648522661271982
Iteration: 97
Loss: 0.004582550907621115
Iteration: 98
Loss: 0.00449467483601283
Iteration: 99
Loss: 0.004376395423783035
Iteration: 100
Loss: 0.004306946960618583
Iteration: 101
Loss: 0.004246802725147728
Iteration: 102
Loss: 0.004155030899976448
Iteration: 103
Loss: 0.004078189416493221
Iteration: 104
Loss: 0.004008940347220476
Iteration: 105
Loss: 0.0039401045807341975
Iteration: 106
Loss: 0.0038889432354353186
Iteration: 107
Loss: 0.003806494003184503
Iteration: 108
Loss: 0.0037422053907748038
Iteration: 109
Loss: 0.0036815930663312716
Iteration: 110
Loss: 0.0036244648281575703
Iteration: 111
Loss: 0.003571758631662261
Iteration: 112
Loss: 0.003515658085234463
Iteration: 113
Loss: 0.0034624665277078748
Iteration: 114
Loss: 0.003417454043789426
Iteration: 115
Loss: 0.0033524271254087
Iteration: 116
Loss: 0.003313435840693007
Iteration: 117
Loss: 0.0032718640400051344
Iteration: 118
Loss: 0.0032192475230592055
Iteration: 119
Loss: 0.003174969202429965
Iteration: 120
Loss: 0.003153882199534919
Iteration: 121
Loss: 0.0030857510069703183
Iteration: 122
Loss: 0.0030675785951089205
Iteration: 123
Loss: 0.003011402287861196
Iteration: 124
Loss: 0.0029827484253385082
Iteration: 125
Loss: 0.0029357602853873153
Iteration: 126
Loss: 0.0029183163040703754
Iteration: 127
Loss: 0.0028779378711677544
Iteration: 128
Loss: 0.0028444787674787933
Iteration: 129
Loss: 0.0028140957796628156
Iteration: 130
Loss: 0.0028011068962968703
Iteration: 131
Loss: 0.0027575252986535796
Iteration: 132
Loss: 0.002727316079161516
Iteration: 133
Loss: 0.0027168104623830538
Iteration: 134
Loss: 0.002682797139419652
Iteration: 135
Loss: 0.0026654571283426954
Iteration: 136
Loss: 0.002643584350419299
Iteration: 137
Loss: 0.002623283309952878
Iteration: 138
Loss: 0.002602790842945801
Iteration: 139
Loss: 0.0025824064292331657
Iteration: 140
Loss: 0.0025482248946479182
Iteration: 141
Loss: 0.00253183648512676
Iteration: 142
Loss: 0.002526663990532298
Iteration: 143
Loss: 0.002487199469015184
Iteration: 144
Loss: 0.0024844837181906146
Iteration: 145
Loss: 0.0024755360677883756
Iteration: 146
Loss: 0.0024426926790577608
Iteration: 147
Loss: 0.002427414002851015
Iteration: 148
Loss: 0.0024261652156937776
Iteration: 149
Loss: 0.0024110406421397517
Iteration: 150
Loss: 0.0023821925079995174
Iteration: 151
Loss: 0.0023936340755714874
Iteration: 152
Loss: 0.0023644191139127785
Iteration: 153
Loss: 0.002359976319073722
Iteration: 154
Loss: 0.0023447549561174904
Iteration: 155
Loss: 0.0023259789393306144
Iteration: 156
Loss: 0.0023275180691398863
Iteration: 157
Loss: 0.00231266222190021
Iteration: 158
Loss: 0.0022975467514564714
Iteration: 159
Loss: 0.0022967350356871397
Iteration: 160
Loss: 0.002293783242859673
Iteration: 161
Loss: 0.0022691019027062304
Iteration: 162
Loss: 0.002275193851191278
Iteration: 163
Loss: 0.002262846419087997
Iteration: 164
Loss: 0.0022615668131038547
Iteration: 165
Loss: 0.0022517517407810907
Iteration: 166
Loss: 0.0022401529415397017
Iteration: 167
Loss: 0.0022384338989490417
Iteration: 168
Loss: 0.0022272535077318915
Iteration: 169
Loss: 0.0022126151186345915
Iteration: 170
Loss: 0.002201630838964952
Iteration: 171
Loss: 0.0022046598080522947
Iteration: 172
Loss: 0.002186481774182672
Iteration: 173
Loss: 0.0021972533403441493
Iteration: 174
Loss: 0.002185936814185414
Iteration: 175
Loss: 0.002189424385816404
Iteration: 176
Loss: 0.002182166171600906
Iteration: 177
Loss: 0.0021804999929239473
Iteration: 178
Loss: 0.0021660121984030233
Iteration: 179
Loss: 0.0021601016462848683
Iteration: 180
Loss: 0.0021509667815890437
Iteration: 181
Loss: 0.0021457633276174707
Iteration: 182
Loss: 0.0021429958543172332
Iteration: 183
Loss: 0.002156047529552314
Iteration: 184
Loss: 0.002139656888264254
Iteration: 185
Loss: 0.0021440908170827643
Iteration: 186
Loss: 0.0021290233466637934
Iteration: 187
Loss: 0.002119903882863227
Iteration: 188
Loss: 0.002126194251326435
Iteration: 189
Loss: 0.002118235544824019
Iteration: 190
Loss: 0.0021176619892485623
Iteration: 191
Loss: 0.0021207667731043952
Iteration: 192
Loss: 0.0020964499548781755
Iteration: 193
Loss: 0.002098724903345744
Iteration: 194
Loss: 0.0021045710337802585
Iteration: 195
Loss: 0.0020961255564854093
Iteration: 196
Loss: 0.0020886481393174064
Iteration: 197
Loss: 0.0020910658612393026
Iteration: 198
Loss: 0.0020820830789644545
Iteration: 199
Loss: 0.0020900244764960937
Iteration: 200
Loss: 0.0020820759634328323
Iteration: 201
Loss: 0.0020733307734722406
Iteration: 202
Loss: 0.002078119529093184
Iteration: 203
Loss: 0.0020757850711006763
Iteration: 204
Loss: 0.002065182055869117
Iteration: 205
Loss: 0.0020638601570503743
Iteration: 206
Loss: 0.002066563902899805
Iteration: 207
Loss: 0.002059893205580188
Iteration: 208
Loss: 0.002060975758756352
Iteration: 209
Loss: 0.0020501268658469
Iteration: 210
Loss: 0.002055926408857198
Iteration: 211
Loss: 0.0020566710949557403
Iteration: 212
Loss: 0.002069510293772399
Iteration: 213
Loss: 0.002065261644197673
Iteration: 214
Loss: 0.0020568827720835988
Iteration: 215
Loss: 0.00203351703012453
Iteration: 216
Loss: 0.0020487554919360797
Iteration: 217
Loss: 0.0020449246427544005
Iteration: 218
Loss: 0.002032129698796425
Iteration: 219
Loss: 0.002036125910263963
Iteration: 220
Loss: 0.0020289960325831865
Iteration: 221
Loss: 0.002033079140989973
Iteration: 222
Loss: 0.0020424935572053785
Iteration: 223
Loss: 0.0020295255760684975
Iteration: 224
Loss: 0.0020382027950978316
Iteration: 225
Loss: 0.0020256211524034237
Iteration: 226
Loss: 0.0020330025908173767
Iteration: 227
Loss: 0.0020278566786678643
Iteration: 228
Loss: 0.0020233996683822536
Iteration: 229
Loss: 0.002016666035803898
Iteration: 230
Loss: 0.0020182933076284826
Iteration: 231
Loss: 0.002030025983070273
Iteration: 232
Loss: 0.0020174806065880128
Iteration: 233
Loss: 0.0020125448723641655
Iteration: 234
Loss: 0.002014349592372593
Iteration: 235
Loss: 0.002007893271028723
Iteration: 236
Loss: 0.002016486923732772
Iteration: 237
Loss: 0.0020084643421308477
Iteration: 238
Loss: 0.00201051529474193
Iteration: 239
Loss: 0.0020058745257102136
Iteration: 240
Loss: 0.00200549517218705
Iteration: 241
Loss: 0.002001953782604598
Iteration: 242
Loss: 0.0020106744131913818
Iteration: 243
Loss: 0.002017272387847032
Iteration: 244
Loss: 0.0020020216797115055
Iteration: 245
Loss: 0.002004941655663637
Iteration: 246
Loss: 0.0019980511463406248
Iteration: 247
Loss: 0.002002818180566154
Iteration: 248
Loss: 0.001990300936840202
Iteration: 249
Loss: 0.0020003701196756305
Iteration: 250
Loss: 0.0019992459437628163
Iteration: 251
Loss: 0.0019956302315155727
Iteration: 252
Loss: 0.001984899498738076
Iteration: 253
Loss: 0.0020022970445385973
Iteration: 254
Loss: 0.0019831136578875707
Iteration: 255
Loss: 0.0019957008601234455
Iteration: 256
Loss: 0.0019869019743055105
Iteration: 257
Loss: 0.0019961595663051234
Iteration: 258
Loss: 0.0019849504659338515
Iteration: 259
Loss: 0.0019900724405377376
Iteration: 260
Loss: 0.0019846339823632706
Iteration: 261
Loss: 0.0019932227356859096
Iteration: 262
Loss: 0.0019906323698417445
Iteration: 263
Loss: 0.0019829453270110054
Iteration: 264
Loss: 0.0019848138369369978
Iteration: 265
Loss: 0.0019915276377393703
Iteration: 266
Loss: 0.0019847672196991015
Iteration: 267
Loss: 0.001969867015382411
Iteration: 268
Loss: 0.0019744314159630093
Iteration: 269
Loss: 0.0019782209504827313
Iteration: 270
Loss: 0.001983723222709647
Iteration: 271
Loss: 0.001975862675619016
Iteration: 272
Loss: 0.0019852293033448118
Iteration: 273
Loss: 0.0019714249874374307
Iteration: 274
Loss: 0.0019853739422836863
Iteration: 275
Loss: 0.001972690860672695
Iteration: 276
Loss: 0.001976902055747171
Iteration: 277
Loss: 0.0019691004247472783
Iteration: 278
Loss: 0.0019878147854242565
Iteration: 279
Loss: 0.001985674271098815
Iteration: 280
Loss: 0.001969444991384701
Iteration: 281
Loss: 0.0019698190806637997
Iteration: 282
Loss: 0.0019676138209633346
Iteration: 283
Loss: 0.0019717539033125633
Iteration: 284
Loss: 0.001957950440644309
Iteration: 285
Loss: 0.0019643044244020995
Iteration: 286
Loss: 0.001979769722887928
Iteration: 287
Loss: 0.0019638451801544826
Iteration: 288
Loss: 0.0019644724727585546
Iteration: 289
Loss: 0.0019604337881546376
Iteration: 290
Loss: 0.0019626880217934162
Iteration: 291
Loss: 0.001955616333317466
Iteration: 292
Loss: 0.001961188855092609
Iteration: 293
Loss: 0.001973160707232792
Iteration: 294
Loss: 0.0019607426749156196
Iteration: 295
Loss: 0.001956670241225965
Iteration: 296
Loss: 0.0019546090128331832
Iteration: 297
Loss: 0.0019584214882698
Iteration: 298
Loss: 0.0019689878269961874
Iteration: 299
Loss: 0.001963558176634606
Iteration: 300
Loss: 0.00196139236753125
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.2901098901098901
accuracy: 0.966
confusion: 66 199 124 9111
precision: 0.2490566037735849
recall: 0.3473684210526316
Finding results in directory: ../../output/ecoli/tucker
[{'f1': 0.039383368965905254, 'accuracy': 0.025734664764621967, 'tn': 202, 'fp': 34147, 'fn': 1, 'tp': 700, 'precision': 0.020087812437225586, 'recall': 0.9985734664764622}, {'f1': 0.05075311067452521, 'accuracy': 0.8345791726105564, 'tn': 29097, 'fp': 5252, 'fn': 546, 'tp': 155, 'precision': 0.028666543369705935, 'recall': 0.2211126961483595}, {'f1': 0.04377335468618928, 'accuracy': 0.6422539229671897, 'tn': 22224, 'fp': 12125, 'fn': 414, 'tp': 287, 'precision': 0.023122784402191426, 'recall': 0.4094151212553495}, {'f1': 0.047874179052886275, 'accuracy': 0.6856490727532097, 'tn': 23755, 'fp': 10594, 'fn': 424, 'tp': 277, 'precision': 0.02548063655597461, 'recall': 0.39514978601997147}, {'f1': 0.041092112520683954, 'accuracy': 0.8015977175463623, 'tn': 27947, 'fp': 6402, 'fn': 552, 'tp': 149, 'precision': 0.02274461914211571, 'recall': 0.21255349500713266}]
Aggregated results before taking average:
{'f1': [0.039383368965905254, 0.05075311067452521, 0.04377335468618928, 0.047874179052886275, 0.041092112520683954], 'accuracy': [0.025734664764621967, 0.8345791726105564, 0.6422539229671897, 0.6856490727532097, 0.8015977175463623], 'tn': [202, 29097, 22224, 23755, 27947], 'fp': [34147, 5252, 12125, 10594, 6402], 'fn': [1, 546, 414, 424, 552], 'tp': [700, 155, 287, 277, 149], 'precision': [0.020087812437225586, 0.028666543369705935, 0.023122784402191426, 0.02548063655597461, 0.02274461914211571], 'recall': [0.9985734664764622, 0.2211126961483595, 0.4094151212553495, 0.39514978601997147, 0.21255349500713266]}
Aggregated results after taking average:
{'f1': '0.045±0.004', 'accuracy': '0.598±0.295', 'tn': '20645.000±10534.107', 'fp': '13704.000±10534.107', 'fn': '387.400±201.792', 'tp': '313.600±201.792', 'precision': '0.024±0.003', 'recall': '0.447±0.288'}
num_iterations: [5]
batch_size: [128]
learning_rate: [0.0002]
decay_rate: [1.0]
ent_vec_dim: [200]
rel_vec_dim: [30]
input_dropout: [0.2]
hidden_dropout1: [0.4]
hidden_dropout2: [0.5]
label_smoothing: [0.1]
Loading train data...
Loading test data...
Training the TuckER model...
Number of training data points: 84873
Starting training...
Iteration: 1
Loss: 0.7015970701194671
Iteration: 2
Loss: 0.6937036492738379
Iteration: 3
Loss: 0.6736766064023397
Iteration: 4
Loss: 0.6324396765375712
Iteration: 5
Loss: 0.5744377755257021
Mode: final
