  0%|          | 0/28050 [00:00<?, ?it/s]  0%|          | 1/28050 [00:09<72:29:48,  9.30s/it]  4%|▎         | 1001/28050 [00:09<02:59, 151.01it/s] 10%|█         | 2898/28050 [00:09<00:45, 548.14it/s] 14%|█▍        | 4046/28050 [00:18<01:40, 238.31it/s] 21%|██▏       | 6001/28050 [00:27<01:37, 225.15it/s] 25%|██▍       | 7001/28050 [00:27<01:09, 301.41it/s] 29%|██▊       | 8001/28050 [00:28<00:49, 405.64it/s] 32%|███▏      | 9001/28050 [00:37<01:22, 232.24it/s] 36%|███▌      | 10001/28050 [00:37<00:56, 321.97it/s] 39%|███▉      | 11001/28050 [00:37<00:38, 446.14it/s] 43%|████▎     | 12001/28050 [00:46<01:08, 235.47it/s] 46%|████▋     | 13001/28050 [00:46<00:45, 329.85it/s] 50%|████▉     | 14001/28050 [00:46<00:30, 461.82it/s] 53%|█████▎    | 15001/28050 [00:55<00:54, 237.58it/s] 57%|█████▋    | 16001/28050 [00:55<00:36, 333.70it/s] 61%|██████    | 17001/28050 [00:56<00:23, 465.69it/s] 64%|██████▍   | 18001/28050 [01:05<00:42, 235.66it/s] 68%|██████▊   | 19001/28050 [01:05<00:27, 332.14it/s] 75%|███████▍  | 21001/28050 [01:14<00:26, 266.29it/s] 78%|███████▊  | 22001/28050 [01:14<00:17, 349.34it/s] 86%|████████▌ | 24001/28050 [01:24<00:14, 277.12it/s] 89%|████████▉ | 25001/28050 [01:24<00:08, 353.55it/s] 96%|█████████▋| 27001/28050 [01:32<00:03, 296.58it/s]100%|██████████| 28050/28050 [01:32<00:00, 302.55it/s]
  0%|          | 0/28050 [00:00<?, ?it/s]  0%|          | 1/28050 [00:09<73:14:10,  9.40s/it]  4%|▎         | 1001/28050 [00:09<03:00, 149.55it/s]  7%|▋         | 2001/28050 [00:09<01:13, 353.91it/s] 11%|█         | 3001/28050 [00:18<02:21, 177.26it/s] 14%|█▍        | 4001/28050 [00:18<01:23, 288.18it/s] 18%|█▊        | 5001/28050 [00:18<00:52, 441.07it/s] 21%|██▏       | 6001/28050 [00:28<01:41, 217.61it/s] 25%|██▍       | 7001/28050 [00:28<01:06, 318.21it/s] 29%|██▊       | 8001/28050 [00:28<00:44, 452.82it/s] 32%|███▏      | 9001/28050 [00:37<01:22, 230.11it/s] 36%|███▌      | 10001/28050 [00:37<00:54, 328.92it/s] 39%|███▉      | 11001/28050 [00:37<00:37, 459.54it/s] 43%|████▎     | 12001/28050 [00:46<01:08, 234.51it/s] 46%|████▋     | 13001/28050 [00:46<00:45, 332.70it/s] 50%|████▉     | 14001/28050 [00:47<00:30, 463.37it/s] 53%|█████▎    | 15001/28050 [00:56<00:54, 237.40it/s] 57%|█████▋    | 16001/28050 [00:56<00:36, 334.61it/s] 61%|██████    | 17001/28050 [00:56<00:23, 464.40it/s] 64%|██████▍   | 18001/28050 [01:05<00:42, 235.64it/s] 68%|██████▊   | 19001/28050 [01:05<00:27, 332.10it/s] 71%|███████▏  | 20001/28050 [01:05<00:17, 460.59it/s] 75%|███████▍  | 21001/28050 [01:14<00:29, 235.26it/s] 78%|███████▊  | 22001/28050 [01:15<00:18, 332.18it/s] 82%|████████▏ | 23001/28050 [01:15<00:10, 462.87it/s] 86%|████████▌ | 24001/28050 [01:24<00:17, 237.41it/s] 89%|████████▉ | 25001/28050 [01:24<00:09, 335.03it/s] 93%|█████████▎| 26001/28050 [01:24<00:04, 468.68it/s] 96%|█████████▋| 27001/28050 [01:32<00:04, 253.74it/s]100%|██████████| 28050/28050 [01:32<00:00, 302.53it/s]
  0%|          | 0/28050 [00:00<?, ?it/s]  0%|          | 1/28050 [00:09<72:50:58,  9.35s/it]  7%|▋         | 2001/28050 [00:09<01:27, 297.34it/s] 11%|█         | 3001/28050 [00:18<02:23, 174.73it/s] 14%|█▍        | 4001/28050 [00:18<01:27, 275.49it/s] 21%|██▏       | 6001/28050 [00:27<01:30, 242.37it/s] 25%|██▍       | 7001/28050 [00:27<01:04, 328.14it/s] 29%|██▊       | 8001/28050 [00:28<00:44, 446.20it/s] 32%|███▏      | 9001/28050 [00:37<01:19, 238.42it/s] 39%|███▉      | 11001/28050 [00:37<00:40, 424.20it/s] 43%|████▎     | 12001/28050 [00:46<01:04, 249.75it/s] 46%|████▋     | 13001/28050 [00:46<00:45, 332.82it/s] 53%|█████▎    | 15001/28050 [00:55<00:48, 271.07it/s] 57%|█████▋    | 16001/28050 [00:56<00:34, 348.68it/s] 64%|██████▍   | 18001/28050 [01:05<00:35, 280.14it/s] 68%|██████▊   | 19001/28050 [01:05<00:25, 356.42it/s] 71%|███████▏  | 20001/28050 [01:05<00:17, 463.13it/s] 75%|███████▍  | 21001/28050 [01:14<00:27, 252.47it/s] 78%|███████▊  | 22001/28050 [01:14<00:17, 341.02it/s] 86%|████████▌ | 24001/28050 [01:24<00:14, 273.96it/s] 89%|████████▉ | 25001/28050 [01:24<00:08, 354.90it/s] 96%|█████████▋| 27001/28050 [01:32<00:03, 298.13it/s]100%|██████████| 28050/28050 [01:32<00:00, 303.20it/s]
  0%|          | 0/28000 [00:00<?, ?it/s]  0%|          | 1/28000 [00:09<72:29:21,  9.32s/it]  7%|▋         | 2001/28000 [00:09<01:27, 296.79it/s] 11%|█         | 3001/28000 [00:18<02:22, 174.85it/s] 14%|█▍        | 4001/28000 [00:18<01:27, 274.44it/s] 18%|█▊        | 5001/28000 [00:18<00:55, 411.52it/s] 21%|██▏       | 6001/28000 [00:27<01:41, 217.07it/s] 25%|██▌       | 7001/28000 [00:28<01:06, 314.81it/s] 29%|██▊       | 8001/28000 [00:28<00:45, 443.23it/s] 32%|███▏      | 9001/28000 [00:37<01:22, 230.00it/s] 39%|███▉      | 11001/28000 [00:37<00:40, 418.62it/s] 43%|████▎     | 12001/28000 [00:47<01:06, 241.09it/s] 50%|█████     | 14001/28000 [00:47<00:34, 401.93it/s] 54%|█████▎    | 15001/28000 [00:56<00:51, 250.30it/s] 61%|██████    | 17001/28000 [00:56<00:27, 402.42it/s] 64%|██████▍   | 18001/28000 [01:05<00:39, 252.43it/s] 71%|███████▏  | 20001/28000 [01:06<00:20, 399.87it/s] 75%|███████▌  | 21001/28000 [01:15<00:27, 254.32it/s] 82%|████████▏ | 23001/28000 [01:15<00:12, 400.36it/s] 86%|████████▌ | 24001/28000 [01:24<00:15, 257.05it/s] 93%|█████████▎| 26001/28000 [01:25<00:04, 405.50it/s] 96%|█████████▋| 27001/28000 [01:33<00:03, 269.59it/s]100%|██████████| 28000/28000 [01:33<00:00, 299.94it/s]
  0%|          | 0/28000 [00:00<?, ?it/s]  0%|          | 1/28000 [00:10<84:21:53, 10.85s/it]  7%|▋         | 2001/28000 [00:11<01:41, 257.33it/s] 11%|█         | 3102/28000 [00:20<02:28, 167.69it/s] 14%|█▍        | 4001/28000 [00:20<01:35, 251.07it/s] 18%|█▊        | 5001/28000 [00:20<01:00, 379.89it/s] 21%|██▏       | 6001/28000 [00:30<01:53, 193.61it/s] 25%|██▌       | 7001/28000 [00:31<01:15, 279.47it/s] 29%|██▊       | 8001/28000 [00:31<00:52, 377.82it/s] 32%|███▏      | 9001/28000 [00:42<01:37, 195.46it/s] 36%|███▌      | 10001/28000 [00:42<01:04, 279.74it/s] 39%|███▉      | 11001/28000 [00:42<00:43, 393.67it/s] 43%|████▎     | 12001/28000 [00:52<01:15, 210.53it/s] 50%|█████     | 14001/28000 [00:52<00:36, 383.04it/s] 50%|█████     | 14001/28000 [01:05<00:36, 383.04it/s] 54%|█████▎    | 15001/28000 [01:05<01:05, 198.78it/s] 57%|█████▋    | 16001/28000 [01:05<00:44, 266.96it/s] 61%|██████    | 17001/28000 [01:05<00:31, 345.47it/s] 64%|██████▍   | 18001/28000 [01:24<01:14, 134.72it/s] 71%|███████▏  | 20001/28000 [01:25<00:33, 240.17it/s] 75%|███████▌  | 21001/28000 [01:36<00:40, 171.45it/s] 82%|████████▏ | 23001/28000 [01:36<00:17, 286.36it/s] 86%|████████▌ | 24001/28000 [01:50<00:23, 173.32it/s] 89%|████████▉ | 25001/28000 [01:50<00:13, 227.70it/s] 93%|█████████▎| 26001/28000 [01:50<00:06, 302.28it/s] 96%|█████████▋| 27001/28000 [01:59<00:04, 205.88it/s]100%|██████████| 28000/28000 [01:59<00:00, 234.54it/s]
  0%|          | 0/28050 [00:00<?, ?it/s]  0%|          | 1/28050 [00:09<75:46:23,  9.73s/it]  7%|▋         | 2001/28050 [00:09<01:31, 286.03it/s] 11%|█         | 3001/28050 [00:19<02:26, 170.58it/s] 14%|█▍        | 4001/28050 [00:19<01:29, 269.35it/s] 20%|█▉        | 5513/28050 [00:19<00:46, 479.53it/s] 23%|██▎       | 6445/28050 [00:28<01:34, 227.58it/s] 29%|██▊       | 8001/28050 [00:29<00:54, 370.74it/s] 32%|███▏      | 9001/28050 [00:38<01:25, 223.62it/s] 39%|███▉      | 11001/28050 [00:38<00:43, 389.17it/s] 43%|████▎     | 12001/28050 [00:47<01:06, 239.87it/s] 46%|████▋     | 13001/28050 [00:48<00:47, 318.45it/s] 53%|█████▎    | 15001/28050 [00:57<00:49, 263.88it/s] 57%|█████▋    | 16001/28050 [00:57<00:35, 339.78it/s] 64%|██████▍   | 18001/28050 [01:06<00:36, 274.04it/s] 71%|███████▏  | 20001/28050 [01:07<00:19, 423.45it/s] 75%|███████▍  | 21001/28050 [01:16<00:27, 260.41it/s] 82%|████████▏ | 23001/28050 [01:16<00:12, 408.25it/s] 86%|████████▌ | 24001/28050 [01:26<00:15, 258.14it/s] 96%|█████████▋| 27001/28050 [01:34<00:03, 294.72it/s]100%|██████████| 28050/28050 [01:34<00:00, 296.19it/s]
  0%|          | 0/28050 [00:00<?, ?it/s]  0%|          | 1/28050 [00:09<74:31:35,  9.57s/it]  7%|▋         | 2001/28050 [00:09<01:30, 288.82it/s] 11%|█         | 3001/28050 [00:19<02:27, 170.24it/s] 18%|█▊        | 5001/28050 [00:19<01:03, 364.62it/s] 21%|██▏       | 6001/28050 [00:28<01:40, 218.76it/s] 29%|██▊       | 8001/28050 [00:28<00:52, 383.04it/s] 32%|███▏      | 9001/28050 [00:38<01:20, 235.85it/s] 39%|███▉      | 11001/28050 [00:38<00:43, 391.26it/s] 43%|████▎     | 12001/28050 [00:47<01:05, 245.86it/s] 46%|████▋     | 13001/28050 [00:47<00:46, 322.38it/s] 50%|████▉     | 14001/28050 [00:47<00:32, 428.54it/s] 53%|█████▎    | 15001/28050 [00:57<00:55, 236.36it/s] 61%|██████    | 17001/28050 [00:57<00:27, 409.03it/s] 64%|██████▍   | 18001/28050 [01:06<00:40, 245.56it/s] 71%|███████▏  | 20001/28050 [01:07<00:19, 403.37it/s] 75%|███████▍  | 21001/28050 [01:16<00:28, 248.77it/s] 78%|███████▊  | 22001/28050 [01:16<00:18, 325.16it/s] 82%|████████▏ | 23001/28050 [01:16<00:11, 429.95it/s] 86%|████████▌ | 24001/28050 [01:26<00:17, 236.08it/s] 89%|████████▉ | 25001/28050 [01:26<00:09, 323.66it/s] 96%|█████████▋| 27001/28050 [01:35<00:03, 274.50it/s]100%|██████████| 28050/28050 [01:35<00:00, 295.25it/s]
  0%|          | 0/28050 [00:00<?, ?it/s]  0%|          | 1/28050 [00:09<76:26:49,  9.81s/it]  7%|▋         | 2001/28050 [00:09<01:31, 284.34it/s] 11%|█         | 3001/28050 [00:19<02:27, 170.29it/s] 14%|█▍        | 4001/28050 [00:19<01:29, 268.10it/s] 18%|█▊        | 5001/28050 [00:19<00:56, 405.14it/s] 21%|██▏       | 6001/28050 [00:28<01:43, 212.73it/s] 25%|██▍       | 7001/28050 [00:28<01:08, 307.64it/s] 29%|██▊       | 8001/28050 [00:28<00:45, 437.88it/s] 32%|███▏      | 9001/28050 [00:38<01:24, 224.66it/s] 39%|███▉      | 11001/28050 [00:38<00:41, 412.92it/s] 43%|████▎     | 12001/28050 [00:47<01:05, 243.40it/s] 46%|████▋     | 13001/28050 [00:47<00:45, 327.75it/s] 50%|████▉     | 14001/28050 [00:47<00:31, 441.14it/s] 53%|█████▎    | 15001/28050 [00:57<00:55, 237.14it/s] 57%|█████▋    | 16001/28050 [00:57<00:36, 328.89it/s] 61%|██████    | 17001/28050 [00:57<00:24, 451.76it/s] 64%|██████▍   | 18001/28050 [01:06<00:43, 233.38it/s] 68%|██████▊   | 19001/28050 [01:06<00:27, 327.61it/s] 71%|███████▏  | 20001/28050 [01:06<00:17, 455.31it/s] 75%|███████▍  | 21001/28050 [01:16<00:30, 231.89it/s] 78%|███████▊  | 22001/28050 [01:16<00:18, 326.88it/s] 82%|████████▏ | 23001/28050 [01:16<00:11, 454.99it/s] 86%|████████▌ | 24001/28050 [01:25<00:17, 234.02it/s] 89%|████████▉ | 25001/28050 [01:25<00:09, 329.85it/s] 93%|█████████▎| 26001/28050 [01:25<00:04, 461.71it/s] 96%|█████████▋| 27001/28050 [01:34<00:04, 250.16it/s]100%|██████████| 28050/28050 [01:34<00:00, 297.62it/s]
  0%|          | 0/28000 [00:00<?, ?it/s]  0%|          | 1/28000 [00:09<74:00:31,  9.52s/it]  7%|▋         | 2001/28000 [00:09<01:29, 291.90it/s] 11%|█         | 3001/28000 [00:18<02:25, 171.23it/s] 14%|█▍        | 4001/28000 [00:19<01:29, 269.39it/s] 21%|██▏       | 6001/28000 [00:28<01:31, 240.17it/s] 25%|██▌       | 7001/28000 [00:28<01:04, 323.18it/s] 29%|██▊       | 8001/28000 [00:28<00:45, 439.14it/s] 32%|███▏      | 9001/28000 [00:37<01:20, 234.88it/s] 39%|███▉      | 11001/28000 [00:38<00:40, 417.48it/s] 43%|████▎     | 12001/28000 [00:47<01:04, 247.43it/s] 46%|████▋     | 13001/28000 [00:47<00:45, 329.70it/s] 54%|█████▎    | 15001/28000 [00:56<00:48, 268.48it/s] 57%|█████▋    | 16001/28000 [00:56<00:34, 345.16it/s] 61%|██████    | 17001/28000 [00:56<00:24, 454.08it/s] 64%|██████▍   | 18001/28000 [01:06<00:40, 246.94it/s] 68%|██████▊   | 19001/28000 [01:06<00:26, 333.77it/s] 75%|███████▌  | 21001/28000 [01:15<00:26, 268.12it/s] 79%|███████▊  | 22001/28000 [01:15<00:17, 347.83it/s] 82%|████████▏ | 23001/28000 [01:16<00:10, 457.76it/s] 86%|████████▌ | 24001/28000 [01:25<00:16, 239.70it/s] 93%|█████████▎| 26001/28000 [01:25<00:04, 415.59it/s] 96%|█████████▋| 27001/28000 [01:34<00:03, 263.26it/s]100%|██████████| 28000/28000 [01:34<00:00, 297.42it/s]
  0%|          | 0/28000 [00:00<?, ?it/s]  0%|          | 1/28000 [00:09<74:03:11,  9.52s/it]  7%|▋         | 2001/28000 [00:09<01:29, 292.06it/s] 11%|█         | 3001/28000 [00:19<02:26, 170.34it/s] 14%|█▍        | 4001/28000 [00:19<01:29, 268.59it/s] 21%|██▏       | 6001/28000 [00:28<01:32, 238.94it/s] 29%|██▊       | 8001/28000 [00:28<00:49, 405.02it/s] 32%|███▏      | 9001/28000 [00:37<01:17, 245.59it/s] 43%|████▎     | 12001/28000 [00:47<00:57, 279.24it/s] 50%|█████     | 14001/28000 [00:47<00:34, 406.92it/s] 54%|█████▎    | 15001/28000 [00:56<00:48, 268.01it/s] 57%|█████▋    | 16001/28000 [00:56<00:35, 336.89it/s] 64%|██████▍   | 18001/28000 [01:06<00:36, 274.88it/s] 68%|██████▊   | 19001/28000 [01:06<00:26, 343.98it/s] 75%|███████▌  | 21001/28000 [01:15<00:24, 281.67it/s] 79%|███████▊  | 22001/28000 [01:15<00:16, 354.53it/s] 82%|████████▏ | 23001/28000 [01:15<00:10, 456.25it/s] 86%|████████▌ | 24001/28000 [01:25<00:15, 253.31it/s] 89%|████████▉ | 25001/28000 [01:25<00:08, 340.06it/s] 96%|█████████▋| 27001/28000 [01:33<00:03, 289.70it/s]100%|██████████| 28000/28000 [01:33<00:00, 299.20it/s]
  0%|          | 0/35050 [00:00<?, ?it/s]  0%|          | 1/35050 [00:11<114:02:25, 11.71s/it]  3%|▎         | 1001/35050 [00:11<04:43, 120.14it/s]  6%|▌         | 2001/35050 [00:12<01:56, 283.78it/s]  9%|▊         | 3001/35050 [00:23<03:46, 141.53it/s] 11%|█▏        | 4001/35050 [00:23<02:15, 228.72it/s] 14%|█▍        | 5001/35050 [00:23<01:26, 348.58it/s] 17%|█▋        | 6001/35050 [00:35<02:52, 168.10it/s] 20%|█▉        | 7001/35050 [00:35<01:54, 244.71it/s] 23%|██▎       | 8001/35050 [00:36<01:16, 352.57it/s] 26%|██▌       | 9001/35050 [00:47<02:22, 182.28it/s] 29%|██▊       | 10001/35050 [00:47<01:37, 257.53it/s] 31%|███▏      | 11001/35050 [00:47<01:06, 363.78it/s] 34%|███▍      | 12001/35050 [00:59<02:02, 187.51it/s] 37%|███▋      | 13001/35050 [00:59<01:24, 262.29it/s] 40%|███▉      | 14001/35050 [00:59<00:57, 367.97it/s] 40%|███▉      | 14001/35050 [01:09<00:57, 367.97it/s] 43%|████▎     | 15001/35050 [01:10<01:46, 188.94it/s] 46%|████▌     | 16001/35050 [01:11<01:12, 262.67it/s] 49%|████▊     | 17001/35050 [01:11<00:48, 369.20it/s] 51%|█████▏    | 18001/35050 [01:22<01:31, 186.48it/s] 54%|█████▍    | 19001/35050 [01:23<01:02, 258.77it/s] 57%|█████▋    | 20001/35050 [01:23<00:41, 364.45it/s] 60%|█████▉    | 21001/35050 [01:34<01:14, 188.46it/s] 63%|██████▎   | 22001/35050 [01:35<00:50, 260.00it/s] 66%|██████▌   | 23001/35050 [01:35<00:32, 365.40it/s] 68%|██████▊   | 24001/35050 [01:46<00:59, 185.17it/s] 71%|███████▏  | 25001/35050 [01:47<00:39, 254.96it/s] 74%|███████▍  | 26001/35050 [01:47<00:25, 359.77it/s] 77%|███████▋  | 27001/35050 [01:58<00:42, 189.78it/s] 80%|███████▉  | 28001/35050 [01:59<00:26, 261.85it/s] 83%|████████▎ | 29001/35050 [01:59<00:16, 369.76it/s] 83%|████████▎ | 29001/35050 [02:09<00:16, 369.76it/s] 86%|████████▌ | 30001/35050 [02:10<00:26, 189.93it/s] 88%|████████▊ | 31001/35050 [02:10<00:15, 261.75it/s] 91%|█████████▏| 32001/35050 [02:10<00:08, 366.04it/s] 94%|█████████▍| 33001/35050 [02:21<00:10, 192.84it/s] 97%|█████████▋| 34001/35050 [02:22<00:03, 268.96it/s]100%|██████████| 35050/35050 [02:22<00:00, 246.49it/s]
  0%|          | 0/35050 [00:00<?, ?it/s]  0%|          | 1/35050 [00:11<116:44:42, 11.99s/it]  3%|▎         | 1001/35050 [00:12<04:51, 116.69it/s]  6%|▌         | 2001/35050 [00:12<01:59, 277.39it/s]  6%|▌         | 2001/35050 [00:23<01:59, 277.39it/s]  9%|▊         | 3001/35050 [00:23<03:52, 138.05it/s] 11%|█▏        | 4001/35050 [00:24<02:18, 223.53it/s] 14%|█▍        | 5001/35050 [00:24<01:27, 343.61it/s] 17%|█▋        | 6001/35050 [00:35<02:49, 170.95it/s] 20%|█▉        | 7001/35050 [00:36<01:52, 249.34it/s] 23%|██▎       | 8001/35050 [00:36<01:14, 361.37it/s] 26%|██▌       | 9001/35050 [00:47<02:24, 179.76it/s] 29%|██▊       | 10001/35050 [00:48<01:38, 255.29it/s] 31%|███▏      | 11001/35050 [00:48<01:06, 362.40it/s] 34%|███▍      | 12001/35050 [01:00<02:09, 177.52it/s] 37%|███▋      | 13001/35050 [01:00<01:28, 248.82it/s] 40%|███▉      | 14001/35050 [01:00<00:59, 352.58it/s] 43%|████▎     | 15001/35050 [01:12<01:53, 177.01it/s] 46%|████▌     | 16001/35050 [01:13<01:17, 247.32it/s] 49%|████▊     | 17001/35050 [01:13<00:51, 349.35it/s] 51%|█████▏    | 18001/35050 [01:24<01:33, 182.82it/s] 54%|█████▍    | 19001/35050 [01:25<01:02, 255.53it/s] 57%|█████▋    | 20001/35050 [01:25<00:41, 360.46it/s] 60%|█████▉    | 21001/35050 [01:36<01:16, 184.18it/s] 63%|██████▎   | 22001/35050 [01:37<00:50, 257.04it/s] 68%|██████▊   | 24001/35050 [01:48<00:52, 209.41it/s] 71%|███████▏  | 25001/35050 [01:49<00:37, 270.28it/s] 77%|███████▋  | 27001/35050 [02:00<00:36, 219.91it/s] 80%|███████▉  | 28001/35050 [02:01<00:25, 280.46it/s] 83%|████████▎ | 29001/35050 [02:01<00:16, 367.48it/s] 86%|████████▌ | 30001/35050 [02:12<00:25, 199.17it/s] 88%|████████▊ | 31001/35050 [02:12<00:14, 269.94it/s] 91%|█████████▏| 32001/35050 [02:13<00:08, 366.23it/s] 91%|█████████▏| 32001/35050 [02:23<00:08, 366.23it/s] 94%|█████████▍| 33001/35050 [02:23<00:10, 198.80it/s]100%|██████████| 35050/35050 [02:24<00:00, 243.34it/s]
  0%|          | 0/35050 [00:00<?, ?it/s]  0%|          | 1/35050 [00:12<118:59:59, 12.22s/it]  3%|▎         | 1001/35050 [00:12<04:58, 114.00it/s]  6%|▌         | 2001/35050 [00:12<02:02, 269.06it/s]  9%|▊         | 3001/35050 [00:24<03:54, 136.74it/s] 11%|█▏        | 4001/35050 [00:24<02:19, 222.71it/s] 14%|█▍        | 5001/35050 [00:24<01:27, 342.60it/s] 14%|█▍        | 5001/35050 [00:35<01:27, 342.60it/s] 17%|█▋        | 6001/35050 [00:36<02:50, 169.89it/s] 20%|█▉        | 7001/35050 [00:36<01:52, 249.12it/s] 23%|██▎       | 8001/35050 [00:36<01:15, 357.93it/s] 26%|██▌       | 9001/35050 [00:48<02:24, 180.12it/s] 29%|██▊       | 10001/35050 [00:48<01:37, 256.36it/s] 31%|███▏      | 11001/35050 [00:48<01:06, 361.73it/s] 34%|███▍      | 12001/35050 [01:00<02:05, 184.00it/s] 37%|███▋      | 13001/35050 [01:00<01:24, 259.89it/s] 40%|███▉      | 14001/35050 [01:00<00:57, 366.83it/s] 43%|████▎     | 15001/35050 [01:11<01:48, 185.28it/s] 46%|████▌     | 16001/35050 [01:12<01:13, 259.56it/s] 49%|████▊     | 17001/35050 [01:12<00:49, 366.61it/s] 51%|█████▏    | 18001/35050 [01:24<01:34, 179.68it/s] 54%|█████▍    | 19001/35050 [01:24<01:03, 252.50it/s] 57%|█████▋    | 20001/35050 [01:24<00:42, 356.75it/s] 57%|█████▋    | 20001/35050 [01:35<00:42, 356.75it/s] 60%|█████▉    | 21001/35050 [01:36<01:15, 184.92it/s] 63%|██████▎   | 22001/35050 [01:36<00:50, 259.07it/s] 68%|██████▊   | 24001/35050 [01:48<00:52, 209.63it/s] 71%|███████▏  | 25001/35050 [01:48<00:36, 274.11it/s] 74%|███████▍  | 26001/35050 [01:48<00:24, 367.22it/s] 77%|███████▋  | 27001/35050 [02:00<00:41, 195.07it/s] 80%|███████▉  | 28001/35050 [02:00<00:26, 266.14it/s] 86%|████████▌ | 30001/35050 [02:11<00:23, 215.27it/s] 88%|████████▊ | 31001/35050 [02:12<00:14, 279.41it/s] 91%|█████████▏| 32001/35050 [02:12<00:08, 371.92it/s] 94%|█████████▍| 33001/35050 [02:23<00:10, 204.23it/s] 97%|█████████▋| 34001/35050 [02:23<00:03, 279.45it/s]100%|██████████| 35050/35050 [02:23<00:00, 244.62it/s]
  0%|          | 0/35050 [00:00<?, ?it/s]  0%|          | 1/35050 [00:11<115:37:48, 11.88s/it]  3%|▎         | 1001/35050 [00:12<04:48, 117.83it/s]  6%|▌         | 2001/35050 [00:12<01:58, 278.14it/s]  9%|▊         | 3001/35050 [00:23<03:50, 139.28it/s] 11%|█▏        | 4001/35050 [00:23<02:16, 227.21it/s] 14%|█▍        | 5001/35050 [00:24<01:26, 346.56it/s] 17%|█▋        | 6001/35050 [00:35<02:49, 171.10it/s] 20%|█▉        | 7001/35050 [00:35<01:51, 250.49it/s] 23%|██▎       | 8001/35050 [00:36<01:15, 356.41it/s] 26%|██▌       | 9001/35050 [00:47<02:23, 181.52it/s] 29%|██▊       | 10001/35050 [00:47<01:37, 258.16it/s] 31%|███▏      | 11001/35050 [00:48<01:07, 357.70it/s] 31%|███▏      | 11001/35050 [00:58<01:07, 357.70it/s] 34%|███▍      | 12001/35050 [00:59<02:05, 184.12it/s] 37%|███▋      | 13001/35050 [00:59<01:25, 259.07it/s] 40%|███▉      | 14001/35050 [01:00<00:58, 360.73it/s] 43%|████▎     | 15001/35050 [01:11<01:47, 186.82it/s] 46%|████▌     | 16001/35050 [01:11<01:13, 260.91it/s] 49%|████▊     | 17001/35050 [01:11<00:49, 361.99it/s] 51%|█████▏    | 18001/35050 [01:23<01:31, 186.96it/s] 54%|█████▍    | 19001/35050 [01:23<01:01, 261.40it/s] 57%|█████▋    | 20001/35050 [01:23<00:41, 360.26it/s] 60%|█████▉    | 21001/35050 [01:35<01:15, 186.69it/s] 63%|██████▎   | 22001/35050 [01:35<00:50, 260.00it/s] 66%|██████▌   | 23001/35050 [01:35<00:33, 359.30it/s] 68%|██████▊   | 24001/35050 [01:47<01:00, 182.11it/s] 71%|███████▏  | 25001/35050 [01:47<00:39, 254.97it/s] 74%|███████▍  | 26001/35050 [01:48<00:25, 353.39it/s] 74%|███████▍  | 26001/35050 [01:58<00:25, 353.39it/s] 77%|███████▋  | 27001/35050 [01:59<00:43, 186.29it/s] 80%|███████▉  | 28001/35050 [01:59<00:26, 261.08it/s] 83%|████████▎ | 29001/35050 [02:00<00:16, 360.67it/s] 86%|████████▌ | 30001/35050 [02:11<00:26, 188.04it/s] 88%|████████▊ | 31001/35050 [02:11<00:15, 263.35it/s] 91%|█████████▏| 32001/35050 [02:11<00:08, 362.26it/s] 94%|█████████▍| 33001/35050 [02:22<00:10, 195.17it/s] 97%|█████████▋| 34001/35050 [02:22<00:03, 275.12it/s]100%|██████████| 35050/35050 [02:22<00:00, 245.55it/s]
  0%|          | 0/35050 [00:00<?, ?it/s]  0%|          | 1/35050 [00:11<115:41:01, 11.88s/it]  3%|▎         | 1001/35050 [00:12<04:48, 117.83it/s]  6%|▌         | 2001/35050 [00:12<01:57, 280.23it/s]  6%|▌         | 2001/35050 [00:23<01:57, 280.23it/s]  9%|▊         | 3001/35050 [00:23<03:51, 138.73it/s] 11%|█▏        | 4001/35050 [00:23<02:18, 224.56it/s] 14%|█▍        | 5001/35050 [00:24<01:26, 346.57it/s] 17%|█▋        | 6001/35050 [00:35<02:49, 171.05it/s] 20%|█▉        | 7001/35050 [00:35<01:52, 248.94it/s] 23%|██▎       | 8001/35050 [00:36<01:15, 360.64it/s] 26%|██▌       | 9001/35050 [00:47<02:25, 179.12it/s] 29%|██▊       | 10001/35050 [00:48<01:38, 253.11it/s] 31%|███▏      | 11001/35050 [00:48<01:06, 359.96it/s] 34%|███▍      | 12001/35050 [00:59<02:05, 182.97it/s] 37%|███▋      | 13001/35050 [01:00<01:26, 253.95it/s] 43%|████▎     | 15001/35050 [01:11<01:36, 207.82it/s] 46%|████▌     | 16001/35050 [01:12<01:10, 270.67it/s] 49%|████▊     | 17001/35050 [01:12<00:49, 362.80it/s] 49%|████▊     | 17001/35050 [01:23<00:49, 362.80it/s] 51%|█████▏    | 18001/35050 [01:23<01:28, 192.74it/s] 54%|█████▍    | 19001/35050 [01:24<01:01, 262.44it/s] 57%|█████▋    | 20001/35050 [01:24<00:41, 361.64it/s] 60%|█████▉    | 21001/35050 [01:35<01:14, 188.46it/s] 63%|██████▎   | 22001/35050 [01:36<00:49, 261.76it/s] 66%|██████▌   | 23001/35050 [01:36<00:33, 363.41it/s] 68%|██████▊   | 24001/35050 [01:47<00:59, 186.76it/s] 71%|███████▏  | 25001/35050 [01:48<00:38, 261.82it/s] 74%|███████▍  | 26001/35050 [01:48<00:24, 364.35it/s] 77%|███████▋  | 27001/35050 [01:59<00:43, 186.27it/s] 80%|███████▉  | 28001/35050 [02:00<00:26, 262.00it/s] 83%|████████▎ | 29001/35050 [02:00<00:16, 363.51it/s] 86%|████████▌ | 30001/35050 [02:12<00:27, 181.41it/s] 88%|████████▊ | 31001/35050 [02:12<00:15, 256.36it/s] 91%|█████████▏| 32001/35050 [02:12<00:08, 354.65it/s] 91%|█████████▏| 32001/35050 [02:23<00:08, 354.65it/s] 94%|█████████▍| 33001/35050 [02:23<00:10, 191.51it/s]100%|██████████| 35050/35050 [02:23<00:00, 244.11it/s]
39
Loss: 0.056685019905368485
Iteration: 40
Loss: 0.0523269090992518
Iteration: 41
Loss: 0.048357971442433506
Iteration: 42
Loss: 0.04490687201420466
Iteration: 43
Loss: 0.04181382341835743
Iteration: 44
Loss: 0.03886772439074822
Iteration: 45
Loss: 0.035972766912518404
Iteration: 46
Loss: 0.03341345188136284
Iteration: 47
Loss: 0.03124510231786049
Iteration: 48
Loss: 0.029233332651738938
Iteration: 49
Loss: 0.02732414933733451
Iteration: 50
Loss: 0.025579687781058826
Iteration: 51
Loss: 0.023890758983981915
Iteration: 52
Loss: 0.02241714186488818
Iteration: 53
Loss: 0.02108784745901059
Iteration: 54
Loss: 0.019923523617669556
Iteration: 55
Loss: 0.018716608054744892
Iteration: 56
Loss: 0.01770895702812152
Iteration: 57
Loss: 0.016719199024522916
Iteration: 58
Loss: 0.015775818294153
Iteration: 59
Loss: 0.015134658389844192
Iteration: 60
Loss: 0.01422727131881775
Iteration: 61
Loss: 0.013483164354394644
Iteration: 62
Loss: 0.012827480259614114
Iteration: 63
Loss: 0.012225831202111946
Iteration: 64
Loss: 0.011675861162634997
Iteration: 65
Loss: 0.011075892163297305
Iteration: 66
Loss: 0.010612567385228781
Iteration: 67
Loss: 0.010129687674821187
Iteration: 68
Loss: 0.009757962412176987
Iteration: 69
Loss: 0.009355203523181187
Iteration: 70
Loss: 0.008980992262084514
Iteration: 71
Loss: 0.008585111847004065
Iteration: 72
Loss: 0.008266983094075933
Iteration: 73
Loss: 0.007973382267384576
Iteration: 74
Loss: 0.007713697211912427
Iteration: 75
Loss: 0.007447139001809633
Iteration: 76
Loss: 0.0072192195026824875
Iteration: 77
Loss: 0.00698612707380492
Iteration: 78
Loss: 0.006738075485023169
Iteration: 79
Loss: 0.006531305097712156
Iteration: 80
Loss: 0.006325342871535283
Iteration: 81
Loss: 0.006126455783557434
Iteration: 82
Loss: 0.005958653108861584
Iteration: 83
Loss: 0.005800872527731535
Iteration: 84
Loss: 0.005665793680609801
Iteration: 85
Loss: 0.0054852784109803345
Iteration: 86
Loss: 0.005327700446240413
Iteration: 87
Loss: 0.005202845324021883
Iteration: 88
Loss: 0.0050958564731841665
Iteration: 89
Loss: 0.00497495056464313
Iteration: 90
Loss: 0.004870006295207601
Iteration: 91
Loss: 0.004755498214553182
Iteration: 92
Loss: 0.004669620195785776
Iteration: 93
Loss: 0.004575871068458908
Iteration: 94
Loss: 0.004486042570370512
Iteration: 95
Loss: 0.0044268034995557405
Iteration: 96
Loss: 0.004322268713552218
Iteration: 97
Loss: 0.004256587015846983
Iteration: 98
Loss: 0.004173661203075869
Iteration: 99
Loss: 0.004108129967696583
Iteration: 100
Loss: 0.00403399954442508
Iteration: 101
Loss: 0.0040065846943224855
Iteration: 102
Loss: 0.00392611065091422
Iteration: 103
Loss: 0.0038824534992902325
Iteration: 104
Loss: 0.003822625077401216
Iteration: 105
Loss: 0.003771987815315907
Iteration: 106
Loss: 0.003737742293220109
Iteration: 107
Loss: 0.003683595897223896
Iteration: 108
Loss: 0.003628348173478093
Iteration: 109
Loss: 0.003616330318320065
Iteration: 110
Loss: 0.003562314364199455
Iteration: 111
Loss: 0.003528054517049056
Iteration: 112
Loss: 0.0035123004195972895
Iteration: 113
Loss: 0.0034612973746007835
Iteration: 114
Loss: 0.003443339553016883
Iteration: 115
Loss: 0.0034050425777259544
Iteration: 116
Loss: 0.0033907318780294214
Iteration: 117
Loss: 0.0033448413939764486
Iteration: 118
Loss: 0.003333812337726928
Iteration: 119
Loss: 0.0033135451525688553
Iteration: 120
Loss: 0.0032934532082902314
Iteration: 121
Loss: 0.003257616309640117
Iteration: 122
Loss: 0.0032417563727507605
Iteration: 123
Loss: 0.003219722791049534
Iteration: 124
Loss: 0.0031955987006091536
Iteration: 125
Loss: 0.003170517762788595
Iteration: 126
Loss: 0.0031545121694365754
Iteration: 127
Loss: 0.003133257788319427
Iteration: 128
Loss: 0.0031177094116663705
Iteration: 129
Loss: 0.003102995383624847
Iteration: 130
Loss: 0.0030851577295181462
Iteration: 131
Loss: 0.0030653953600006225
Iteration: 132
Loss: 0.0030463891766535547
Iteration: 133
Loss: 0.0030359585070982575
Iteration: 134
Loss: 0.00301239547582391
Iteration: 135
Loss: 0.0029956219831290534
Iteration: 136
Loss: 0.0029754719416348217
Iteration: 137
Loss: 0.002957508753006084
Iteration: 138
Loss: 0.002942394438342979
Iteration: 139
Loss: 0.0029123067700614533
Iteration: 140
Loss: 0.002902035037270532
Iteration: 141
Loss: 0.002890942976451837
Iteration: 142
Loss: 0.002872714480289664
Iteration: 143
Loss: 0.0028574168950749133
Iteration: 144
Loss: 0.0028286409319545594
Iteration: 145
Loss: 0.0028131869143973556
Iteration: 146
Loss: 0.0027891059065810763
Iteration: 147
Loss: 0.00277983388887384
Iteration: 148
Loss: 0.002753394187237017
Iteration: 149
Loss: 0.002746147617029074
Iteration: 150
Loss: 0.002724245704041842
Iteration: 151
Loss: 0.002706432918038888
Iteration: 152
Loss: 0.002685303390861895
Iteration: 153
Loss: 0.002685765422379168
Iteration: 154
Loss: 0.0026511916777310083
Iteration: 155
Loss: 0.0026337236034660004
Iteration: 156
Loss: 0.0026266973208970367
Iteration: 157
Loss: 0.0026035275823699357
Iteration: 158
Loss: 0.0025794663024731935
Iteration: 159
Loss: 0.0025652213117633113
Iteration: 160
Loss: 0.0025647180632520947
Iteration: 161
Loss: 0.002524404350715952
Iteration: 162
Loss: 0.0025176097555324817
Iteration: 163
Loss: 0.002509122571119895
Iteration: 164
Loss: 0.0024894581934532677
Iteration: 165
Loss: 0.0024783516165394434
Iteration: 166
Loss: 0.002446808252268686
Iteration: 167
Loss: 0.00244233508904775
Iteration: 168
Loss: 0.0024204004185799607
Iteration: 169
Loss: 0.0024224401135236407
Iteration: 170
Loss: 0.0024021683941380335
Iteration: 171
Loss: 0.002386250712264043
Iteration: 172
Loss: 0.0023861288104970488
Iteration: 173
Loss: 0.002374805426463867
Iteration: 174
Loss: 0.0023469321581964884
Iteration: 175
Loss: 0.002351765491020603
Iteration: 176
Loss: 0.00232861155214218
Iteration: 177
Loss: 0.002321112396505972
Iteration: 178
Loss: 0.0023092486752340426
Iteration: 179
Loss: 0.002320621762639628
Iteration: 180
Loss: 0.002302216253995609
Iteration: 181
Loss: 0.002288269365612322
Iteration: 182
Loss: 0.0023329203518537376
Iteration: 183
Loss: 0.0022747183848435106
Iteration: 184
Loss: 0.0022793104125855444
Iteration: 185
Loss: 0.002260761299672035
Iteration: 186
Loss: 0.002252939112412815
Iteration: 187
Loss: 0.002244602942552704
Iteration: 188
Loss: 0.002232582360589638
Iteration: 189
Loss: 0.00223440775134338
Iteration: 190
Loss: 0.002222352401496699
Iteration: 191
Loss: 0.002214669348457112
Iteration: 192
Loss: 0.0022239827324087038
Iteration: 193
Loss: 0.0022048998925572215
Iteration: 194
Loss: 0.002194107602684735
Iteration: 195
Loss: 0.0021795490726780817
Iteration: 196
Loss: 0.0021843413914888143
Iteration: 197
Loss: 0.002174074442173617
Iteration: 198
Loss: 0.002174071555670637
Iteration: 199
Loss: 0.002177053361008756
Iteration: 200
Loss: 0.0021618830535608605
Iteration: 201
Loss: 0.0021700015387091883
Iteration: 202
Loss: 0.002141603659719038
Iteration: 203
Loss: 0.002151492015960125
Iteration: 204
Loss: 0.0021496727519358196
Iteration: 205
Loss: 0.00213797428072072
Iteration: 206
Loss: 0.0021449371363418414
Iteration: 207
Loss: 0.002143685314923716
Iteration: 208
Loss: 0.0021154505323069408
Iteration: 209
Loss: 0.002131349178783309
Iteration: 210
Loss: 0.0021139932960534515
Iteration: 211
Loss: 0.002155569520814774
Iteration: 212
Loss: 0.0021070990836820924
Iteration: 213
Loss: 0.002114332657164106
Iteration: 214
Loss: 0.00213762246317063
Iteration: 215
Loss: 0.002107320208615886
Iteration: 216
Loss: 0.0020994993123727348
Iteration: 217
Loss: 0.00209651564886698
Iteration: 218
Loss: 0.0020877557451454685
Iteration: 219
Loss: 0.002103370521814586
Iteration: 220
Loss: 0.002091083627862808
Iteration: 221
Loss: 0.002084369735362438
Iteration: 222
Loss: 0.0021601664857604564
Iteration: 223
Loss: 0.00208927960976815
Iteration: 224
Loss: 0.002080200662394651
Iteration: 225
Loss: 0.00207593091405355
Iteration: 226
Loss: 0.0020855015027336776
Iteration: 227
Loss: 0.0020631799662414077
Iteration: 228
Loss: 0.002073420302715535
Iteration: 229
Loss: 0.002055046173970764
Iteration: 230
Loss: 0.002073498733807355
Iteration: 231
Loss: 0.0020682139214701378
Iteration: 232
Loss: 0.0020525820362262237
Iteration: 233
Loss: 0.0020648983757919036
Iteration: 234
Loss: 0.0020702933871712633
Iteration: 235
Loss: 0.0020546578079796373
Iteration: 236
Loss: 0.0020548032286266484
Iteration: 237
Loss: 0.0020507908476970326
Iteration: 238
Loss: 0.002039068687456445
Iteration: 239
Loss: 0.0020462314669902506
Iteration: 240
Loss: 0.0020416629372331766
Iteration: 241
Loss: 0.0020388888705593464
Iteration: 242
Loss: 0.002036598775213441
Iteration: 243
Loss: 0.002037704840171127
Iteration: 244
Loss: 0.0020426824473990845
Iteration: 245
Loss: 0.002037010167558224
Iteration: 246
Loss: 0.002036920988944192
Iteration: 247
Loss: 0.002024469240449178
Iteration: 248
Loss: 0.0020260671019339217
Iteration: 249
Loss: 0.002025294534038179
Iteration: 250
Loss: 0.0020190161960318876
Iteration: 251
Loss: 0.0020272228165338626
Iteration: 252
Loss: 0.00201898571014261
Iteration: 253
Loss: 0.002017625761278069
Iteration: 254
Loss: 0.002006418597048674
Iteration: 255
Loss: 0.002008618312314726
Iteration: 256
Loss: 0.002016729021516557
Iteration: 257
Loss: 0.0020028851666631033
Iteration: 258
Loss: 0.0020059183813058413
Iteration: 259
Loss: 0.0020102975301993773
Iteration: 260
Loss: 0.002001321940229107
Iteration: 261
Loss: 0.0020001320741497553
Iteration: 262
Loss: 0.0020209190850623716
Iteration: 263
Loss: 0.00201516111757463
Iteration: 264
Loss: 0.0020095330785410716
Iteration: 265
Loss: 0.0020153897259241114
Iteration: 266
Loss: 0.0019988983030765294
Iteration: 267
Loss: 0.001993467768606467
Iteration: 268
Loss: 0.001987681789801289
Iteration: 269
Loss: 0.001994985699032744
Iteration: 270
Loss: 0.0019990760841980004
Iteration: 271
Loss: 0.0020078699602984274
Iteration: 272
Loss: 0.0019995958532373873
Iteration: 273
Loss: 0.001990195096601756
Iteration: 274
Loss: 0.00198399546281554
Iteration: 275
Loss: 0.0019835489936387883
Iteration: 276
Loss: 0.00198456596953269
Iteration: 277
Loss: 0.0019814105844722153
Iteration: 278
Loss: 0.0019746227577949562
Iteration: 279
Loss: 0.00198142720052065
Iteration: 280
Loss: 0.0019783376811597594
Iteration: 281
Loss: 0.0019935617754713465
Iteration: 282
Loss: 0.0019840877622556994
Iteration: 283
Loss: 0.001977681850966735
Iteration: 284
Loss: 0.00197810189685044
Iteration: 285
Loss: 0.001970708471699021
Iteration: 286
Loss: 0.0019664050882849363
Iteration: 287
Loss: 0.0019744526048620734
Iteration: 288
Loss: 0.001971647079186275
Iteration: 289
Loss: 0.001966771627895725
Iteration: 290
Loss: 0.001964577184154246
Iteration: 291
Loss: 0.001967954458310627
Iteration: 292
Loss: 0.0019762374696512828
Iteration: 293
Loss: 0.0019679511345039383
Iteration: 294
Loss: 0.001955331738990469
Iteration: 295
Loss: 0.001978072597501943
Iteration: 296
Loss: 0.0019658347576832734
Iteration: 297
Loss: 0.0019592458609109507
Iteration: 298
Loss: 0.00196765925293454
Iteration: 299
Loss: 0.001963732746214821
Iteration: 300
Loss: 0.001955955369899479
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.31846019247594043
accuracy: 0.9722281639928698
confusion: 182 400 379 27089
precision: 0.3127147766323024
recall: 0.3244206773618538
Processing fold: ../../../kg_constructor/output/ecoli/folds/fold_3
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 26185
Starting training...
Iteration: 1
Loss: 0.703171004087497
Iteration: 2
Loss: 0.6961553417719327
Iteration: 3
Loss: 0.6767615538377029
Iteration: 4
Loss: 0.6372093458970388
Iteration: 5
Loss: 0.5762226940729679
Iteration: 6
Loss: 0.5096876319402304
Iteration: 7
Loss: 0.449920464020509
Iteration: 8
Loss: 0.40406835270233643
Iteration: 9
Loss: 0.368501997146851
Iteration: 10
Loss: 0.3425507312401747
Iteration: 11
Loss: 0.3217557764206177
Iteration: 12
Loss: 0.3050546264037108
Iteration: 13
Loss: 0.29037441313266754
Iteration: 14
Loss: 0.277038567341291
Iteration: 15
Loss: 0.26570394673408604
Iteration: 16
Loss: 0.2539863928388327
Iteration: 17
Loss: 0.24312233332640085
Iteration: 18
Loss: 0.23240635333917078
Iteration: 19
Loss: 0.2215745447155757
Iteration: 20
Loss: 0.21107909503655556
Iteration: 21
Loss: 0.19982935163455132
Iteration: 22
Loss: 0.18929215234059554
Iteration: 23
Loss: 0.1790395044745543
Iteration: 24
Loss: 0.16862345372255033
Iteration: 25
Loss: 0.15818732881393188
Iteration: 26
Loss: 0.1480974744145687
Iteration: 27
Loss: 0.13855334600576988
Iteration: 28
Loss: 0.12901310078226602
Iteration: 29
Loss: 0.12023719543447861
Iteration: 30
Loss: 0.11175454484346585
Iteration: 31
Loss: 0.10315611165685531
Iteration: 32
Loss: 0.0956268422305584
Iteration: 33
Loss: 0.08879926638343395
Iteration: 34
Loss: 0.08180029336840679
Iteration: 35
Loss: 0.07570181586421453
Iteration: 36
Loss: 0.0698791109980681
Iteration: 37
Loss: 0.06450599006926402
Iteration: 38
Loss: 0.0591687569155907
Iteration: 39
Loss: 0.05499201392134031
Iteration: 40
Loss: 0.05102449808365259
Iteration: 41
Loss: 0.0474162253145224
Iteration: 42
Loss: 0.04374466697948101
Iteration: 43
Loss: 0.04058876848564698
Iteration: 44
Loss: 0.037730999290943146
Iteration: 45
Loss: 0.03502203419040411
Iteration: 46
Loss: 0.03245228134955351
Iteration: 47
Loss: 0.030346049616734188
Iteration: 48
Loss: 0.02841719588599144
Iteration: 49
Loss: 0.026650910074703205
Iteration: 50
Loss: 0.024803911598447043
Iteration: 51
Loss: 0.023252086116908453
Iteration: 52
Loss: 0.02184308138795388
Iteration: 53
Loss: 0.020653420796570104
Iteration: 54
Loss: 0.019367890695157725
Iteration: 55
Loss: 0.018253387692265022
Iteration: 56
Loss: 0.017236285365353793
Iteration: 57
Loss: 0.016232495984205834
Iteration: 58
Loss: 0.01537913248802607
Iteration: 59
Loss: 0.014524895076950392
Iteration: 60
Loss: 0.013851607194504676
Iteration: 61
Loss: 0.013169060461223125
Iteration: 62
Loss: 0.012547126362243524
Iteration: 63
Loss: 0.011870292373574697
Iteration: 64
Loss: 0.011385903729555698
Iteration: 65
Loss: 0.010936985914714826
Iteration: 66
Loss: 0.010510706378576847
Iteration: 67
Loss: 0.010005359108058305
Iteration: 68
Loss: 0.009486949333968835
Iteration: 69
Loss: 0.009137576386236992
Iteration: 70
Loss: 0.00881405796807928
Iteration: 71
Loss: 0.00844274366943118
Iteration: 72
Loss: 0.008132422713037485
Iteration: 73
Loss: 0.007829498410081634
Iteration: 74
Loss: 0.007583207050815989
Iteration: 75
Loss: 0.007342006258953076
Iteration: 76
Loss: 0.007014068010716866
Iteration: 77
Loss: 0.0068423529633153705
Iteration: 78
Loss: 0.006601033690504921
Iteration: 79
Loss: 0.006410317244724586
Iteration: 80
Loss: 0.00624723710382405
Iteration: 81
Loss: 0.006049143699690318
Iteration: 82
Loss: 0.005872865117943058
Iteration: 83
Loss: 0.005695396138784977
Iteration: 84
Loss: 0.005560722984134769
Iteration: 85
Loss: 0.005428462808665175
Iteration: 86
Loss: 0.005253666026804309
Iteration: 87
Loss: 0.005148819158188043
Iteration: 88
Loss: 0.005041115063553055
Iteration: 89
Loss: 0.004914864479826811
Iteration: 90
Loss: 0.0047841670099072735
Iteration: 91
Loss: 0.004694336147692341
Iteration: 92
Loss: 0.00463379863410806
Iteration: 93
Loss: 0.004527991297296607
Iteration: 94
Loss: 0.004418467410290852
Iteration: 95
Loss: 0.004341487223521257
Iteration: 96
Loss: 0.004269807527844722
Iteration: 97
Loss: 0.004178760152381773
Iteration: 98
Loss: 0.0041540701372119095
Iteration: 99
Loss: 0.004078656581278222
Iteration: 100
Loss: 0.004014527132639136
Iteration: 101
Loss: 0.003952018056924527
Iteration: 102
Loss: 0.0039033146861654064
Iteration: 103
Loss: 0.003850595703205237
Iteration: 104
Loss: 0.003802867307781409
Iteration: 105
Loss: 0.003743517087199367
Iteration: 106
Loss: 0.0037122124161284705
Iteration: 107
Loss: 0.0036650687300910554
Iteration: 108
Loss: 0.0036208844439198193
Iteration: 109
Loss: 0.003585107702141007
Iteration: 110
Loss: 0.0035589452570256516
Iteration: 111
Loss: 0.0035260294576008352
Iteration: 112
Loss: 0.0034908501950737377
Iteration: 113
Loss: 0.003448628462277926
Iteration: 114
Loss: 0.0034178898442918672
Iteration: 115
Loss: 0.0033898873385997154
Iteration: 116
Loss: 0.0033732728960995492
Iteration: 117
Loss: 0.0033451001792668533
Iteration: 118
Loss: 0.0033161547064828947
Iteration: 119
Loss: 0.00329986594330806
Iteration: 120
Loss: 0.0032860047739142408
Iteration: 121
Loss: 0.003237512464133593
Iteration: 122
Loss: 0.0032182151529317102
Iteration: 123
Loss: 0.0032104489226371814
Iteration: 124
Loss: 0.0031807578544920455
Iteration: 125
Loss: 0.0031740526006055567
Iteration: 126
Loss: 0.0031491057941308007
Iteration: 127
Loss: 0.003124785579693241
Iteration: 128
Loss: 0.003107045660726726
Iteration: 129
Loss: 0.0031136233204354844
Iteration: 130
Loss: 0.0030682455294598373
Iteration: 131
Loss: 0.003059507807334646
Iteration: 132
Loss: 0.0030356386634640587
Iteration: 133
Loss: 0.0030277603192445943
Iteration: 134
Loss: 0.0030038918499858715
Iteration: 135
Loss: 0.0029780816298742327
Iteration: 136
Loss: 0.0029641188328894666
Iteration: 137
Loss: 0.002942014700518205
Iteration: 138
Loss: 0.002947709391801021
Iteration: 139
Loss: 0.0029110275597001114
Iteration: 140
Loss: 0.002898009231266303
Iteration: 141
Loss: 0.0028769200345358024
Iteration: 142
Loss: 0.002866377603286543
Iteration: 143
Loss: 0.0028436727290495466
Iteration: 144
Loss: 0.0028312289400790366
Iteration: 145
Loss: 0.0028023833337311563
Iteration: 146
Loss: 0.0027920259103083457
Iteration: 147
Loss: 0.002757852500041899
Iteration: 148
Loss: 0.0027479559857732593
Iteration: 149
Loss: 0.002755219934699245
Iteration: 150
Loss: 0.002711973669461142
Iteration: 151
Loss: 0.0026967039522834313
Iteration: 152
Loss: 0.002678062316054144
Iteration: 153
Loss: 0.0026555258977728393
Iteration: 154
Loss: 0.0026422388260610974
Iteration: 155
Loss: 0.0026236214937689975
Iteration: 156
Loss: 0.0026066216777484766
Iteration: 157
Loss: 0.0025945333166955374
Iteration: 158
Loss: 0.0025781124788455856
Iteration: 159
Loss: 0.0025671785906291543
Iteration: 160
Loss: 0.0025341795184291326
Iteration: 161
Loss: 0.002540434904706975
Iteration: 162
Loss: 0.0025142967187536834
Iteration: 163
Loss: 0.00250647305093037
Iteration: 164
Loss: 0.002470613755763341
Iteration: 165
Loss: 0.0024682442980030407
Iteration: 166
Loss: 0.002448609122672142
Iteration: 167
Loss: 0.0024361334956036164
Iteration: 168
Loss: 0.002414968043852311
Iteration: 169
Loss: 0.0024027731478548585
Iteration: 170
Loss: 0.002403831975056957
Iteration: 171
Loss: 0.002399338895860964
Iteration: 172
Loss: 0.002375757656036279
Iteration: 173
Loss: 0.0023782365621091463
Iteration: 174
Loss: 0.0023473875764279794
Iteration: 175
Loss: 0.0023486050687587033
Iteration: 176
Loss: 0.0023319624388256134
Iteration: 177
Loss: 0.002334441368778547
Iteration: 178
Loss: 0.002313311772946364
Iteration: 179
Loss: 0.002301341860005871
Iteration: 180
Loss: 0.0022915778174184454
Iteration: 181
Loss: 0.002283735499263574
Iteration: 182
Loss: 0.0022760915737121534
Iteration: 183
Loss: 0.00226364511763677
Iteration: 184
Loss: 0.0022753137104117717
Iteration: 185
Loss: 0.0022504467155951527
Iteration: 186
Loss: 0.002241125351223999
Iteration: 187
Loss: 0.002234035107390716
Iteration: 188
Loss: 0.002224921726454527
Iteration: 189
Loss: 0.0022143133324929155
Iteration: 190
Loss: 0.002208057907409966
Iteration: 191
Loss: 0.002210803252334396
Iteration: 192
Loss: 0.0022038684393732976
Iteration: 193
Loss: 0.0022083620693033133
Iteration: 194
Loss: 0.0021979273281967603
Iteration: 195
Loss: 0.002195664421798518
Iteration: 196
Loss: 0.0021813446402143785
Iteration: 197
Loss: 0.002185487354066796
Iteration: 198
Loss: 0.0021727689055319973
Iteration: 199
Loss: 0.00219262298495055
Iteration: 200
Loss: 0.0021739399287467585
Iteration: 201
Loss: 0.0021661773013571897
Iteration: 202
Loss: 0.0021656440535130408
Iteration: 203
Loss: 0.0021402035013008383
Iteration: 204
Loss: 0.00214318496784052
Iteration: 205
Loss: 0.0021400500763541996
Iteration: 206
Loss: 0.002146100676140915
Iteration: 207
Loss: 0.002132573635138285
Iteration: 208
Loss: 0.002121655248069706
Iteration: 209
Loss: 0.0021120308456608118
Iteration: 210
Loss: 0.0021244110032104147
Iteration: 211
Loss: 0.0021080526460606893
Iteration: 212
Loss: 0.0021191349697824665
Iteration: 213
Loss: 0.002107005502180889
Iteration: 214
Loss: 0.0021676870006232117
Iteration: 215
Loss: 0.0021178910049060597
Iteration: 216
Loss: 0.002093118464705558
Iteration: 217
Loss: 0.0021015194495423483
Iteration: 218
Loss: 0.0021023908988214456
Iteration: 219
Loss: 0.0021046144269908276
Iteration: 220
Loss: 0.0020917935643154075
Iteration: 221
Loss: 0.0020879348904157104
Iteration: 222
Loss: 0.002081912416869249
Iteration: 223
Loss: 0.0020818055267087542
Iteration: 224
Loss: 0.0020904496628552293
Iteration: 225
Loss: 0.00206950899416533
Iteration: 226
Loss: 0.0020703478381992914
Iteration: 227
Loss: 0.0020659551746808947
Iteration: 228
Loss: 0.002074052994617094
Iteration: 229
Loss: 0.00205058331350581
Iteration: 230
Loss: 0.002061680741369342
Iteration: 231
Loss: 0.002055940915269252
Iteration: 232
Loss: 0.002059263863767951
Iteration: 233
Loss: 0.0020585011762495223
Iteration: 234
Loss: 0.0020576150228197756
Iteration: 235
Loss: 0.0020768230076497183
Iteration: 236
Loss: 0.0020459258066227613
Iteration: 237
Loss: 0.0020495542707160497
Iteration: 238
Loss: 0.002043117378026438
Iteration: 239
Loss: 0.002045568914558643
Iteration: 240
Loss: 0.002048706437329738
Iteration: 241
Loss: 0.002041303407914268
Iteration: 242
Loss: 0.002044218455035335
Iteration: 243
Loss: 0.0020380427117626635
Iteration: 244
Loss: 0.0020353099829756105
Iteration: 245
Loss: 0.0020250106821409785
Iteration: 246
Loss: 0.002030365142183235
Iteration: 247
Loss: 0.0020497756419130242
Iteration: 248
Loss: 0.0020238381932871654
Iteration: 249
Loss: 0.0020153672682145275
Iteration: 250
Loss: 0.002024989007996061
Iteration: 251
Loss: 0.0020331229330995716
Iteration: 252
Loss: 0.0020253343152431534
Iteration: 253
Loss: 0.002019001642624155
Iteration: 254
Loss: 0.0020283153982092747
Iteration: 255
Loss: 0.0020717515965374424
Iteration: 256
Loss: 0.002028114116117836
Iteration: 257
Loss: 0.0020143229690475916
Iteration: 258
Loss: 0.0020208214454424498
Iteration: 259
Loss: 0.002010152445365794
Iteration: 260
Loss: 0.002007782527914223
Iteration: 261
Loss: 0.00200679645431825
Iteration: 262
Loss: 0.002014249145316008
Iteration: 263
Loss: 0.002009795138385529
Iteration: 264
Loss: 0.0019932476526054624
Iteration: 265
Loss: 0.0020040161833047676
Iteration: 266
Loss: 0.0019988640575693585
Iteration: 267
Loss: 0.0019959785769550274
Iteration: 268
Loss: 0.002001616063539703
Iteration: 269
Loss: 0.0020066764778815783
Iteration: 270
Loss: 0.0019937394998776605
Iteration: 271
Loss: 0.0019909952340337136
Iteration: 272
Loss: 0.0019951463820269476
Iteration: 273
Loss: 0.0019927720736282375
Iteration: 274
Loss: 0.0019919523142445353
Iteration: 275
Loss: 0.001976321678226575
Iteration: 276
Loss: 0.001988738834953461
Iteration: 277
Loss: 0.0019845294285541736
Iteration: 278
Loss: 0.001983183052522154
Iteration: 279
Loss: 0.0019850333546622633
Iteration: 280
Loss: 0.0019802562176035
Iteration: 281
Loss: 0.001985761725033323
Iteration: 282
Loss: 0.001986516327656901
Iteration: 283
Loss: 0.0019841946151035908
Iteration: 284
Loss: 0.001983912819877076
Iteration: 285
Loss: 0.001980244074589931
Iteration: 286
Loss: 0.001994641797724538
Iteration: 287
Loss: 0.001985704556155281
Iteration: 288
Loss: 0.0019734966126867593
Iteration: 289
Loss: 0.0019705447872790196
Iteration: 290
Loss: 0.0019845802617522003
Iteration: 291
Loss: 0.0019866924759788583
Iteration: 292
Loss: 0.0019891232741662324
Iteration: 293
Loss: 0.001976400222748709
Iteration: 294
Loss: 0.001991524116303294
Iteration: 295
Loss: 0.001975699260723419
Iteration: 296
Loss: 0.001968844547473754
Iteration: 297
Loss: 0.0019684212643485996
Iteration: 298
Loss: 0.001973790436004026
Iteration: 299
Loss: 0.0019648485124493255
Iteration: 300
Loss: 0.0019685735326045407
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.3108003108003108
accuracy: 0.9683214285714286
confusion: 200 527 360 26913
precision: 0.2751031636863824
recall: 0.35714285714285715
Processing fold: ../../../kg_constructor/output/ecoli/folds/fold_4
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 26185
Starting training...
Iteration: 1
Loss: 0.7029169316475208
Iteration: 2
Loss: 0.6952130091496003
Iteration: 3
Loss: 0.6766242331419235
Iteration: 4
Loss: 0.6352891356517107
Iteration: 5
Loss: 0.5774748554596534
Iteration: 6
Loss: 0.5101471997988529
Iteration: 7
Loss: 0.4511957714955012
Iteration: 8
Loss: 0.4053647437920937
Iteration: 9
Loss: 0.3705113789974115
Iteration: 10
Loss: 0.3424791063253696
Iteration: 11
Loss: 0.3227273065310258
Iteration: 12
Loss: 0.30573714276154834
Iteration: 13
Loss: 0.2915674734574098
Iteration: 14
Loss: 0.2792146079815351
Iteration: 15
Loss: 0.26643155935483104
Iteration: 16
Loss: 0.25525525479744643
Iteration: 17
Loss: 0.24388528920901129
Iteration: 18
Loss: 0.23386632402737936
Iteration: 19
Loss: 0.22237925709058076
Iteration: 20
Loss: 0.2121595533994528
Iteration: 21
Loss: 0.20133076780117476
Iteration: 22
Loss: 0.19099401720823386
Iteration: 23
Loss: 0.18011842056726798
Iteration: 24
Loss: 0.16943565736978483
Iteration: 25
Loss: 0.15957889075462633
Iteration: 26
Loss: 0.14910006867005274
Iteration: 27
Loss: 0.13985242656408212
Iteration: 28
Loss: 0.1302470509440471
Iteration: 29
Loss: 0.12154956649129207
Iteration: 30
Loss: 0.11270507597006284
Iteration: 31
Loss: 0.1043322348059752
Iteration: 32
Loss: 0.09696351774992087
Iteration: 33
Loss: 0.08925463526676862
Iteration: 34
Loss: 0.08264865955481163
Iteration: 35
Loss: 0.07632462603923602
Iteration: 36
Loss: 0.07072065694209857
Iteration: 37
Loss: 0.06515905581032619
Iteration: 38
Loss: 0.06036895876511549
Iteration: 39
Loss: 0.055616262106177136
Iteration: 40
Loss: 0.05170855035957617
Iteration: 41
Loss: 0.04768551642505022
Iteration: 42
Loss: 0.04414844154738463
Iteration: 43
Loss: 0.04084977216254442
Iteration: 44
Loss: 0.038097305700947076
Iteration: 45
Loss: 0.0353836997006184
Iteration: 46
Loss: 0.03292244128309763
Iteration: 47
Loss: 0.030696907582191322
Iteration: 48
Loss: 0.028744487068018854
Iteration: 49
Loss: 0.02689569827933342
Iteration: 50
Loss: 0.02511294436855958
Iteration: 51
Loss: 0.023599086615901727
Iteration: 52
Loss: 0.022096543907164
Iteration: 53
Loss: 0.020819060050715238
Iteration: 54
Loss: 0.01945862568055208
Iteration: 55
Loss: 0.018432514789776925
Iteration: 56
Loss: 0.01733129891829613
Iteration: 57
Loss: 0.016406149567606356
Iteration: 58
Loss: 0.015518134483733239
Iteration: 59
Loss: 0.014771056827157736
Iteration: 60
Loss: 0.013913604729355145
Iteration: 61
Loss: 0.013317282992200209
Iteration: 62
Loss: 0.012732502622291064
Iteration: 63
Loss: 0.01209732761176733
Iteration: 64
Loss: 0.011433587970737463
Iteration: 65
Loss: 0.010990946852148343
Iteration: 66
Loss: 0.010466786985022899
Iteration: 67
Loss: 0.010056369591695376
Iteration: 68
Loss: 0.009628260949960886
Iteration: 69
Loss: 0.009212630836722942
Iteration: 70
Loss: 0.008836977327099213
Iteration: 71
Loss: 0.008508961850729508
Iteration: 72
Loss: 0.008185010952636218
Iteration: 73
Loss: 0.007866826308413576
Iteration: 74
Loss: 0.007622057798867807
Iteration: 75
Loss: 0.0073572837580472995
Iteration: 76
Loss: 0.007106044789394125
Iteration: 77
Loss: 0.006894722210768706
Iteration: 78
Loss: 0.006667614401055452
Iteration: 79
Loss: 0.0064487217638928155
Iteration: 80
Loss: 0.006261530606888044
Iteration: 81
Loss: 0.006105724602746658
Iteration: 82
Loss: 0.0058676840212100595
Iteration: 83
Loss: 0.00575049546881555
Iteration: 84
Loss: 0.005585820945457389
Iteration: 85
Loss: 0.0054334843626771216
Iteration: 86
Loss: 0.005293380397443588
Iteration: 87
Loss: 0.005174776163095465
Iteration: 88
Loss: 0.005039886069985537
Iteration: 89
Loss: 0.00494041641911444
Iteration: 90
Loss: 0.004828333794975128
Iteration: 91
Loss: 0.004737569825150645
Iteration: 92
Loss: 0.004638889947762856
Iteration: 93
Loss: 0.004574581497133925
Iteration: 94
Loss: 0.00444927947738996
Iteration: 95
Loss: 0.004371418808706296
Iteration: 96
Loss: 0.004295153188137099
Iteration: 97
Loss: 0.004219953370734285
Iteration: 98
Loss: 0.00415714877920273
Iteration: 99
Loss: 0.0040824609474302866
Iteration: 100
Loss: 0.00403065413224678
Iteration: 101
Loss: 0.0039685453491237685
Iteration: 102
Loss: 0.003924455433988418
Iteration: 103
Loss: 0.0038526875969882193
Iteration: 104
Loss: 0.0038091463920397637
Iteration: 105
Loss: 0.0037523835766105317
Iteration: 106
Loss: 0.0037290364897881565
Iteration: 107
Loss: 0.003709904088352162
Iteration: 108
Loss: 0.003632853419567721
Iteration: 109
Loss: 0.0036029266850210917
Iteration: 110
Loss: 0.0035675524357849588
Iteration: 111
Loss: 0.003531897885318941
Iteration: 112
Loss: 0.003502600718862735
Iteration: 113
Loss: 0.003468381049923408
Iteration: 114
Loss: 0.0034431842788576316
Iteration: 115
Loss: 0.0034139049717057976
Iteration: 116
Loss: 0.003374036452255379
Iteration: 117
Loss: 0.003346323262518033
Iteration: 118
Loss: 0.0033234074198378203
Iteration: 119
Loss: 0.0033100211921219644
Iteration: 120
Loss: 0.0032905575234251907
Iteration: 121
Loss: 0.0032599675319850063
Iteration: 122
Loss: 0.0032454254045986976
Iteration: 123
Loss: 0.00321893968905967
Iteration: 124
Loss: 0.0032143309019888057
Iteration: 125
Loss: 0.003173716139430419
Iteration: 126
Loss: 0.0031586499848904517
Iteration: 127
Loss: 0.0031575971665099645
Iteration: 128
Loss: 0.0031147090455469414
Iteration: 129
Loss: 0.0030935040006461814
Iteration: 130
Loss: 0.0030768389980762433
Iteration: 131
Loss: 0.003059215786962364
Iteration: 132
Loss: 0.003067598812497006
Iteration: 133
Loss: 0.0030354378380788825
Iteration: 134
Loss: 0.0030274349145400217
Iteration: 135
Loss: 0.0030127241162774274
Iteration: 136
Loss: 0.0029809833862460577
Iteration: 137
Loss: 0.002964710240634397
Iteration: 138
Loss: 0.0029576358845075355
Iteration: 139
Loss: 0.0029223135385948876
Iteration: 140
Loss: 0.002912887813857733
Iteration: 141
Loss: 0.002899659412483183
Iteration: 142
Loss: 0.0028864955619120826
Iteration: 143
Loss: 0.002895603389240419
Iteration: 144
Loss: 0.0028396509169863584
Iteration: 145
Loss: 0.0028270505762730655
Iteration: 146
Loss: 0.0028067121509080515
Iteration: 147
Loss: 0.0027955537870860635
Iteration: 148
Loss: 0.0027630855568135395
Iteration: 149
Loss: 0.0027551237248982755
Iteration: 150
Loss: 0.0027298191980196116
Iteration: 151
Loss: 0.002722205716567353
Iteration: 152
Loss: 0.0026983544648362277
Iteration: 153
Loss: 0.002680525097112434
Iteration: 154
Loss: 0.002673641068096726
Iteration: 155
Loss: 0.0026593937878855146
Iteration: 156
Loss: 0.0026293194387108088
Iteration: 157
Loss: 0.00260871225514282
Iteration: 158
Loss: 0.0025946448097387566
Iteration: 159
Loss: 0.0025954594990859428
Iteration: 160
Loss: 0.0025571946096487152
Iteration: 161
Loss: 0.002559028097237341
Iteration: 162
Loss: 0.002522646204735606
Iteration: 163
Loss: 0.002502171268973213
Iteration: 164
Loss: 0.0025023296982861864
Iteration: 165
Loss: 0.002479664530628958
Iteration: 166
Loss: 0.002454607589289737
Iteration: 167
Loss: 0.002448722161949636
Iteration: 168
Loss: 0.002444597274566499
Iteration: 169
Loss: 0.0024201317618672666
Iteration: 170
Loss: 0.0024111655821355106
Iteration: 171
Loss: 0.0023856356095236083
Iteration: 172
Loss: 0.0023815272620711955
Iteration: 173
Loss: 0.0023784109970363667
Iteration: 174
Loss: 0.0023732893706227723
Iteration: 175
Loss: 0.002342042740052327
Iteration: 176
Loss: 0.0023360769831551574
Iteration: 177
Loss: 0.0023183239361223504
Iteration: 178
Loss: 0.002315158108010506
Iteration: 179
Loss: 0.0023023828249185896
Iteration: 180
Loss: 0.0022875209524033545
Iteration: 181
Loss: 0.0022868746563266865
Iteration: 182
Loss: 0.002273060721703447
Iteration: 183
Loss: 0.002263452554754435
Iteration: 184
Loss: 0.002277722082232149
Iteration: 185
Loss: 0.0022565135105441394
Iteration: 186
Loss: 0.0022630409982341984
Iteration: 187
Loss: 0.0022409097694505295
Iteration: 188
Loss: 0.0022376536060339557
Iteration: 189
Loss: 0.0022210778552107513
Iteration: 190
Loss: 0.002217209429289095
Iteration: 191
Loss: 0.0022136304223050293
Iteration: 192
Loss: 0.002203617971808387
Iteration: 193
Loss: 0.002195082851148282
Iteration: 194
Loss: 0.0021887176882069656
Iteration: 195
Loss: 0.002178607278694518
Iteration: 196
Loss: 0.0021949154399454785
Iteration: 197
Loss: 0.0021729162246442568
Iteration: 198
Loss: 0.002173447012841606
Iteration: 199
Loss: 0.002161780337934406
Iteration: 200
Loss: 0.0021636563036829615
Iteration: 201
Loss: 0.0021437790307502905
Iteration: 202
Loss: 0.002138879842674121
Iteration: 203
Loss: 0.0021631186126540294
Iteration: 204
Loss: 0.002149489748542412
Iteration: 205
Loss: 0.0021277987025081157
Iteration: 206
Loss: 0.002133839538332839
Iteration: 207
Loss: 0.0021342599573425758
Iteration: 208
Loss: 0.002122585242315649
Iteration: 209
Loss: 0.0021213547204239056
Iteration: 210
Loss: 0.0021222300890188376
Iteration: 211
Loss: 0.0021202039952652576
Iteration: 212
Loss: 0.002119938660866748
Iteration: 213
Loss: 0.002101942499836859
Iteration: 214
Loss: 0.0020988468432990024
Iteration: 215
Loss: 0.002102696274120647
Iteration: 216
Loss: 0.0020995062450544
Iteration: 217
Loss: 0.002097209646868018
Iteration: 218
Loss: 0.002102485342990034
Iteration: 219
Loss: 0.00208834572456395
Iteration: 220
Loss: 0.0020948560618890976
Iteration: 221
Loss: 0.002093714439513114
Iteration: 222
Loss: 0.0020780748574254224
Iteration: 223
Loss: 0.0020814851860706815
Iteration: 224
Loss: 0.0020759876068227757
Iteration: 225
Loss: 0.0020687782148926114
Iteration: 226
Loss: 0.0020705691661136462
Iteration: 227
Loss: 0.0020749076114346585
Iteration: 228
Loss: 0.002061839979619552
Iteration: 229
Loss: 0.0020637554907574295
Iteration: 230
Loss: 0.0020487248742332063
Iteration: 231
Loss: 0.0020590023949551275
Iteration: 232
Loss: 0.0020414393780251536
Iteration: 233
Loss: 0.002055176424805839
Iteration: 234
Loss: 0.002047959356926955
Iteration: 235
Loss: 0.0020356997504281118
Iteration: 236
Loss: 0.0020385654493927574
Iteration: 237
Loss: 0.002036960777611687
Iteration: 238
Loss: 0.0020466317461493113
Iteration: 239
Loss: 0.0020285720283237216
Iteration: 240
Loss: 0.002050192507270437
Iteration: 241
Loss: 0.002044869301757083
Iteration: 242
Loss: 0.002035883397090798
Iteration: 243
Loss: 0.0020327324343200484
Iteration: 244
Loss: 0.002043880792394376
Iteration: 245
Loss: 0.0020294432820847784
Iteration: 246
Loss: 0.0020488438402445843
Iteration: 247
Loss: 0.0020313567544620196
Iteration: 248
Loss: 0.002022938865523499
Iteration: 249
Loss: 0.002033641706638707
Iteration: 250
Loss: 0.002026474780928439
Iteration: 251
Loss: 0.002024283711738789
Iteration: 252
Loss: 0.0020178041886538267
Iteration: 253
Loss: 0.0020228859293871583
Iteration: 254
Loss: 0.0020296737963620285
Iteration: 255
Loss: 0.0020070383713270226
Iteration: 256
Loss: 0.0020095767656293437
Iteration: 257
Loss: 0.0020177665715798354
Iteration: 258
Loss: 0.0020122146562267192
Iteration: 259
Loss: 0.002014925216849989
Iteration: 260
Loss: 0.0020041824497569064
Iteration: 261
Loss: 0.002007525753026876
Iteration: 262
Loss: 0.0020112880410101167
Iteration: 263
Loss: 0.0019954804999706074
Iteration: 264
Loss: 0.0019880211882245466
Iteration: 265
Loss: 0.00200180844284403
Iteration: 266
Loss: 0.0019932153921287795
Iteration: 267
Loss: 0.0020166907537107668
Iteration: 268
Loss: 0.001994148032584538
Iteration: 269
Loss: 0.001995282461090634
Iteration: 270
Loss: 0.0019944484065024136
Iteration: 271
Loss: 0.0019984664977528155
Iteration: 272
Loss: 0.0019841486370214857
Iteration: 273
Loss: 0.00200277227962103
Iteration: 274
Loss: 0.001985944552311244
Iteration: 275
Loss: 0.0020040911965621398
Iteration: 276
Loss: 0.0019817678153347703
Iteration: 277
Loss: 0.0019779068847688343
Iteration: 278
Loss: 0.0019902868109802976
Iteration: 279
Loss: 0.0019823399981340538
Iteration: 280
Loss: 0.002000935610013608
Iteration: 281
Loss: 0.0020033080542746643
Iteration: 282
Loss: 0.001985718778227098
Iteration: 283
Loss: 0.0019862678182574036
Iteration: 284
Loss: 0.001974887704142393
Iteration: 285
Loss: 0.001990279975311401
Iteration: 286
Loss: 0.001969671035066056
Iteration: 287
Loss: 0.001973442991491025
Iteration: 288
Loss: 0.0019727617499227515
Iteration: 289
Loss: 0.001975911760475869
Iteration: 290
Loss: 0.001973215620427464
Iteration: 291
Loss: 0.001977778010022564
Iteration: 292
Loss: 0.001971620637983179
Iteration: 293
Loss: 0.001974475429727672
Iteration: 294
Loss: 0.001959612540817127
Iteration: 295
Loss: 0.00196245252692069
Iteration: 296
Loss: 0.0019626125935106897
Iteration: 297
Loss: 0.001963146739842322
Iteration: 298
Loss: 0.001958710517591009
Iteration: 299
Loss: 0.0019602561369538307
Iteration: 300
Loss: 0.001963646233213158
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.3459715639810426
accuracy: 0.9704285714285714
confusion: 219 487 341 26953
precision: 0.3101983002832861
recall: 0.39107142857142857
*******************************
Processing 2/2...
Hyperparameters: (300, 128, 0.0002, 1.0, 200, 30, 0.2, 0.4, 0.5, 0.1)
Processing fold: ../../../kg_constructor/output/ecoli/folds/fold_0
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 26184
Starting training...
Iteration: 1
Loss: 0.702854702105889
Iteration: 2
Loss: 0.695770854369188
Iteration: 3
Loss: 0.676464570638461
Iteration: 4
Loss: 0.6362782938358111
Iteration: 5
Loss: 0.5765893505169795
Iteration: 6
Loss: 0.5110138593575894
Iteration: 7
Loss: 0.45210175063365543
Iteration: 8
Loss: 0.4037267240958336
Iteration: 9
Loss: 0.3697234258437768
Iteration: 10
Loss: 0.34391117286987793
Iteration: 11
Loss: 0.32386585496939146
Iteration: 12
Loss: 0.30639161780858654
Iteration: 13
Loss: 0.2912698074793204
Iteration: 14
Loss: 0.2785823994722122
Iteration: 15
Loss: 0.26666716237862903
Iteration: 16
Loss: 0.2553879815416458
Iteration: 17
Loss: 0.24447126992237875
Iteration: 18
Loss: 0.23348161425345984
Iteration: 19
Loss: 0.2228890489309262
Iteration: 20
Loss: 0.21254218904635844
Iteration: 21
Loss: 0.2012100733625583
Iteration: 22
Loss: 0.19090819931947267
Iteration: 23
Loss: 0.18049321247216982
Iteration: 24
Loss: 0.17050917217364678
Iteration: 25
Loss: 0.1597833879865133
Iteration: 26
Loss: 0.1498237838729834
Iteration: 27
Loss: 0.14037277205632284
Iteration: 28
Loss: 0.1305523024728665
Iteration: 29
Loss: 0.12226269967280902
Iteration: 30
Loss: 0.11332179663273004
Iteration: 31
Loss: 0.1051178579338086
Iteration: 32
Loss: 0.09728744692909412
Iteration: 33
Loss: 0.0902039586351468
Iteration: 34
Loss: 0.08351992586484322
Iteration: 35
Loss: 0.07726784107776788
Iteration: 36
Loss: 0.07118802431684274
Iteration: 37
Loss: 0.06567918590437143
Iteration: 38
Loss: 0.060585492792037815
Iteration: 39
Loss: 0.05602432156984623
Iteration: 40
Loss: 0.05182223509137447
Iteration: 41
Loss: 0.04789884641575508
Iteration: 42
Loss: 0.04466276405713497
Iteration: 43
Loss: 0.04120059841527389
Iteration: 44
Loss: 0.038242551330954604
Iteration: 45
Loss: 0.03547362844722393
Iteration: 46
Loss: 0.033249771365752585
Iteration: 47
Loss: 0.030985505033570986
Iteration: 48
Loss: 0.028905266370528784
Iteration: 49
Loss: 0.026887909007760193
Iteration: 50
Loss: 0.025179331095363848
Iteration: 51
Loss: 0.023693702374704372
Iteration: 52
Loss: 0.022121855296576634
Iteration: 53
Loss: 0.0208557463513735
Iteration: 54
Loss: 0.019521404415942155
Iteration: 55
Loss: 0.01845710328183113
Iteration: 56
Loss: 0.017497799860743377
Iteration: 57
Loss: 0.01642301408812786
Iteration: 58
Loss: 0.015527109949825665
Iteration: 59
Loss: 0.014810305387259293
Iteration: 60
Loss: 0.013970161585185008
Iteration: 61
Loss: 0.013300789436564231
Iteration: 62
Loss: 0.01271721262198228
Iteration: 63
Loss: 0.012082254346937705
Iteration: 64
Loss: 0.011483328369183417
Iteration: 65
Loss: 0.011013739121456942
Iteration: 66
Loss: 0.010530020922231369
Iteration: 67
Loss: 0.010091008045352422
Iteration: 68
Loss: 0.009683221889038881
Iteration: 69
Loss: 0.00920946560561275
Iteration: 70
Loss: 0.008813656651629852
Iteration: 71
Loss: 0.008608343646837732
Iteration: 72
Loss: 0.00823853096853082
Iteration: 73
Loss: 0.007885096320070518
Iteration: 74
Loss: 0.007650084423426635
Iteration: 75
Loss: 0.007339412776323466
Iteration: 76
Loss: 0.0071042808524977704
Iteration: 77
Loss: 0.006830894310648243
Iteration: 78
Loss: 0.0066813981065001245
Iteration: 79
Loss: 0.0065008117459141295
Iteration: 80
Loss: 0.006291242310395226
Iteration: 81
Loss: 0.006044037783375153
Iteration: 82
Loss: 0.00588636666846772
Iteration: 83
Loss: 0.005752591714740564
Iteration: 84
Loss: 0.0055833015686426405
Iteration: 85
Loss: 0.0054425467880299455
Iteration: 86
Loss: 0.005326984175600302
Iteration: 87
Loss: 0.005180071740864943
Iteration: 88
Loss: 0.005035160718342433
Iteration: 89
Loss: 0.004917931104174409
Iteration: 90
Loss: 0.004822024305422719
Iteration: 91
Loss: 0.004729862563694134
Iteration: 92
Loss: 0.004613374234535373
Iteration: 93
Loss: 0.004527797379220526
Iteration: 94
Loss: 0.004436233534644811
Iteration: 95
Loss: 0.004366578008884039
Iteration: 96
Loss: 0.0042795130463603595
Iteration: 97
Loss: 0.004221740083243602
Iteration: 98
Loss: 0.004137629678902717
Iteration: 99
Loss: 0.004085223354661885
Iteration: 100
Loss: 0.004008102899369521
Iteration: 101
Loss: 0.003967749187722802
Iteration: 102
Loss: 0.0039005232945036813
Iteration: 103
Loss: 0.00383752955792424
Iteration: 104
Loss: 0.0037902883397272
Iteration: 105
Loss: 0.0037641589446232105
Iteration: 106
Loss: 0.003708022389895259
Iteration: 107
Loss: 0.0036714372541516637
Iteration: 108
Loss: 0.0036322793338256767
Iteration: 109
Loss: 0.0035978904865586604
Iteration: 110
Loss: 0.0035431829448311757
Iteration: 111
Loss: 0.0035293949379896126
Iteration: 112
Loss: 0.0034915177384391427
Iteration: 113
Loss: 0.003452539094723761
Iteration: 114
Loss: 0.003424602396523532
Iteration: 115
Loss: 0.003398863849445031
Iteration: 116
Loss: 0.0033720563321063914
Iteration: 117
Loss: 0.0033435944873744096
Iteration: 118
Loss: 0.0033291056155203246
Iteration: 119
Loss: 0.0032922403725723806
Iteration: 120
Loss: 0.003291424277286308
Iteration: 121
Loss: 0.0032597579127655197
Iteration: 122
Loss: 0.0032214145922364714
Iteration: 123
Loss: 0.003205968087945038
Iteration: 124
Loss: 0.0031764241001115013
Iteration: 125
Loss: 0.0031636026329719103
Iteration: 126
Loss: 0.003150985668747662
Iteration: 127
Loss: 0.0031415214971042215
Iteration: 128
Loss: 0.003110751417131187
Iteration: 129
Loss: 0.0030905962832128773
Iteration: 130
Loss: 0.003087070564596126
Iteration: 131
Loss: 0.0030632239575378406
Iteration: 132
Loss: 0.0030581650759977028
Iteration: 133
Loss: 0.003019494061262753
Iteration: 134
Loss: 0.0030150830190485488
Iteration: 135
Loss: 0.0029925796358535686
Iteration: 136
Loss: 0.0029760155056674895
Iteration: 137
Loss: 0.0029573569116063225
Iteration: 138
Loss: 0.002957182366233797
Iteration: 139
Loss: 0.0029310214046675423
Iteration: 140
Loss: 0.0029098023660480976
Iteration: 141
Loss: 0.002897011692253634
Iteration: 142
Loss: 0.002876945526506274
Iteration: 143
Loss: 0.00286212825598434
Iteration: 144
Loss: 0.002848009555003582
Iteration: 145
Loss: 0.0028254681278784308
Iteration: 146
Loss: 0.002820844585911777
Iteration: 147
Loss: 0.002786736807021766
Iteration: 148
Loss: 0.0027750088200450707
Iteration: 149
Loss: 0.002766347284285495
Iteration: 150
Loss: 0.0027422848312805095
Iteration: 151
Loss: 0.002727365900332538
Iteration: 152
Loss: 0.002705597205278583
Iteration: 153
Loss: 0.0026842282416346744
Iteration: 154
Loss: 0.0026923950988417254
Iteration: 155
Loss: 0.0026601148434938528
Iteration: 156
Loss: 0.0026431862766352985
Iteration: 157
Loss: 0.002625727243960286
Iteration: 158
Loss: 0.002610304371979183
Iteration: 159
Loss: 0.0026224443020346835
Iteration: 160
Loss: 0.002577301243152947
Iteration: 161
Loss: 0.002557484021123785
Iteration: 162
Loss: 0.002556794478247563
Iteration: 163
Loss: 0.002531833090007496
Iteration: 164
Loss: 0.002520597139851023
Iteration: 165
Loss: 0.0025014385330275847
Iteration: 166
Loss: 0.002501844329974399
Iteration: 167
Loss: 0.002472855876056621
Iteration: 168
Loss: 0.0024614621771690557
Iteration: 169
Loss: 0.002451551866192275
Iteration: 170
Loss: 0.002431709214877815
Iteration: 171
Loss: 0.0024380498649313664
Iteration: 172
Loss: 0.0024050893172478448
Iteration: 173
Loss: 0.0023966853668053564
Iteration: 174
Loss: 0.002379873227208662
Iteration: 175
Loss: 0.002374155596137429
Iteration: 176
Loss: 0.0023582198042183733
Iteration: 177
Loss: 0.002344537803974862
Iteration: 178
Loss: 0.002338490383221935
Iteration: 179
Loss: 0.002329378272406757
Iteration: 180
Loss: 0.002323424076446547
Iteration: 181
Loss: 0.0023202444187914715
Iteration: 182
Loss: 0.002300882252315298
Iteration: 183
Loss: 0.0022885282553780154
Iteration: 184
Loss: 0.0022803768545436934
Iteration: 185
Loss: 0.0022757404442064655
Iteration: 186
Loss: 0.002274963294323056
Iteration: 187
Loss: 0.0022586051252288506
Iteration: 188
Loss: 0.0022581856539592338
Iteration: 189
Loss: 0.0022348614561801348
Iteration: 190
Loss: 0.002227256931245136
Iteration: 191
Loss: 0.0022366889155445956
Iteration: 192
Loss: 0.00222955502044314
Iteration: 193
Loss: 0.0022212121984921396
Iteration: 194
Loss: 0.0022098909099347508
Iteration: 195
Loss: 0.002216419218502079
Iteration: 196
Loss: 0.002208720828490093
Iteration: 197
Loss: 0.002197850841837816
Iteration: 198
Loss: 0.0021918550363550773
Iteration: 199
Loss: 0.0021888485539537403
Iteration: 200
Loss: 0.00217949858872602
Iteration: 201
Loss: 0.002171591379476759
Iteration: 202
Loss: 0.002169011587587496
Iteration: 203
Loss: 0.002167381729799299
Iteration: 204
Loss: 0.0021703010335421334
Iteration: 205
Loss: 0.0021522823461474706
Iteration: 206
Loss: 0.002143777015869721
Iteration: 207
Loss: 0.0021441789248432866
Iteration: 208
Loss: 0.0021493924178708442
Iteration: 209
Loss: 0.0021448868732803906
Iteration: 210
Loss: 0.002135974277431766
Iteration: 211
Loss: 0.0021182850513678905
Iteration: 212
Loss: 0.0021387993310315488
Iteration: 213
Loss: 0.0021235905289172363
Iteration: 214
Loss: 0.002115014782294822
Iteration: 215
Loss: 0.002121220672658334
Iteration: 216
Loss: 0.0021210852965640905
Iteration: 217
Loss: 0.0021080524386026156
Iteration: 218
Loss: 0.0021192405689268922
Iteration: 219
Loss: 0.0021190348764857612
Iteration: 220
Loss: 0.0020927544951868746
Iteration: 221
Loss: 0.0020871333545073867
Iteration: 222
Loss: 0.002097677999140265
Iteration: 223
Loss: 0.002088939600910705
Iteration: 224
Loss: 0.0020862773645860264
Iteration: 225
Loss: 0.0020893263893249706
Iteration: 226
Loss: 0.0020893832417921377
Iteration: 227
Loss: 0.0020932200773714636
Iteration: 228
Loss: 0.0020828025761800697
Iteration: 229
Loss: 0.002069852258174274
Iteration: 230
Loss: 0.0020884413657208476
Iteration: 231
Loss: 0.002079282812654781
Iteration: 232
Loss: 0.002069025984010062
Iteration: 233
Loss: 0.0020562166195864286
Iteration: 234
Loss: 0.0020653248081007637
Iteration: 235
Loss: 0.0020733533833080377
Iteration: 236
Loss: 0.0020599549170583487
Iteration: 237
Loss: 0.0020545185200320794
Iteration: 238
Loss: 0.0020491029523933926
Iteration: 239
Loss: 0.0020611152852861546
Iteration: 240
Loss: 0.002051337101222135
Iteration: 241
Loss: 0.002046449266624852
Iteration: 242
Loss: 0.002044428969757297
Iteration: 243
Loss: 0.0020426663761146558
Iteration: 244
Loss: 0.0020537471789747286
Iteration: 245
Loss: 0.002038530356143243
Iteration: 246
Loss: 0.0020374141824551118
Iteration: 247
Loss: 0.002034015005800682
Iteration: 248
Loss: 0.002039147123025778
Iteration: 249
Loss: 0.0020302934467625352
Iteration: 250
Loss: 0.0020220651446531215
Iteration: 251
Loss: 0.00204783166722896
Iteration: 252
Loss: 0.002031556870907736
Iteration: 253
Loss: 0.002025209059819388
Iteration: 254
Loss: 0.0020189838310799156
Iteration: 255
Loss: 0.0020235784572716323
Iteration: 256
Loss: 0.0020201857342050434
Iteration: 257
Loss: 0.002024979733575422
Iteration: 258
Loss: 0.002021910433167926
Iteration: 259
Loss: 0.0020156150358394743
Iteration: 260
Loss: 0.0020140049567159554
Iteration: 261
Loss: 0.002026095928158611
Iteration: 262
Loss: 0.0020119589783299044
Iteration: 263
Loss: 0.0020152539886438693
Iteration: 264
Loss: 0.0020097946816792665
Iteration: 265
Loss: 0.00200449478908036
Iteration: 266
Loss: 0.0020151757948602047
Iteration: 267
Loss: 0.002011143329302565
Iteration: 268
Loss: 0.002009189484688716
Iteration: 269
Loss: 0.002016234131028446
Iteration: 270
Loss: 0.0020059599191881716
Iteration: 271
Loss: 0.0020070402056145934
Iteration: 272
Loss: 0.0020064148851909125
Iteration: 273
Loss: 0.0020174933328794744
Iteration: 274
Loss: 0.0019951667099331436
Iteration: 275
Loss: 0.00199873844991462
Iteration: 276
Loss: 0.0020015103629049966
Iteration: 277
Loss: 0.0019888164959131526
Iteration: 278
Loss: 0.0019926234590223967
Iteration: 279
Loss: 0.0019952024658544897
Iteration: 280
Loss: 0.0019856528602898694
Iteration: 281
Loss: 0.0019895248219538
Iteration: 282
Loss: 0.002000412729103118
Iteration: 283
Loss: 0.0019974092272325205
Iteration: 284
Loss: 0.0019982836943549607
Iteration: 285
Loss: 0.0019860050032058586
Iteration: 286
Loss: 0.001991078790384703
Iteration: 287
Loss: 0.0019862226073224195
Iteration: 288
Loss: 0.001984516939279647
Iteration: 289
Loss: 0.001987738794503877
Iteration: 290
Loss: 0.0019817980534683624
Iteration: 291
Loss: 0.001985853394636741
Iteration: 292
Loss: 0.001990067856147503
Iteration: 293
Loss: 0.0019910688174721333
Iteration: 294
Loss: 0.001976833493711475
Iteration: 295
Loss: 0.001978918505557932
Iteration: 296
Loss: 0.0019741923002024684
Iteration: 297
Loss: 0.001962534934043502
Iteration: 298
Loss: 0.0019742718058972596
Iteration: 299
Loss: 0.001978995162062347
Iteration: 300
Loss: 0.0019751295390634392
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.3355704697986577
accuracy: 0.971764705882353
confusion: 200 431 361 27058
precision: 0.31695721077654515
recall: 0.35650623885918004
Processing fold: ../../../kg_constructor/output/ecoli/folds/fold_1
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 26184
Starting training...
Iteration: 1
Loss: 0.7030532268377451
Iteration: 2
Loss: 0.695881269681148
Iteration: 3
Loss: 0.678171327480903
Iteration: 4
Loss: 0.6395704111991785
Iteration: 5
Loss: 0.5805225869019827
Iteration: 6
Loss: 0.5133750595343418
Iteration: 7
Loss: 0.45451613114430356
Iteration: 8
Loss: 0.4080460258783438
Iteration: 9
Loss: 0.3711347186412567
Iteration: 10
Loss: 0.34437123093849575
Iteration: 11
Loss: 0.3228376202094249
Iteration: 12
Loss: 0.3058906770669497
Iteration: 13
Loss: 0.2922105750976465
Iteration: 14
Loss: 0.27889810273280513
Iteration: 15
Loss: 0.2664692249053564
Iteration: 16
Loss: 0.25450960432107633
Iteration: 17
Loss: 0.2437488108109205
Iteration: 18
Loss: 0.23264757161721206
Iteration: 19
Loss: 0.22173349903180048
Iteration: 20
Loss: 0.21116336836264685
Iteration: 21
Loss: 0.20078140573623854
Iteration: 22
Loss: 0.19057393207764015
Iteration: 23
Loss: 0.17966773876777062
Iteration: 24
Loss: 0.16937118157362327
Iteration: 25
Loss: 0.15852640301753312
Iteration: 26
Loss: 0.14906651087296316
Iteration: 27
Loss: 0.13874627707096246
Iteration: 28
Loss: 0.13007647486833426
Iteration: 29
Loss: 0.1205677761672399
Iteration: 30
Loss: 0.11222179740285262
Iteration: 31
Loss: 0.10375553503250465
Iteration: 32
Loss: 0.09666278222814584
Iteration: 33
Loss: 0.08897490713458794
Iteration: 34
Loss: 0.08241582432618508
Iteration: 35
Loss: 0.07612317112775949
Iteration: 36
Loss: 0.07016738045674104
Iteration: 37
Loss: 0.06495283763760175
Iteration: 38
Loss: 0.05986868408627999
Iteration: 39
Loss: 0.055621410552889876
Iteration: 40
Loss: 0.05121678185577576
Iteration: 41
Loss: 0.04758109042468744
Iteration: 42
Loss: 0.044077941765769936
Iteration: 43
Loss: 0.041007715540054515
Iteration: 44
Loss: 0.03804641312513596
Iteration: 45
Loss: 0.03533666065106025
Iteration: 46
Loss: 0.03267349520077308
Iteration: 47
Loss: 0.030613364221958015
Iteration: 48
Loss: 0.028645907146617387
Iteration: 49
Loss: 0.026630750236411888
Iteration: 50
Loss: 0.025030026451135293
Iteration: 51
Loss: 0.02349110948256193
Iteration: 52
Loss: 0.022023022724076722
Iteration: 53
Loss: 0.020578157037305526
Iteration: 54
Loss: 0.01953251874790742
Iteration: 55
Loss: 0.01838398466889675
Iteration: 56
Loss: 0.017258114682940338
Iteration: 57
Loss: 0.01638710028372514
Iteration: 58
Loss: 0.01546315471522319
Iteration: 59
Loss: 0.014648501761257648
Iteration: 60
Loss: 0.013861582077179965
Iteration: 61
Loss: 0.013231798719901305
Iteration: 62
Loss: 0.01259159736144237
Iteration: 63
Loss: 0.011980147865147162
Iteration: 64
Loss: 0.011312026494684128
Iteration: 65
Loss: 0.010923544565836588
Iteration: 66
Loss: 0.010426880923123697
Iteration: 67
Loss: 0.010013921198267967
Iteration: 68
Loss: 0.009612409612880303
Iteration: 69
Loss: 0.009183380430421004
Iteration: 70
Loss: 0.008796602630844483
Iteration: 71
Loss: 0.008486663242085623
Iteration: 72
Loss: 0.008186768955336167
Iteration: 73
Loss: 0.007897959831051337
Iteration: 74
Loss: 0.007598427375062154
Iteration: 75
Loss: 0.007357708279950879
Iteration: 76
Loss: 0.007102617576049688
Iteration: 77
Loss: 0.006861907203132525
Iteration: 78
Loss: 0.006640242179855704
Iteration: 79
Loss: 0.00646088511730807
Iteration: 80
Loss: 0.006233312153758911
Iteration: 81
Loss: 0.006085474294825242
Iteration: 82
Loss: 0.005890288909610648
Iteration: 83
Loss: 0.005738435020574775
Iteration: 84
Loss: 0.005561968699718515
Iteration: 85
Loss: 0.005456014601991345
Iteration: 86
Loss: 0.005320395844487043
Iteration: 87
Loss: 0.005186623081755944
Iteration: 88
Loss: 0.0050688686911971904
Iteration: 89
Loss: 0.004936173068693815
Iteration: 90
Loss: 0.004830892048537349
Iteration: 91
Loss: 0.004721961700572417
Iteration: 92
Loss: 0.004627758571400474
Iteration: 93
Loss: 0.004555981773405503
Iteration: 94
Loss: 0.00445812578814534
Iteration: 95
Loss: 0.004378489275367405
Iteration: 96
Loss: 0.004290528693952813
Iteration: 97
Loss: 0.004213533197075893
Iteration: 98
Loss: 0.004172833851323678
Iteration: 99
Loss: 0.004100537679802913
Iteration: 100
Loss: 0.004027262291250129
Iteration: 101
Loss: 0.003976277830915
Iteration: 102
Loss: 0.003917980673484122
Iteration: 103
Loss: 0.0038605363418658576
Iteration: 104
Loss: 0.00381784802243018
Iteration: 105
Loss: 0.003780217193520795
Iteration: 106
Loss: 0.0037099962474969337
Iteration: 107
Loss: 0.003678826522679092
Iteration: 108
Loss: 0.0036412800321928584
Iteration: 109
Loss: 0.0035963402822231636
Iteration: 110
Loss: 0.0035693405900532617
Iteration: 111
Loss: 0.0035319159356638407
Iteration: 112
Loss: 0.003494401605656514
Iteration: 113
Loss: 0.003467923292937951
Iteration: 114
Loss: 0.0034379559324290124
Iteration: 115
Loss: 0.003410586684297484
Iteration: 116
Loss: 0.0033852342599764084
Iteration: 117
Loss: 0.0033631345842224667
Iteration: 118
Loss: 0.0033386736535109007
Iteration: 119
Loss: 0.0033076041772101936
Iteration: 120
Loss: 0.0032785602170639695
Iteration: 121
Loss: 0.003269375886959143
Iteration: 122
Loss: 0.0032392890514949192
Iteration: 123
Loss: 0.0032333538222771427
Iteration: 124
Loss: 0.0032075747208765303
Iteration: 125
Loss: 0.0031874533265064922
Iteration: 126
Loss: 0.0031820579121510186
Iteration: 127
Loss: 0.0031370516173923626
Iteration: 128
Loss: 0.003126161901328044
Iteration: 129
Loss: 0.0031041986314961924
Iteration: 130
Loss: 0.00309144166441491
Iteration: 131
Loss: 0.003096506157770562
Iteration: 132
Loss: 0.00306041698413304
Iteration: 133
Loss: 0.0030453655158336726
Iteration: 134
Loss: 0.003019006672100379
Iteration: 135
Loss: 0.003006290820522759
Iteration: 136
Loss: 0.002982239943379775
Iteration: 137
Loss: 0.002977806176297749
Iteration: 138
Loss: 0.00296127319168777
Iteration: 139
Loss: 0.0029422874646022534
Iteration: 140
Loss: 0.002932473217161038
Iteration: 141
Loss: 0.002909040848652904
Iteration: 142
Loss: 0.002893921886331951
Iteration: 143
Loss: 0.0028676462467186726
Iteration: 144
Loss: 0.002846985410612363
Iteration: 145
Loss: 0.0028360789635767923
Iteration: 146
Loss: 0.002824261232327001
Iteration: 147
Loss: 0.0027979598291075
Iteration: 148
Loss: 0.0027901216839941647
Iteration: 149
Loss: 0.002822762922359965
Iteration: 150
Loss: 0.0027477504798951438
Iteration: 151
Loss: 0.0027302794653970078
Iteration: 152
Loss: 0.0027175997712243446
Iteration: 153
Loss: 0.002698389073021901
Iteration: 154
Loss: 0.0026794913469646606
Iteration: 155
Loss: 0.002662061671845806
Iteration: 156
Loss: 0.002651058755313548
Iteration: 157
Loss: 0.0026319651739098704
Iteration: 158
Loss: 0.00261596272377154
Iteration: 159
Loss: 0.0025980054633691907
Iteration: 160
Loss: 0.0025927438251435375
Iteration: 161
Loss: 0.002575871710761044
Iteration: 162
Loss: 0.002557181917393628
Iteration: 163
Loss: 0.002546504157810257
Iteration: 164
Loss: 0.002529188889102676
Iteration: 165
Loss: 0.0025109456499847463
Iteration: 166
Loss: 0.00249347086243618
Iteration: 167
Loss: 0.0024842537629107633
Iteration: 168
Loss: 0.0024652938024164774
Iteration: 169
Loss: 0.002457383470848585
Iteration: 170
Loss: 0.002461126151805123
Iteration: 171
Loss: 0.0024479032607558062
Iteration: 172
Loss: 0.002416158435293115
Iteration: 173
Loss: 0.002417303887434686
Iteration: 174
Loss: 0.002414599950544727
Iteration: 175
Loss: 0.0023913271486377106
Iteration: 176
Loss: 0.0023834341181776463
Iteration: 177
Loss: 0.0023593804261718807
Iteration: 178
Loss: 0.0023492851222936925
Iteration: 179
Loss: 0.0023535656659171367
Iteration: 180
Loss: 0.002338894254838427
Iteration: 181
Loss: 0.0023340475536548556
Iteration: 182
Loss: 0.002323056478650333
Iteration: 183
Loss: 0.00230682199785056
Iteration: 184
Loss: 0.0023005968646313516
Iteration: 185
Loss: 0.00229222327321529
Iteration: 186
Loss: 0.0022837117087478056
Iteration: 187
Loss: 0.0022808392591869985
Iteration: 188
Loss: 0.002264877751421852
Iteration: 189
Loss: 0.0022502043568051588
Iteration: 190
Loss: 0.0022484726759079746
Iteration: 191
Loss: 0.00225571479397611
Iteration: 192
Loss: 0.0022323713593113306
Iteration: 193
Loss: 0.0022456268809783533
Iteration: 194
Loss: 0.0022338014722873387
Iteration: 195
Loss: 0.002215393729364643
Iteration: 196
Loss: 0.0022236106287425337
Iteration: 197
Loss: 0.0022115746695094574
Iteration: 198
Loss: 0.002204013436149137
Iteration: 199
Loss: 0.0022145092550020376
Iteration: 200
Loss: 0.0021966457731114365
Iteration: 201
Loss: 0.0021942715333679165
Iteration: 202
Loss: 0.0021735296960776816
Iteration: 203
Loss: 0.0021852858630247796
Iteration: 204
Loss: 0.002166891772037324
Iteration: 205
Loss: 0.0021817256869652714
Iteration: 206
Loss: 0.0021547877829736816
Iteration: 207
Loss: 0.0021567245757278916
Iteration: 208
Loss: 0.002162250740907322
Iteration: 209
Loss: 0.002155450635398619
Iteration: 210
Loss: 0.0021583342862028917
Iteration: 211
Loss: 0.0021382479656798146
Iteration: 212
Loss: 0.0021375055866650282
Iteration: 213
Loss: 0.0021427969227377805
Iteration: 214
Loss: 0.002135939028961823
Iteration: 215
Loss: 0.002135011467497605
Iteration: 216
Loss: 0.00214448675680428
Iteration: 217
Loss: 0.0021266117632293548
Iteration: 218
Loss: 0.0021459118609364405
Iteration: 219
Loss: 0.002104951231441914
Iteration: 220
Loss: 0.002110149236754156
Iteration: 221
Loss: 0.002099345036698744
Iteration: 222
Loss: 0.002100663198432766
Iteration: 223
Loss: 0.0021011150704744533
Iteration: 224
Loss: 0.0020914623642173144
Iteration: 225
Loss: 0.002087950094555242
Iteration: 226
Loss: 0.0020960302193625234
Iteration: 227
Loss: 0.0020844277057151953
Iteration: 228
Loss: 0.0021100556343578948
Iteration: 229
Loss: 0.0020858106242182353
Iteration: 230
Loss: 0.0020897706212380375
Iteration: 231
Loss: 0.0020730158251423677
Iteration: 232
Loss: 0.002068515314768331
Iteration: 233
Loss: 0.002070959482807666
Iteration: 234
Loss: 0.0020834056747098192
Iteration: 235
Loss: 0.0020664847808149764
Iteration: 236
Loss: 0.0020784206467513474
Iteration: 237
Loss: 0.0020669249874444152
Iteration: 238
Loss: 0.002068189817528503
Iteration: 239
Loss: 0.0020822022134103836
Iteration: 240
Loss: 0.0020735586250129226
Iteration: 241
Loss: 0.0020590219482516823
Iteration: 242
Loss: 0.0020549059815657064
Iteration: 243
Loss: 0.0020419555560996137
Iteration: 244
Loss: 0.0020543753202312076
Iteration: 245
Loss: 0.002051234599023771
Iteration: 246
Loss: 0.0020576028171210336
Iteration: 247
Loss: 0.002047482704839263
Iteration: 248
Loss: 0.0020503915132930838
Iteration: 249
Loss: 0.0020543067366816103
Iteration: 250
Loss: 0.0020411614453993165
Iteration: 251
Loss: 0.0020312387138031996
Iteration: 252
Loss: 0.002047285955482855
Iteration: 253
Loss: 0.00205582088808744
Iteration: 254
Loss: 0.002027792387450926
Iteration: 255
Loss: 0.0020521568620983414
Iteration: 256
Loss: 0.0020308795382077685
Iteration: 257
Loss: 0.0020201226982932826
Iteration: 258
Loss: 0.002018066355958581
Iteration: 259
Loss: 0.00202114562330863
Iteration: 260
Loss: 0.002036467813946402
Iteration: 261
Loss: 0.002030491712503135
Iteration: 262
Loss: 0.002029707274722079
Iteration: 263
Loss: 0.002014770917295932
Iteration: 264
Loss: 0.0020158767285279166
Iteration: 265
Loss: 0.002010032671909684
Iteration: 266
Loss: 0.002023698809819344
Iteration: 267
Loss: 0.0020319557859180258
Iteration: 268
Loss: 0.0020392527997804186
Iteration: 269
Loss: 0.0020091610733801737
Iteration: 270
Loss: 0.002009280121479279
Iteration: 271
Loss: 0.0020081751520196213
Iteration: 272
Loss: 0.002010848004526148
Iteration: 273
Loss: 0.002015350628286027
Iteration: 274
Loss: 0.001998933918702488
Iteration: 275
Loss: 0.002000191410143788
Iteration: 276
Loss: 0.001998746413916636
Iteration: 277
Loss: 0.002034518554305228
Iteration: 278
Loss: 0.0019930013371870304
Iteration: 279
Loss: 0.002015630542957343
Iteration: 280
Loss: 0.002001892691716934
Iteration: 281
Loss: 0.0019942914532760205
Iteration: 282
Loss: 0.001992880924939154
Iteration: 283
Loss: 0.001997649005482881
Iteration: 284
Loss: 0.0020056419113340476
Iteration: 285
Loss: 0.0020002703009268795
Iteration: 286
Loss: 0.0020357443957040324
Iteration: 287
Loss: 0.0020051091336286985
Iteration: 288
Loss: 0.0019973242544951155
Iteration: 289
Loss: 0.0019955371927398327
Iteration: 290
Loss: 0.00199839412025773
Iteration: 291
Loss: 0.0019903475469432007
Iteration: 292
Loss: 0.001985280705878559
Iteration: 293
Loss: 0.001987188903746219
Iteration: 294
Loss: 0.0019960559513539267
Iteration: 295
Loss: 0.0019883313096868685
Iteration: 296
Loss: 0.0019800181661804137
Iteration: 297
Loss: 0.001973385510679621
Iteration: 298
Loss: 0.001981605341335615
Iteration: 299
Loss: 0.001986990864866246
Iteration: 300
Loss: 0.00199336679920984
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.30745098039215685
accuracy: 0.9685204991087344
confusion: 196 518 365 26971
precision: 0.27450980392156865
recall: 0.3493761140819964
Processing fold: ../../../kg_constructor/output/ecoli/folds/fold_2
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 26184
Starting training...
Iteration: 1
Loss: 0.70300271266546
Iteration: 2
Loss: 0.6954274842372308
Iteration: 3
Loss: 0.6760750856155004
Iteration: 4
Loss: 0.6365197148078527
Iteration: 5
Loss: 0.5769995130025424
Iteration: 6
Loss: 0.5111010987789203
Iteration: 7
Loss: 0.45458045983925843
Iteration: 8
Loss: 0.40629508671088094
Iteration: 9
Loss: 0.3714356613464845
Iteration: 10
Loss: 0.344162411796741
Iteration: 11
Loss: 0.3243840646285277
Iteration: 12
Loss: 0.30544673746977097
Iteration: 13
Loss: 0.291626282227345
Iteration: 14
Loss: 0.2784371356933545
Iteration: 15
Loss: 0.2667619396860783
Iteration: 16
Loss: 0.2554450415265866
Iteration: 17
Loss: 0.2449947931827643
Iteration: 18
Loss: 0.2338185062011083
Iteration: 19
Loss: 0.22301141592936638
Iteration: 20
Loss: 0.21234372640267396
Iteration: 21
Loss: 0.20221764479692167
Iteration: 22
Loss: 0.19138404784294274
Iteration: 23
Loss: 0.18121192298638514
Iteration: 24
Loss: 0.17075073222319284
Iteration: 25
Loss: 0.16011241517769984
Iteration: 26
Loss: 0.1505599849117108
Iteration: 27
Loss: 0.14155288212574446
Iteration: 28
Loss: 0.13190507735961524
Iteration: 29
Loss: 0.12276001093097222
Iteration: 30
Loss: 0.11386577011300968
Iteration: 31
Loss: 0.10598747871625118
Iteration: 32
Loss: 0.09793984058957833
Iteration: 33
Loss: 0.09096215388331658
Iteration: 34
Loss: 0.08410264876408455
Iteration: 35
Loss: 0.07762258967910057
Iteration: 36
Loss: 0.07178196033988243
Iteration: 37
Loss: 0.0663355304262577
Iteration: 38
Loss: 0.06154430796129581
Iteration: 39
Loss: 0.056974018852298074
Iteration: 40
Loss: 0.05253872953546353
Iteration: 41
Loss: 0.04828918066162329
Iteration: 42
Loss: 0.04498759322823622
Iteration: 43
Loss: 0.041782704062568836
Iteration: 44
Loss: 0.03872570830086867
Iteration: 45
Loss: 0.03612283044136488
Iteration: 46
Loss: 0.033675361114243664
Iteration: 47
Loss: 0.031238045495672103
Iteration: 48
Loss: 0.029225990558281924
Iteration: 49
Loss: 0.027489606911937397
Iteration: 50
Loss: 0.025506003115039606
Iteration: 51
Loss: 0.02391434508638504
Iteration: 52
Loss: 0.02234468074181141
Iteration: 53
Loss: 0.02117006717106471
Iteration: 54
Loss: 0.019744701062639553
Iteration: 55
Loss: 0.018700600816653326
Iteration: 56
Loss: 0.01765836505458141
Iteration: 57
Loss: 0.01668852086489399
Iteration: 58
Loss: 0.01589461546152448
Iteration: 59
Loss: 0.014960012434480282
Iteration: 60
Loss: 0.01422687304707674
Iteration: 61
Loss: 0.013491089169222575
Iteration: 62
Loss: 0.012895352087723903
Iteration: 63
Loss: 0.01228527877575312
Iteration: 64
Loss: 0.011630882664273182
Iteration: 65
Loss: 0.011153378702986699
Iteration: 66
Loss: 0.010689819864451122
Iteration: 67
Loss: 0.010154781695932914
Iteration: 68
Loss: 0.009779512332991147
Iteration: 69
Loss: 0.009314458566502882
Iteration: 70
Loss: 0.009001425753992338
Iteration: 71
Loss: 0.008606684746411748
Iteration: 72
Loss: 0.008373196547230085
Iteration: 73
Loss: 0.008015483683452774
Iteration: 74
Loss: 0.007710924372076988
Iteration: 75
Loss: 0.0074689787251349445
Iteration: 76
Loss: 0.00717691733286931
Iteration: 77
Loss: 0.006938207482632536
Iteration: 78
Loss: 0.0067336628070244426
Iteration: 79
Loss: 0.00648205918379319
Iteration: 80
Loss: 0.006330923475802709
Iteration: 81
Loss: 0.006150219893942659
Iteration: 82
Loss: 0.005946154192758677
Iteration: 83
Loss: 0.005813936476046458
Iteration: 84
Loss: 0.005640192506596064
Iteration: 85
Loss: 0.005498141044368729
Iteration: 86
Loss: 0.005361997271673038
Iteration: 87
Loss: 0.005257867049807921
Iteration: 88
Loss: 0.0050873520413939
Iteration: 89
Loss: 0.004967394135653591
Iteration: 90
Loss: 0.004893271574893823
Iteration: 91
Loss: 0.004756854572452796
Iteration: 92
Loss: 0.0046636945066543724
Iteration: 93
Loss: 0.004576762869524268
Iteration: 94
Loss: 0.004487267522236858
Iteration: 95
Loss: 0.004403100838550391
Iteration: 96
Loss: 0.004320828339610345
Iteration: 97
Loss: 0.004244374397855539
Iteration: 98
Loss: 0.00416403994238816
Iteration: 99
Loss: 0.004112394243025054
Iteration: 100
Loss: 0.004032494906240549
Iteration: 101
Loss: 0.003979623538609116
Iteration: 102
Loss: 0.003992599921706968
Iteration: 103
Loss: 0.003873634928216537
Iteration: 104
Loss: 0.003827034609201245
Iteration: 105
Loss: 0.003773382975337788
Iteration: 106
Loss: 0.003720057379000653
Iteration: 107
Loss: 0.003683250107897971
Iteration: 108
Loss: 0.0036543277062427923
Iteration: 109
Loss: 0.0036176734645731556
Iteration: 110
Loss: 0.0035908763678983236
Iteration: 111
Loss: 0.0035306677544632782
Iteration: 112
Loss: 0.003489579694966475
Iteration: 113
Loss: 0.0034845244312563385
Iteration: 114
Loss: 0.003444648506000447
Iteration: 115
Loss: 0.003406378580854298
Iteration: 116
Loss: 0.0033869615200763713
Iteration: 117
Loss: 0.0033608200774790766
Iteration: 118
Loss: 0.0033367979996957076
Iteration: 119
Loss: 0.0032980544432902183
Iteration: 120
Loss: 0.003290546007263355
Iteration: 121
Loss: 0.0032592881799269565
Iteration: 122
Loss: 0.0032349576910909936
Iteration: 123
Loss: 0.0032203250881045675
Iteration: 124
Loss: 0.003214539524215536
Iteration: 125
Loss: 0.0031801883447676515
Iteration: 126
Loss: 0.0031729155661872565
Iteration: 127
Loss: 0.0031389869982376695
Iteration: 128
Loss: 0.003112728886592847
Iteration: 129
Loss: 0.003105997391498815
Iteration: 130
Loss: 0.0030888242658991846
Iteration: 131
Loss: 0.0030759705935055628
Iteration: 132
Loss: 0.0030504162811363735
Iteration: 133
Loss: 0.0030363080993246958
Iteration: 134
Loss: 0.0030211780447130785
Iteration: 135
Loss: 0.0030033355548930094
Iteration: 136
Loss: 0.0029822716859575263
Iteration: 137
Loss: 0.0029787888111642157
Iteration: 138
Loss: 0.002942516193844569
Iteration: 139
Loss: 0.0029384533438879326
Iteration: 140
Loss: 0.0029158291574089956
Iteration: 141
Loss: 0.002895364158739073
Iteration: 142
Loss: 0.002876414768159007
Iteration: 143
Loss: 0.0028670581971080257
Iteration: 144
Loss: 0.002869016696842244
Iteration: 145
Loss: 0.002834012281173506
Iteration: 146
Loss: 0.0028183437728633485
Iteration: 147
Loss: 0.00279760241914445
Iteration: 148
Loss: 0.0028097360090424237
Iteration: 149
Loss: 0.0027626146777318073
Iteration: 150
Loss: 0.0027455189026510105
Iteration: 151
Loss: 0.0027305561965570236
Iteration: 152
Loss: 0.002715776916235112
Iteration: 153
Loss: 0.0026969453170656776
Iteration: 154
Loss: 0.0026806947590114595
Iteration: 155
Loss: 0.00266228348780901
Iteration: 156
Loss: 0.0026581291533194673
Iteration: 157
Loss: 0.002640071283213985
Iteration: 158
Loss: 0.0026167053460644987
Iteration: 159
Loss: 0.0025822864785694922
Iteration: 160
Loss: 0.0025873060415809355
Iteration: 161
Loss: 0.0025713612737420658
Iteration: 162
Loss: 0.002558228879187925
Iteration: 163
Loss: 0.0025456067897045077
Iteration: 164
Loss: 0.002524874420860448
Iteration: 165
Loss: 0.002513427627631105
Iteration: 166
Loss: 0.002489006594539835
Iteration: 167
Loss: 0.0024816924915051996
Iteration: 168
Loss: 0.002468178245740441
Iteration: 169
Loss: 0.002456289799645161
Iteration: 170
Loss: 0.002444165182681993
Iteration: 171
Loss: 0.002425919698837858
Iteration: 172
Loss: 0.0024305029342380855
Iteration: 173
Loss: 0.002413422236433969
Iteration: 174
Loss: 0.0024021005747505487
Iteration: 175
Loss: 0.0023835245669126892
Iteration: 176
Loss: 0.002405625126229074
Iteration: 177
Loss: 0.002353071393325734
Iteration: 178
Loss: 0.0023555028706024853
Iteration: 179
Loss: 0.002327904696218096
Iteration: 180
Loss: 0.002325967933313969
Iteration: 181
Loss: 0.002320111896365117
Iteration: 182
Loss: 0.0023137662523927596
Iteration: 183
Loss: 0.0023229733312454745
Iteration: 184
Loss: 0.0022823109712380055
Iteration: 185
Loss: 0.0022713542432309343
Iteration: 186
Loss: 0.0023017060190725783
Iteration: 187
Loss: 0.0022647182684009657
Iteration: 188
Loss: 0.002260325249666587
Iteration: 189
Loss: 0.0022697628547365847
Iteration: 190
Loss: 0.0022559078553548227
Iteration: 191
Loss: 0.0022400197399875675
Iteration: 192
Loss: 0.002241202473389701
Iteration: 193
Loss: 0.0022229213604870704
Iteration: 194
Loss: 0.0022141570911909905
Iteration: 195
Loss: 0.0022448474669661852
Iteration: 196
Loss: 0.0022062869452966903
Iteration: 197
Loss: 0.0022078585357238087
Iteration: 198
Loss: 0.002200961690575171
Iteration: 199
Loss: 0.002243408018376869
Iteration: 200
Loss: 0.002186111940178447
Iteration: 201
Loss: 0.002182283561127499
Iteration: 202
Loss: 0.0021850083229275276
Iteration: 203
Loss: 0.002168892487250746
Iteration: 204
Loss: 0.0021691788719274486
Iteration: 205
Loss: 0.0021547485390701927
Iteration: 206
Loss: 0.002158621985775729
Iteration: 207
Loss: 0.002170581106418887
Iteration: 208
Loss: 0.002185800636652857
Iteration: 209
Loss: 0.0021486893320121826
Iteration: 210
Loss: 0.002145918948838535
Iteration: 211
Loss: 0.002147781520855064
Iteration: 212
Loss: 0.0021520879161424744
Iteration: 213
Loss: 0.0021379954727868047
Iteration: 214
Loss: 0.0021382096172788013
Iteration: 215
Loss: 0.00212576889540427
Iteration: 216
Loss: 0.0021339422673918307
Iteration: 217
Loss: 0.002120426930606556
Iteration: 218
Loss: 0.002114951353854476
Iteration: 219
Loss: 0.0021147035444394136
Iteration: 220
Loss: 0.002116911913244388
Iteration: 221
Loss: 0.0021158254597909176
Iteration: 222
Loss: 0.0020993826209376445
Iteration: 223
Loss: 0.0020972256465122486
Iteration: 224
Loss: 0.0021006197322351048
Iteration: 225
Loss: 0.0020937158364969758
Iteration: 226
Loss: 0.002090302100464797
Iteration: 227
Loss: 0.002103797064568752
Iteration: 228
Loss: 0.0020954666706996085
Iteration: 229
Loss: 0.002086534312603852
Iteration: 230
Loss: 0.002104879019614787
Iteration: 231
Loss: 0.002081882242913334
Iteration: 232
Loss: 0.0020819698021603892
Iteration: 233
Loss: 0.00207546898850407
Iteration: 234
Loss: 0.0020750646258537206
Iteration: 235
Loss: 0.002067012337740893
Iteration: 236
Loss: 0.0020731330091038193
Iteration: 237
Loss: 0.0020682806274495446
Iteration: 238
Loss: 0.002073269249357952
Iteration: 239
Loss: 0.0020764613753924957
Iteration: 240
Loss: 0.0020572644813607135
Iteration: 241
Loss: 0.002082267910457001
Iteration: 242
Loss: 0.002052345768345568
Iteration: 243
Loss: 0.0020676688783658813
Iteration: 244
Loss: 0.002075210886780555
Iteration: 245
Loss: 0.0020533553578365497
Iteration: 246
Loss: 0.002058116347493174
Iteration: 247
Loss: 0.0020493219758813772
Iteration: 248
Loss: 0.0020467272783534075
Iteration: 249
Loss: 0.00204999932499889
Iteration: 250
Loss: 0.002046138594428507
Iteration: 251
Loss: 0.002037330345513347
Iteration: 252
Loss: 0.002036061343880227
Iteration: 253
Loss: 0.0020499751165819666
Iteration: 254
Loss: 0.0020368613872844246
Iteration: 255
Loss: 0.0020402812798961233
Iteration: 256
Loss: 0.0020321537397252633
Iteration: 257
Loss: 0.0020378624769644095
Iteration: 258
Loss: 0.0020286198048732984
Iteration: 259
Loss: 0.0020269066280422686
Iteration: 260
Loss: 0.0020290962763679908
Iteration: 261
Loss: 0.002033450149704153
Iteration: 262
Loss: 0.0020213211552263834
Iteration: 263
Loss: 0.0020234958053781437
Iteration: 264
Loss: 0.0020250829820258496
Iteration: 265
Loss: 0.002012503736365873
Iteration: 266
Loss: 0.002020293176592065
Iteration: 267
Loss: 0.002015769774189744
Iteration: 268
Loss: 0.0020307589140243065
Iteration: 269
Loss: 0.002013413989259742
Iteration: 270
Loss: 0.0020127758930007424
Iteration: 271
Loss: 0.0020225400758835557
Iteration: 272
Loss: 0.0020104545968560837
Iteration: 273
Loss: 0.00201258001377424
Iteration: 274
Loss: 0.0020091732701238915
Iteration: 275
Loss: 0.001997636014726968
Iteration: 276
Loss: 0.0020154025898171733
Iteration: 277
Loss: 0.002009019094447677
Iteration: 278
Loss: 0.0020016760577280554
Iteration: 279
Loss: 0.002011160770705782
Iteration: 280
Loss: 0.00200240536688421
Iteration: 281
Loss: 0.0020037221256643534
Iteration: 282
Loss: 0.002015876959866056
Iteration: 283
Loss: 0.0019941691933080363
Iteration: 284
Loss: 0.0019921491665837285
Iteration: 285
Loss: 0.0020333603009557687
Iteration: 286
Loss: 0.0019962150030411207
Iteration: 287
Loss: 0.0019885741490631914
Iteration: 288
Loss: 0.0019942878309685066
Iteration: 289
Loss: 0.0019837574591525854
Iteration: 290
Loss: 0.001996604785418663
Iteration: 291
Loss: 0.0019856903519935142
Iteration: 292
Loss: 0.0019817115374816917
Iteration: 293
Loss: 0.001995236448680934
Iteration: 294
Loss: 0.0019895350859046746
Iteration: 295
Loss: 0.0019951193229271434
Iteration: 296
Loss: 0.0019777902067662813
Iteration: 297
Loss: 0.002022587985266
Iteration: 298
Loss: 0.001973869871551123
Iteration: 299
Loss: 0.001992953486012247
Iteration: 300
Loss: 0.0019837542368361773
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.31837916063675836
accuracy: 0.9664171122994653
confusion: 220 601 341 26888
precision: 0.2679658952496955
recall: 0.39215686274509803
Processing fold: ../../../kg_constructor/output/ecoli/folds/fold_3
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 26185
Starting training...
Iteration: 1
Loss: 0.7031625769077203
Iteration: 2
Loss: 0.6957852641741434
Iteration: 3
Loss: 0.6770456425654583
Iteration: 4
Loss: 0.6372118271314181
Iteration: 5
Loss: 0.5776529671289982
Iteration: 6
Loss: 0.5127424937792313
Iteration: 7
Loss: 0.4517645312425418
Iteration: 8
Loss: 0.40514819476848996
Iteration: 9
Loss: 0.3702945583141767
Iteration: 10
Loss: 0.3444778942144834
Iteration: 11
Loss: 0.32364209454793197
Iteration: 12
Loss: 0.3059038569529851
Iteration: 13
Loss: 0.29165564935940963
Iteration: 14
Loss: 0.2791758764248628
Iteration: 15
Loss: 0.266302805298414
Iteration: 16
Loss: 0.2551740702146139
Iteration: 17
Loss: 0.24415002992519966
Iteration: 18
Loss: 0.23308206980045026
Iteration: 19
Loss: 0.222346893678873
Iteration: 20
Loss: 0.21213602351072508
Iteration: 21
Loss: 0.20137394888278765
Iteration: 22
Loss: 0.19101715813844633
Iteration: 23
Loss: 0.18058593972371176
Iteration: 24
Loss: 0.17062034706274667
Iteration: 25
Loss: 0.15989573911214486
Iteration: 26
Loss: 0.149557419694387
Iteration: 27
Loss: 0.13959361421756256
Iteration: 28
Loss: 0.1304079346740857
Iteration: 29
Loss: 0.12114373594522476
Iteration: 30
Loss: 0.11309382749291566
Iteration: 31
Loss: 0.10438667056270134
Iteration: 32
Loss: 0.09672504204970139
Iteration: 33
Loss: 0.08926343019956197
Iteration: 34
Loss: 0.0831824304201664
Iteration: 35
Loss: 0.07623246445869789
Iteration: 36
Loss: 0.07049806788563728
Iteration: 37
Loss: 0.06547969149855468
Iteration: 38
Loss: 0.06031937204683439
Iteration: 39
Loss: 0.05598215233439054
Iteration: 40
Loss: 0.05140610903692551
Iteration: 41
Loss: 0.0480832191518484
Iteration: 42
Loss: 0.04415101772890641
Iteration: 43
Loss: 0.04110748884387505
Iteration: 44
Loss: 0.03833848429031861
Iteration: 45
Loss: 0.035328193448292904
Iteration: 46
Loss: 0.03298525731915083
Iteration: 47
Loss: 0.030754661354690026
Iteration: 48
Loss: 0.028588261407537337
Iteration: 49
Loss: 0.026719648868609697
Iteration: 50
Loss: 0.025104989082767412
Iteration: 51
Loss: 0.023514482765816726
Iteration: 52
Loss: 0.022147078019304153
Iteration: 53
Loss: 0.020758957124482363
Iteration: 54
Loss: 0.019471007494781263
Iteration: 55
Loss: 0.01846415458772427
Iteration: 56
Loss: 0.017423920428905733
Iteration: 57
Loss: 0.016493291689608343
Iteration: 58
Loss: 0.015563039276271295
Iteration: 59
Loss: 0.014672755860747436
Iteration: 60
Loss: 0.013926793068933945
Iteration: 61
Loss: 0.01339076739998582
Iteration: 62
Loss: 0.012642587475383129
Iteration: 63
Loss: 0.011948114284911217
Iteration: 64
Loss: 0.011470515292902023
Iteration: 65
Loss: 0.010973933009574046
Iteration: 66
Loss: 0.010456847946326703
Iteration: 67
Loss: 0.010094084908278326
Iteration: 68
Loss: 0.009625849911035636
Iteration: 69
Loss: 0.009227380908739109
Iteration: 70
Loss: 0.008879938365843816
Iteration: 71
Loss: 0.00853488951897583
Iteration: 72
Loss: 0.008174246165137261
Iteration: 73
Loss: 0.007928014995578008
Iteration: 74
Loss: 0.007584016477593627
Iteration: 75
Loss: 0.007318801210763363
Iteration: 76
Loss: 0.007114842940026369
Iteration: 77
Loss: 0.006880286454151456
Iteration: 78
Loss: 0.006635739354607768
Iteration: 79
Loss: 0.006473894344451718
Iteration: 80
Loss: 0.006263893354349794
Iteration: 81
Loss: 0.006080863657049262
Iteration: 82
Loss: 0.005915575224953966
Iteration: 83
Loss: 0.00570947506154577
Iteration: 84
Loss: 0.005569455518315618
Iteration: 85
Loss: 0.005466132502382
Iteration: 86
Loss: 0.005294281810235519
Iteration: 87
Loss: 0.005188351646304512
Iteration: 88
Loss: 0.005076878656370518
Iteration: 89
Loss: 0.004943308695100057
Iteration: 90
Loss: 0.004844760880447351
Iteration: 91
Loss: 0.004733015639850726
Iteration: 92
Loss: 0.004617186603494561
Iteration: 93
Loss: 0.004558258492929431
Iteration: 94
Loss: 0.004472514309992011
Iteration: 95
Loss: 0.004347816214132576
Iteration: 96
Loss: 0.0042924732686235355
Iteration: 97
Loss: 0.0042220624283147165
Iteration: 98
Loss: 0.004145674013461058
Iteration: 99
Loss: 0.0040829186566556114
Iteration: 100
Loss: 0.004019529890054121
Iteration: 101
Loss: 0.004007523022711468
Iteration: 102
Loss: 0.003911288159612853
Iteration: 103
Loss: 0.003854523499448521
Iteration: 104
Loss: 0.003804942184032347
Iteration: 105
Loss: 0.0037672041754166666
Iteration: 106
Loss: 0.0037176762050829637
Iteration: 107
Loss: 0.0036803047972707413
Iteration: 108
Loss: 0.003651599975852057
Iteration: 109
Loss: 0.0036061968093212596
Iteration: 110
Loss: 0.00355824994114347
Iteration: 111
Loss: 0.003514368860170436
Iteration: 112
Loss: 0.003498714357518997
Iteration: 113
Loss: 0.003461183290570401
Iteration: 114
Loss: 0.0034345124000444626
Iteration: 115
Loss: 0.0034064355437667705
Iteration: 116
Loss: 0.0033913775591943893
Iteration: 117
Loss: 0.003349642666319433
Iteration: 118
Loss: 0.0033143792832747866
Iteration: 119
Loss: 0.003295873524621129
Iteration: 120
Loss: 0.0032791958626311943
Iteration: 121
Loss: 0.0032526228117207303
Iteration: 122
Loss: 0.003229230197552496
Iteration: 123
Loss: 0.003217864092081212
Iteration: 124
Loss: 0.003195391511186384
Iteration: 125
Loss: 0.003186342954181899
Iteration: 126
Loss: 0.0031566597813835894
Iteration: 127
Loss: 0.0031353075337858917
Iteration: 128
Loss: 0.0031219395265604057
Iteration: 129
Loss: 0.0031145171781715294
Iteration: 130
Loss: 0.003081184892485348
Iteration: 131
Loss: 0.0030664753389711944
Iteration: 132
Loss: 0.0030446254069176623
Iteration: 133
Loss: 0.0030295976962989722
Iteration: 134
Loss: 0.0030171119172173813
Iteration: 135
Loss: 0.002996550165474988
Iteration: 136
Loss: 0.002974836140847168
Iteration: 137
Loss: 0.0029667273569756593
Iteration: 138
Loss: 0.002941988605576066
Iteration: 139
Loss: 0.002927543831248887
Iteration: 140
Loss: 0.002905414735253614
Iteration: 141
Loss: 0.002891599785727568
Iteration: 142
Loss: 0.002868377197629366
Iteration: 143
Loss: 0.002897523071927329
Iteration: 144
Loss: 0.002845053605806942
Iteration: 145
Loss: 0.002825525329591563
Iteration: 146
Loss: 0.0028143559630291583
Iteration: 147
Loss: 0.0027868468165158844
Iteration: 148
Loss: 0.002769809750577387
Iteration: 149
Loss: 0.0027593110885041263
Iteration: 150
Loss: 0.002745411722944715
Iteration: 151
Loss: 0.0027313443551508663
Iteration: 152
Loss: 0.0026997534814887703
Iteration: 153
Loss: 0.002700284998028133
Iteration: 154
Loss: 0.00267837345540428
Iteration: 155
Loss: 0.0026605541232782295
Iteration: 156
Loss: 0.002636283143566778
Iteration: 157
Loss: 0.002633610289567747
Iteration: 158
Loss: 0.002633797333169824
Iteration: 159
Loss: 0.002610973440683805
Iteration: 160
Loss: 0.0025747666786162136
Iteration: 161
Loss: 0.002586190564892231
Iteration: 162
Loss: 0.0025467436420373046
Iteration: 163
Loss: 0.0025425802331226757
Iteration: 164
Loss: 0.0025247657635750678
Iteration: 165
Loss: 0.002519501274666534
Iteration: 166
Loss: 0.0025043025738201463
Iteration: 167
Loss: 0.002492717762764257
Iteration: 168
Loss: 0.0024823562080900255
Iteration: 169
Loss: 0.0024660709791649612
Iteration: 170
Loss: 0.0024455943210528065
Iteration: 171
Loss: 0.0024235224864708306
Iteration: 172
Loss: 0.0024142724443943454
Iteration: 173
Loss: 0.002412260826438283
Iteration: 174
Loss: 0.002397835463619767
Iteration: 175
Loss: 0.0023884859073381773
Iteration: 176
Loss: 0.002369854673862648
Iteration: 177
Loss: 0.0023615866099508144
Iteration: 178
Loss: 0.0023430249749276885
Iteration: 179
Loss: 0.00234805466607213
Iteration: 180
Loss: 0.002326420768020818
Iteration: 181
Loss: 0.002326046901707275
Iteration: 182
Loss: 0.0023123844592378307
Iteration: 183
Loss: 0.002304032674799554
Iteration: 184
Loss: 0.002294700613651329
Iteration: 185
Loss: 0.0022918152807949064
Iteration: 186
Loss: 0.002297714089927001
Iteration: 187
Loss: 0.002269120952592064
Iteration: 188
Loss: 0.0022611837902774988
Iteration: 189
Loss: 0.002253810901982853
Iteration: 190
Loss: 0.0022448111737433532
Iteration: 191
Loss: 0.002245905598661361
Iteration: 192
Loss: 0.0022337612403461183
Iteration: 193
Loss: 0.0022587641515434743
Iteration: 194
Loss: 0.002227383952301282
Iteration: 195
Loss: 0.0022188080803085216
Iteration: 196
Loss: 0.002214033650652243
Iteration: 197
Loss: 0.0021949853443612275
Iteration: 198
Loss: 0.0022035295424314262
Iteration: 199
Loss: 0.0022046127377483896
Iteration: 200
Loss: 0.0021864221467135046
Iteration: 201
Loss: 0.002187283169358778
Iteration: 202
Loss: 0.0021957600629554153
Iteration: 203
Loss: 0.002167021290052873
Iteration: 204
Loss: 0.002169801589423934
Iteration: 205
Loss: 0.0021780109621035173
Iteration: 206
Loss: 0.002163950756836969
Iteration: 207
Loss: 0.002160323792710327
Iteration: 208
Loss: 0.00215956505585032
Iteration: 209
Loss: 0.0021696262783967913
Iteration: 210
Loss: 0.002137012805085247
Iteration: 211
Loss: 0.0021354398012567214
Iteration: 212
Loss: 0.002147927486266081
Iteration: 213
Loss: 0.0021513337194800186
Iteration: 214
Loss: 0.002134044908393079
Iteration: 215
Loss: 0.002137610045536302
Iteration: 216
Loss: 0.002148103109226586
Iteration: 217
Loss: 0.0021372432596623325
Iteration: 218
Loss: 0.0021154140241635153
Iteration: 219
Loss: 0.002116655754761245
Iteration: 220
Loss: 0.0021041505835735453
Iteration: 221
Loss: 0.002107044737129353
Iteration: 222
Loss: 0.00211224847067243
Iteration: 223
Loss: 0.0021002065264978088
Iteration: 224
Loss: 0.002097599021791934
Iteration: 225
Loss: 0.0020922552689742776
Iteration: 226
Loss: 0.0020945571252526953
Iteration: 227
Loss: 0.0020908795159835466
Iteration: 228
Loss: 0.0020857378168819616
Iteration: 229
Loss: 0.0020994610445669447
Iteration: 230
Loss: 0.0020893300265575256
Iteration: 231
Loss: 0.002071232473752342
Iteration: 232
Loss: 0.002080482265088134
Iteration: 233
Loss: 0.002066343427241708
Iteration: 234
Loss: 0.002070437763065386
Iteration: 235
Loss: 0.0020729260614667185
Iteration: 236
Loss: 0.00207132797759886
Iteration: 237
Loss: 0.0020801305087307133
Iteration: 238
Loss: 0.0020749932393813743
Iteration: 239
Loss: 0.0020556199895695616
Iteration: 240
Loss: 0.0020556175463403072
Iteration: 241
Loss: 0.0020798579073296143
Iteration: 242
Loss: 0.0020531872302317657
Iteration: 243
Loss: 0.002048991645913189
Iteration: 244
Loss: 0.00205734426018376
Iteration: 245
Loss: 0.002051055785089445
Iteration: 246
Loss: 0.0020407264311917317
Iteration: 247
Loss: 0.002049877515767152
Iteration: 248
Loss: 0.0020610009758876492
Iteration: 249
Loss: 0.0020377162891702773
Iteration: 250
Loss: 0.0020461816785451122
Iteration: 251
Loss: 0.0020489691434284816
Iteration: 252
Loss: 0.0020788695830374192
Iteration: 253
Loss: 0.002036655611263063
Iteration: 254
Loss: 0.0020321703841312765
Iteration: 255
Loss: 0.0020296824319909015
Iteration: 256
Loss: 0.0020268838867569007
Iteration: 257
Loss: 0.0020278664708782276
Iteration: 258
Loss: 0.0020273622104492136
Iteration: 259
Loss: 0.00203428223866444
Iteration: 260
Loss: 0.0020194244839680884
Iteration: 261
Loss: 0.0020272611873224378
Iteration: 262
Loss: 0.0020278689096299694
Iteration: 263
Loss: 0.0020153490148890666
Iteration: 264
Loss: 0.0020159126754897908
Iteration: 265
Loss: 0.00201010078830549
Iteration: 266
Loss: 0.0020166693556791125
Iteration: 267
Loss: 0.002021416114308895
Iteration: 268
Loss: 0.0020026081638673367
Iteration: 269
Loss: 0.002007263055883157
Iteration: 270
Loss: 0.00204015540657565
Iteration: 271
Loss: 0.00202425851379163
Iteration: 272
Loss: 0.002008522250271665
Iteration: 273
Loss: 0.0020047469580999743
Iteration: 274
Loss: 0.002004985046835664
Iteration: 275
Loss: 0.0020023541217550444
Iteration: 276
Loss: 0.001999210024503275
Iteration: 277
Loss: 0.001999897873387314
Iteration: 278
Loss: 0.0020059384883214268
Iteration: 279
Loss: 0.002010437792752129
Iteration: 280
Loss: 0.0020090584189463886
Iteration: 281
Loss: 0.0020276690812375494
Iteration: 282
Loss: 0.002001152202212371
Iteration: 283
Loss: 0.00199231226892712
Iteration: 284
Loss: 0.0019898189766069828
Iteration: 285
Loss: 0.0020025499353113654
Iteration: 286
Loss: 0.001998174249027402
Iteration: 287
Loss: 0.0019813185387577573
Iteration: 288
Loss: 0.0020018323811177067
Iteration: 289
Loss: 0.0019838496585949683
Iteration: 290
Loss: 0.001986832037054671
Iteration: 291
Loss: 0.0019839532846489395
Iteration: 292
Loss: 0.001987140240649191
Iteration: 293
Loss: 0.001981955509668646
Iteration: 294
Loss: 0.0019815243013465824
Iteration: 295
Loss: 0.0019860995115521243
Iteration: 296
Loss: 0.0019840207965805745
Iteration: 297
Loss: 0.001985037582926452
Iteration: 298
Loss: 0.001983377618345026
Iteration: 299
Loss: 0.00198520267328534
Iteration: 300
Loss: 0.0019837445325743505
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.3193116634799235
accuracy: 0.9745714285714285
confusion: 167 319 393 27121
precision: 0.3436213991769547
recall: 0.2982142857142857
Processing fold: ../../../kg_constructor/output/ecoli/folds/fold_4
Doing training and validation
Loading train data...
Loading val data...
Training the TuckER model...
Number of training data points: 26185
Starting training...
Iteration: 1
Loss: 0.703108073809208
Iteration: 2
Loss: 0.6960483009998615
Iteration: 3
Loss: 0.6773367722829183
Iteration: 4
Loss: 0.6381097680483109
Iteration: 5
Loss: 0.5781086301192259
Iteration: 6
Loss: 0.5127587345166084
Iteration: 7
Loss: 0.4535143161431337
Iteration: 8
Loss: 0.4059940886039
Iteration: 9
Loss: 0.37107438766039336
Iteration: 10
Loss: 0.34299370570060533
Iteration: 11
Loss: 0.32326631744702655
Iteration: 12
Loss: 0.3064203147704785
Iteration: 13
Loss: 0.2915914016656386
Iteration: 14
Loss: 0.27904838285385036
Iteration: 15
Loss: 0.2670080547149365
Iteration: 16
Loss: 0.2553826538033975
Iteration: 17
Loss: 0.24484213097737387
Iteration: 18
Loss: 0.23296779833542994
Iteration: 19
Loss: 0.22290403758868194
Iteration: 20
Loss: 0.2119843745842958
Iteration: 21
Loss: 0.2016778120245689
Iteration: 22
Loss: 0.1907397585037427
Iteration: 23
Loss: 0.1803236179626905
Iteration: 24
Loss: 0.17021133253971735
Iteration: 25
Loss: 0.15962396160914347
Iteration: 26
Loss: 0.14994708467752504
Iteration: 27
Loss: 0.1393783837556839
Iteration: 28
Loss: 0.1302407413530044
Iteration: 29
Loss: 0.12141192532502688
Iteration: 30
Loss: 0.11282660573338851
Iteration: 31
Loss: 0.10460223935735531
Iteration: 32
Loss: 0.09667772646897878
Iteration: 33
Loss: 0.08970289486340988
Iteration: 34
Loss: 0.08286843907374603
Iteration: 35
Loss: 0.07671464014893924
Iteration: 36
Loss: 0.0708164533552451
Iteration: 37
Loss: 0.06538421694093789
Iteration: 38
Loss: 0.06023299756149451
Iteration: 39
Loss: 0.0556563150901825
Iteration: 40
Loss: 0.05185224220920832
Iteration: 41
Loss: 0.047839247645475924
Iteration: 42
Loss: 0.04428943709876293
Iteration: 43
Loss: 0.04106033340287514
Iteration: 44
Loss: 0.03799166033665339
Iteration: 45
Loss: 0.03557638093256033
Iteration: 46
Loss: 0.03297561927674673
Iteration: 47
Loss: 0.03075936720825923
Iteration: 48
Loss: 0.028732450822224982
Iteration: 49
Loss: 0.026761837661839448
Iteration: 50
Loss: 0.02509906431898857
Iteration: 51
Loss: 0.023533954810446654
Iteration: 52
Loss: 0.022041141031644285
Iteration: 53
Loss: 0.020745588657565605
Iteration: 54
Loss: 0.019662214992329095
Iteration: 55
Loss: 0.018416850206752617
Iteration: 56
Loss: 0.017338852159296855
Iteration: 57
Loss: 0.01650400336784048
Iteration: 58
Loss: 0.01549892915556064
Iteration: 59
Loss: 0.01467938033434061
Iteration: 60
Loss: 0.01391648700556312
Iteration: 61
Loss: 0.013313658822041292
Iteration: 62
Loss: 0.012669807455192009
Iteration: 63
Loss: 0.012085089352555
Iteration: 64
Loss: 0.011509184439021807
Iteration: 65
Loss: 0.010953986181471592
Iteration: 66
Loss: 0.010490135695689764
Iteration: 67
Loss: 0.009980639072660452
Iteration: 68
Loss: 0.00963230643612452
Iteration: 69
Loss: 0.009231531359732915
Iteration: 70
Loss: 0.008774780196686968
Iteration: 71
Loss: 0.008533794647799088
Iteration: 72
Loss: 0.008188686948508406
Iteration: 73
Loss: 0.007920394950092603
Iteration: 74
Loss: 0.007597669457586912
Iteration: 75
Loss: 0.007340508148981593
Iteration: 76
Loss: 0.007108269330973809
Iteration: 77
Loss: 0.0068903136520813675
Iteration: 78
Loss: 0.006650526924297595
Iteration: 79
Loss: 0.006468605650111268
Iteration: 80
Loss: 0.006212509732741194
Iteration: 81
Loss: 0.0060754915675482685
Iteration: 82
Loss: 0.005909329638458216
Iteration: 83
Loss: 0.005746396897265162
Iteration: 84
Loss: 0.0055622573022563486
Iteration: 85
Loss: 0.005454689509068162
Iteration: 86
Loss: 0.005351303055739174
Iteration: 87
Loss: 0.00518456413052403
Iteration: 88
Loss: 0.005043633998586581
Iteration: 89
Loss: 0.004946931820506087
Iteration: 90
Loss: 0.004829833021339698
Iteration: 91
Loss: 0.004730995272835478
Iteration: 92
Loss: 0.004671964107845456
Iteration: 93
Loss: 0.0045276156220680625
Iteration: 94
Loss: 0.004445493537693834
Iteration: 95
Loss: 0.004406514464137264
Iteration: 96
Loss: 0.004296839186467995
Iteration: 97
Loss: 0.00421853125310288
Iteration: 98
Loss: 0.004142487400736756
Iteration: 99
Loss: 0.004081716638607666
Iteration: 100
Loss: 0.0040172103446168015
Iteration: 101
Loss: 0.003964069523275471
Iteration: 102
Loss: 0.003907893742554081
Iteration: 103
Loss: 0.0038653369933271254
Iteration: 104
Loss: 0.003807525872849883
Iteration: 105
Loss: 0.0037543222976800725
Iteration: 106
Loss: 0.0037422507541636243
Iteration: 107
Loss: 0.0036768941566921198
Iteration: 108
Loss: 0.0036373421108015836
Iteration: 109
Loss: 0.003601993520696385
Iteration: 110
Loss: 0.003553490611151434
Iteration: 111
Loss: 0.0035200067765963003
Iteration: 112
Loss: 0.0035032544834300494
Iteration: 113
Loss: 0.0034613238337139287
Iteration: 114
Loss: 0.0034339552064641165
Iteration: 115
Loss: 0.003405335039879458
Iteration: 116
Loss: 0.0033845118611143567
Iteration: 117
Loss: 0.0033523713966879323
Iteration: 118
Loss: 0.0033399700187146664
Iteration: 119
Loss: 0.003297932419137886
Iteration: 120
Loss: 0.003276155357105801
Iteration: 121
Loss: 0.0032602275321737696
Iteration: 122
Loss: 0.0032378897721616505
Iteration: 123
Loss: 0.0032160835674940012
Iteration: 124
Loss: 0.0031804311841439744
Iteration: 125
Loss: 0.0031665779190520062
Iteration: 126
Loss: 0.003159188859475156
Iteration: 127
Loss: 0.0031425718080777773
Iteration: 128
Loss: 0.0031288144304058873
Iteration: 129
Loss: 0.00310461697443269
Iteration: 130
Loss: 0.003077754888158196
Iteration: 131
Loss: 0.0030714553565933155
Iteration: 132
Loss: 0.003051904403270246
Iteration: 133
Loss: 0.0030322713502802146
Iteration: 134
Loss: 0.0030198887271137955
Iteration: 135
Loss: 0.002995584646645838
Iteration: 136
Loss: 0.002989585896452459
Iteration: 137
Loss: 0.002957771535222538
Iteration: 138
Loss: 0.0029462724088285216
Iteration: 139
Loss: 0.0029269560353639415
Iteration: 140
Loss: 0.002915468237076241
Iteration: 141
Loss: 0.0028994545185317597
Iteration: 142
Loss: 0.002884921611835941
Iteration: 143
Loss: 0.002864183646101409
Iteration: 144
Loss: 0.002856637485540257
Iteration: 145
Loss: 0.002836435552662573
Iteration: 146
Loss: 0.0028154558669298124
Iteration: 147
Loss: 0.0028084469123337515
Iteration: 148
Loss: 0.0027891698723229077
Iteration: 149
Loss: 0.0027723003011674453
Iteration: 150
Loss: 0.0027594533331023576
Iteration: 151
Loss: 0.0027387585335721574
Iteration: 152
Loss: 0.0027342026945776665
Iteration: 153
Loss: 0.002702578538073561
Iteration: 154
Loss: 0.002704625605987624
Iteration: 155
Loss: 0.002679133826556305
Iteration: 156
Loss: 0.0026477982606500005
Iteration: 157
Loss: 0.0026315803287359765
Iteration: 158
Loss: 0.002615932563248162
Iteration: 159
Loss: 0.0026126541794301607
Iteration: 160
Loss: 0.002592020963605207
Iteration: 161
Loss: 0.0025750982025877023
Iteration: 162
Loss: 0.0025516943677734486
Iteration: 163
Loss: 0.00254147888149302
Iteration: 164
Loss: 0.0025412763277880657
Iteration: 165
Loss: 0.002520022122786404
Iteration: 166
Loss: 0.0024971420763251493
Iteration: 167
Loss: 0.0024745573456852864
Iteration: 168
Loss: 0.0024665523087605834
Iteration: 169
Loss: 0.0024559450819371985
Iteration: 170
Loss: 0.002440228679169638
Iteration: 171
Loss: 0.0024317925025780615
Iteration: 172
Loss: 0.0024103940798877142
Iteration: 173
Loss: 0.0023953516591483583
Iteration: 174
Loss: 0.0023822265659244014
Iteration: 175
Loss: 0.0023788050121556106
Iteration: 176
Loss: 0.0023583836005761838
Iteration: 177
Loss: 0.002361673425930815
Iteration: 178
Loss: 0.0023391362494574143
Iteration: 179
Loss: 0.002344727163943343
Iteration: 180
Loss: 0.0023297492552023283
Iteration: 181
Loss: 0.002314612384324368
Iteration: 182
Loss: 0.002309850629013127
Iteration: 183
Loss: 0.0022932592442092034
Iteration: 184
Loss: 0.0022892160162043115
Iteration: 185
Loss: 0.0022706231624723817
Iteration: 186
Loss: 0.002271801531792451
Iteration: 187
Loss: 0.002256063523535163
Iteration: 188
Loss: 0.0022414977847335813
Iteration: 189
Loss: 0.0022581873748164913
Iteration: 190
Loss: 0.0022352631129205036
Iteration: 191
Loss: 0.002236596292231041
Iteration: 192
Loss: 0.0022224099330532435
Iteration: 193
Loss: 0.002218553429254546
Iteration: 194
Loss: 0.0022286225516253556
Iteration: 195
Loss: 0.0022237801369136344
Iteration: 196
Loss: 0.0022034387504204344
Iteration: 197
Loss: 0.0021986239082299364
Iteration: 198
Loss: 0.0021933522985842177
Iteration: 199
Loss: 0.002207798244527135
Iteration: 200
Loss: 0.0021766368822099115
Iteration: 201
Loss: 0.002169278492100346
Iteration: 202
Loss: 0.002158180734393402
Iteration: 203
Loss: 0.0021655559912920953
Iteration: 204
Loss: 0.0021765712985936073
Iteration: 205
Loss: 0.0021470932540698694
Iteration: 206
Loss: 0.002155008645226749
Iteration: 207
Loss: 0.0021475215340988375
Iteration: 208
Loss: 0.0021311481414625468
Iteration: 209
Loss: 0.0021261560912721623
Iteration: 210
Loss: 0.0021459063341936623
Iteration: 211
Loss: 0.002139400052002225
Iteration: 212
Loss: 0.002164763274590652
Iteration: 213
Loss: 0.002118815533601894
Iteration: 214
Loss: 0.0021180277675366364
Iteration: 215
Loss: 0.0021119017022637986
Iteration: 216
Loss: 0.002128580165728449
Iteration: 217
Loss: 0.0021168075036257505
Iteration: 218
Loss: 0.002101975054336855
Iteration: 219
Loss: 0.0020989300802541086
Iteration: 220
Loss: 0.0021026480184772457
Iteration: 221
Loss: 0.0021028700911511597
Iteration: 222
Loss: 0.0020980672472013305
Iteration: 223
Loss: 0.0021210075579941846
Iteration: 224
Loss: 0.002089901788470646
Iteration: 225
Loss: 0.002087195778492456
Iteration: 226
Loss: 0.0020857280782925394
Iteration: 227
Loss: 0.002091220169602774
Iteration: 228
Loss: 0.0020737762798746237
Iteration: 229
Loss: 0.0020778859093880807
Iteration: 230
Loss: 0.0020836944175430406
Iteration: 231
Loss: 0.0020716986977137052
Iteration: 232
Loss: 0.0020816910874623903
Iteration: 233
Loss: 0.002066798714132836
Iteration: 234
Loss: 0.0020724641896473863
Iteration: 235
Loss: 0.0020616957679008827
Iteration: 236
Loss: 0.0020631943718912318
Iteration: 237
Loss: 0.00206731129079484
Iteration: 238
Loss: 0.0020572433728748597
Iteration: 239
Loss: 0.0020523316164214453
Iteration: 240
Loss: 0.002063019305634766
Iteration: 241
Loss: 0.0020578016067305817
Iteration: 242
Loss: 0.002062291994941636
Iteration: 243
Loss: 0.002060270959284539
Iteration: 244
Loss: 0.0020472421922768727
Iteration: 245
Loss: 0.00204218881807696
Iteration: 246
Loss: 0.00204884099553172
Iteration: 247
Loss: 0.002041286920221188
Iteration: 248
Loss: 0.0020670322343133963
Iteration: 249
Loss: 0.0020316643491148567
Iteration: 250
Loss: 0.002057844578909377
Iteration: 251
Loss: 0.0020339073947606943
Iteration: 252
Loss: 0.002035349022406034
Iteration: 253
Loss: 0.002035921841012075
Iteration: 254
Loss: 0.002030425036373811
Iteration: 255
Loss: 0.002039839622552674
Iteration: 256
Loss: 0.002038763698715812
Iteration: 257
Loss: 0.0020275081818302474
Iteration: 258
Loss: 0.00203311397956732
Iteration: 259
Loss: 0.0020288684142705724
Iteration: 260
Loss: 0.0020214190873771142
Iteration: 261
Loss: 0.002022830114210359
Iteration: 262
Loss: 0.0020303834387913155
Iteration: 263
Loss: 0.0020216317139924145
Iteration: 264
Loss: 0.002017974035623364
Iteration: 265
Loss: 0.0020135679306128086
Iteration: 266
Loss: 0.002022401199867137
Iteration: 267
Loss: 0.002016259224500316
Iteration: 268
Loss: 0.0020107513380189165
Iteration: 269
Loss: 0.002003032711143486
Iteration: 270
Loss: 0.0019999778939960287
Iteration: 271
Loss: 0.0020011311026815423
Iteration: 272
Loss: 0.0020120346527666044
Iteration: 273
Loss: 0.0020074880717752073
Iteration: 274
Loss: 0.0019999153118055216
Iteration: 275
Loss: 0.002010976500176371
Iteration: 276
Loss: 0.0019987925442341617
Iteration: 277
Loss: 0.002002612830927739
Iteration: 278
Loss: 0.0020178242523867925
Iteration: 279
Loss: 0.0019939900032626703
Iteration: 280
Loss: 0.002007900479022796
Iteration: 281
Loss: 0.0019846347694953857
Iteration: 282
Loss: 0.002013077311671506
Iteration: 283
Loss: 0.001991171758466711
Iteration: 284
Loss: 0.0019925456756773666
Iteration: 285
Loss: 0.002008400812673454
Iteration: 286
Loss: 0.00200489572643374
Iteration: 287
Loss: 0.002004575958917252
Iteration: 288
Loss: 0.001988353586803453
Iteration: 289
Loss: 0.001984017169795548
Iteration: 290
Loss: 0.0019845174989686944
Iteration: 291
Loss: 0.001984730815120901
Iteration: 292
Loss: 0.0019796928226685105
Iteration: 293
Loss: 0.0019890953523990437
Iteration: 294
Loss: 0.0019810475507536186
Iteration: 295
Loss: 0.0019927800570328077
Iteration: 296
Loss: 0.0019838818265363956
Iteration: 297
Loss: 0.0020086287882011863
Iteration: 298
Loss: 0.0019850883728418597
Iteration: 299
Loss: 0.0019914190052077174
Iteration: 300
Loss: 0.001979331747115327
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.3557168784029038
accuracy: 0.9746428571428571
confusion: 196 346 364 27094
precision: 0.36162361623616235
recall: 0.35
Finding results in directory: ../../output/ecoli/tucker
Found 1 files in the directory.
Hyperparameters search result sorted by f1:
                                     hyperparameters        f1
0  (300, 128, 0.0002, 1.0, 200, 100, 0.2, 0.4, 0....  0.331956
1  (300, 128, 0.0002, 1.0, 200, 30, 0.2, 0.4, 0.5...  0.327286
num_iterations: [300]
batch_size: [128]
learning_rate: [0.0002]
decay_rate: [1.0]
ent_vec_dim: [200]
rel_vec_dim: [30]
input_dropout: [0.2]
hidden_dropout1: [0.4]
hidden_dropout2: [0.5]
label_smoothing: [0.1]
Loading train data...
Loading val data...
Loading test data...
Training the TuckER model...
Number of training data points: 26745
Starting training...
Iteration: 1
Loss: 0.7027366236795353
Iteration: 2
Loss: 0.6949729029136368
Iteration: 3
Loss: 0.6751410161392598
Iteration: 4
Loss: 0.6334177376348761
Iteration: 5
Loss: 0.5730164322672011
Iteration: 6
Loss: 0.5073186079912548
Iteration: 7
Loss: 0.4490674749205384
Iteration: 8
Loss: 0.4026184832747978
Iteration: 9
Loss: 0.36771882787535465
Iteration: 10
Loss: 0.3417397265947318
Iteration: 11
Loss: 0.32124914021431644
Iteration: 12
Loss: 0.3050123106075239
Iteration: 13
Loss: 0.2910230476644975
Iteration: 14
Loss: 0.27773486964310273
Iteration: 15
Loss: 0.26598932282834115
Iteration: 16
Loss: 0.25501935693282113
Iteration: 17
Loss: 0.24332066842272312
Iteration: 18
Loss: 0.23210044639020028
Iteration: 19
Loss: 0.22227616777902917
Iteration: 20
Loss: 0.21105001923404162
Iteration: 21
Loss: 0.20064281398736977
Iteration: 22
Loss: 0.19031885285166245
Iteration: 23
Loss: 0.17994802228257625
Iteration: 24
Loss: 0.1687181882465942
Iteration: 25
Loss: 0.1589683022680162
Iteration: 26
Loss: 0.14855789327168767
Iteration: 27
Loss: 0.13890927841391745
Iteration: 28
Loss: 0.12949474702907515
Iteration: 29
Loss: 0.12044879926156395
Iteration: 30
Loss: 0.11159434075219722
Iteration: 31
Loss: 0.10367500395337238
Iteration: 32
Loss: 0.0956257727893093
Iteration: 33
Loss: 0.08840259211727336
Iteration: 34
Loss: 0.0821265031265307
Iteration: 35
Loss: 0.07559172262119342
Iteration: 36
Loss: 0.06968127643760247
Iteration: 37
Loss: 0.0641686587205416
Iteration: 38
Loss: 0.059632516642914545
Iteration: 39
Loss: 0.05473899978059757
Iteration: 40
Loss: 0.05084212704361239
Iteration: 41
Loss: 0.04690067130553571
Iteration: 42
Loss: 0.04336906903529469
Iteration: 43
Loss: 0.04021216532852076
Iteration: 44
Loss: 0.03737900854100155
Iteration: 45
Loss: 0.03482072838122332
Iteration: 46
Loss: 0.032425463458971134
Iteration: 47
Loss: 0.030050705175233793
Iteration: 48
Loss: 0.028019800192759008
Iteration: 49
Loss: 0.026276904146505308
Iteration: 50
Loss: 0.024604470127179652
Iteration: 51
Loss: 0.02299881662843348
Iteration: 52
Loss: 0.02152526498878304
Iteration: 53
Loss: 0.020292616909063314
Iteration: 54
Loss: 0.01907141353416292
Iteration: 55
Loss: 0.017964909701973578
Iteration: 56
Loss: 0.01704214737290823
Iteration: 57
Loss: 0.01615375659982615
Iteration: 58
Loss: 0.015266816155348398
Iteration: 59
Loss: 0.014388756496430953
Iteration: 60
Loss: 0.013638707703050179
Iteration: 61
Loss: 0.013001848955320406
Iteration: 62
Loss: 0.01236781957736121
Iteration: 63
Loss: 0.011787875685133512
Iteration: 64
Loss: 0.011204039265366294
Iteration: 65
Loss: 0.010704476810708831
Iteration: 66
Loss: 0.010280451477988612
Iteration: 67
Loss: 0.009793677308325525
Iteration: 68
Loss: 0.009379443005198919
Iteration: 69
Loss: 0.009023633101790012
Iteration: 70
Loss: 0.008696418644601031
Iteration: 71
Loss: 0.008318543080476266
Iteration: 72
Loss: 0.008004244464107706
Iteration: 73
Loss: 0.007731434564943178
Iteration: 74
Loss: 0.007448046090953712
Iteration: 75
Loss: 0.007238356249194734
Iteration: 76
Loss: 0.0070047057646362085
Iteration: 77
Loss: 0.006732628716132309
Iteration: 78
Loss: 0.00653325837058357
Iteration: 79
Loss: 0.006339463299211067
Iteration: 80
Loss: 0.006122635966284743
Iteration: 81
Loss: 0.006011662038066719
Iteration: 82
Loss: 0.005787876564302022
Iteration: 83
Loss: 0.005638865117408052
Iteration: 84
Loss: 0.005476483674366263
Iteration: 85
Loss: 0.0053588238226462016
Iteration: 86
Loss: 0.005216542430976524
Iteration: 87
Loss: 0.0051120636642828015
Iteration: 88
Loss: 0.004967698921697049
Iteration: 89
Loss: 0.004864876618302321
Iteration: 90
Loss: 0.004755624057135627
Iteration: 91
Loss: 0.00465240733863055
Iteration: 92
Loss: 0.004544584086471343
Iteration: 93
Loss: 0.0044524528593107866
Iteration: 94
Loss: 0.004368172436834702
Iteration: 95
Loss: 0.004302078410039974
Iteration: 96
Loss: 0.004247124398221509
Iteration: 97
Loss: 0.004169875938633952
Iteration: 98
Loss: 0.004107694577755807
Iteration: 99
Loss: 0.004033150873441673
Iteration: 100
Loss: 0.003967097253078902
Iteration: 101
Loss: 0.003930381950156032
Iteration: 102
Loss: 0.0038623289835839707
Iteration: 103
Loss: 0.003810196802538784
Iteration: 104
Loss: 0.003758325636434027
Iteration: 105
Loss: 0.0037290196176146782
Iteration: 106
Loss: 0.003673205729809743
Iteration: 107
Loss: 0.0036356065221885337
Iteration: 108
Loss: 0.003604355097241417
Iteration: 109
Loss: 0.003567897505421616
Iteration: 110
Loss: 0.0035408069880655674
Iteration: 111
Loss: 0.003498186263008208
Iteration: 112
Loss: 0.003464983929467352
Iteration: 113
Loss: 0.0034320526550040593
Iteration: 114
Loss: 0.0034114159938372387
Iteration: 115
Loss: 0.00338221863914234
Iteration: 116
Loss: 0.0033497055491456124
Iteration: 117
Loss: 0.003328153990279836
Iteration: 118
Loss: 0.003308174954845181
Iteration: 119
Loss: 0.003278140364733489
Iteration: 120
Loss: 0.0032576213331445107
Iteration: 121
Loss: 0.0032293443579839757
Iteration: 122
Loss: 0.00321672068425466
Iteration: 123
Loss: 0.0032005479995491385
Iteration: 124
Loss: 0.0031773980995758047
Iteration: 125
Loss: 0.003150212749414429
Iteration: 126
Loss: 0.003159645732633675
Iteration: 127
Loss: 0.0031234499662407215
Iteration: 128
Loss: 0.0031052623769339126
Iteration: 129
Loss: 0.0030727393194278584
Iteration: 130
Loss: 0.0030630696535440564
Iteration: 131
Loss: 0.003041423989837117
Iteration: 132
Loss: 0.003032116991573874
Iteration: 133
Loss: 0.003006296173398253
Iteration: 134
Loss: 0.0029873003552467387
Iteration: 135
Loss: 0.002979525939643949
Iteration: 136
Loss: 0.0029595508505413427
Iteration: 137
Loss: 0.0029466736625549913
Iteration: 138
Loss: 0.002917897191983235
Iteration: 139
Loss: 0.0029190458428067496
Iteration: 140
Loss: 0.0028885018192467433
Iteration: 141
Loss: 0.002872193117236978
Iteration: 142
Loss: 0.00285261877388045
Iteration: 143
Loss: 0.0028386050540529476
Iteration: 144
Loss: 0.002824240753167792
Iteration: 145
Loss: 0.0027980929169851014
Iteration: 146
Loss: 0.002788390719650101
Iteration: 147
Loss: 0.002756373196415886
Iteration: 148
Loss: 0.0027418796863123963
Iteration: 149
Loss: 0.0027360419648451897
Iteration: 150
Loss: 0.002712446642167206
Iteration: 151
Loss: 0.0026993373456069185
Iteration: 152
Loss: 0.0026790093677707865
Iteration: 153
Loss: 0.0026630862745680387
Iteration: 154
Loss: 0.00264183831212547
Iteration: 155
Loss: 0.002621545211730313
Iteration: 156
Loss: 0.002616492825076927
Iteration: 157
Loss: 0.0026173692615113304
Iteration: 158
Loss: 0.002573957731593636
Iteration: 159
Loss: 0.002563843350338785
Iteration: 160
Loss: 0.0025560483395393138
Iteration: 161
Loss: 0.0025345337948514315
Iteration: 162
Loss: 0.0025172254772078766
Iteration: 163
Loss: 0.002500418529409585
Iteration: 164
Loss: 0.002483833383745219
Iteration: 165
Loss: 0.0024743641555733695
Iteration: 166
Loss: 0.002464480288444629
Iteration: 167
Loss: 0.002446659778751716
Iteration: 168
Loss: 0.0024403068060173264
Iteration: 169
Loss: 0.0024313806037453913
Iteration: 170
Loss: 0.0024251527819028006
Iteration: 171
Loss: 0.0023949729025222457
Iteration: 172
Loss: 0.002406202038585008
Iteration: 173
Loss: 0.002375076527860534
Iteration: 174
Loss: 0.0023554770712139487
Iteration: 175
Loss: 0.002371631391770855
Iteration: 176
Loss: 0.0023457049775302786
Iteration: 177
Loss: 0.002339964803261093
Iteration: 178
Loss: 0.002321995264272901
Iteration: 179
Loss: 0.002320457677227221
Iteration: 180
Loss: 0.002301069380291089
Iteration: 181
Loss: 0.0023010705120248506
Iteration: 182
Loss: 0.002289651545306927
Iteration: 183
Loss: 0.00227965150158145
Iteration: 184
Loss: 0.0022741879183280317
Iteration: 185
Loss: 0.0022769358902108633
Iteration: 186
Loss: 0.0022670266298058478
Iteration: 187
Loss: 0.002246697361085894
Iteration: 188
Loss: 0.0022371803215834535
Iteration: 189
Loss: 0.0022371742724075537
Iteration: 190
Loss: 0.0022229901028655566
Iteration: 191
Loss: 0.0022370392085300592
Iteration: 192
Loss: 0.0022175134457294134
Iteration: 193
Loss: 0.002205978946383052
Iteration: 194
Loss: 0.002209981835647663
Iteration: 195
Loss: 0.002205634250930404
Iteration: 196
Loss: 0.0021958709856542418
Iteration: 197
Loss: 0.0021818893492999898
Iteration: 198
Loss: 0.0021834517902800743
Iteration: 199
Loss: 0.0021815986631886115
Iteration: 200
Loss: 0.0021643431229469707
Iteration: 201
Loss: 0.0021815541041189735
Iteration: 202
Loss: 0.0021629765220104328
Iteration: 203
Loss: 0.0021609043882263802
Iteration: 204
Loss: 0.0021532195844227754
Iteration: 205
Loss: 0.002150297338803169
Iteration: 206
Loss: 0.002143030327485426
Iteration: 207
Loss: 0.0021481912765840563
Iteration: 208
Loss: 0.002147645951849939
Iteration: 209
Loss: 0.0021329926447707075
Iteration: 210
Loss: 0.0021352629483783548
Iteration: 211
Loss: 0.0021248387667951703
Iteration: 212
Loss: 0.002123265743809693
Iteration: 213
Loss: 0.002136195032810203
Iteration: 214
Loss: 0.0021160379187734443
Iteration: 215
Loss: 0.0021093887515672587
Iteration: 216
Loss: 0.0021223281451229806
Iteration: 217
Loss: 0.002114250582548542
Iteration: 218
Loss: 0.0021147773368284106
Iteration: 219
Loss: 0.002098623222577138
Iteration: 220
Loss: 0.0020950738579341315
Iteration: 221
Loss: 0.002094688940250987
Iteration: 222
Loss: 0.002095635569230005
Iteration: 223
Loss: 0.0020948103157496906
Iteration: 224
Loss: 0.002081072949686477
Iteration: 225
Loss: 0.0020928773822710862
Iteration: 226
Loss: 0.002077333860488349
Iteration: 227
Loss: 0.0020837002902444972
Iteration: 228
Loss: 0.0020793059979317876
Iteration: 229
Loss: 0.0020718998437794516
Iteration: 230
Loss: 0.0020815312797557326
Iteration: 231
Loss: 0.002084248263058783
Iteration: 232
Loss: 0.0020743544402096093
Iteration: 233
Loss: 0.00206364961665218
Iteration: 234
Loss: 0.0020709082608028682
Iteration: 235
Loss: 0.0020539756109820137
Iteration: 236
Loss: 0.0020584010731287397
Iteration: 237
Loss: 0.002054933190369342
Iteration: 238
Loss: 0.0020535817941580016
Iteration: 239
Loss: 0.002058847668828278
Iteration: 240
Loss: 0.0020461643550875068
Iteration: 241
Loss: 0.002050157944388876
Iteration: 242
Loss: 0.002051496418499494
Iteration: 243
Loss: 0.002047880714806386
Iteration: 244
Loss: 0.0020404476878716597
Iteration: 245
Loss: 0.002043789840609873
Iteration: 246
Loss: 0.002038056173809816
Iteration: 247
Loss: 0.0020328836583331984
Iteration: 248
Loss: 0.0020333329271643033
Iteration: 249
Loss: 0.0020530144639808357
Iteration: 250
Loss: 0.0020411913533683254
Iteration: 251
Loss: 0.002038107044358231
Iteration: 252
Loss: 0.0020305260093411125
Iteration: 253
Loss: 0.002038831230182248
Iteration: 254
Loss: 0.0020292062117847837
Iteration: 255
Loss: 0.002039225714552346
Iteration: 256
Loss: 0.0020293345353625055
Iteration: 257
Loss: 0.002024160065876815
Iteration: 258
Loss: 0.002024990961816209
Iteration: 259
Loss: 0.002021168672503267
Iteration: 260
Loss: 0.002028377819511615
Iteration: 261
Loss: 0.0020250122628728802
Iteration: 262
Loss: 0.002015712408679007
Iteration: 263
Loss: 0.0020123085351896625
Iteration: 264
Loss: 0.0020115669268548865
Iteration: 265
Loss: 0.002006618418035251
Iteration: 266
Loss: 0.002009492311508784
Iteration: 267
Loss: 0.002011548209039471
Iteration: 268
Loss: 0.002014619989403158
Iteration: 269
Loss: 0.002001930608803147
Iteration: 270
Loss: 0.002011387567684243
Iteration: 271
Loss: 0.002025334410593385
Iteration: 272
Loss: 0.0020005473383714126
Iteration: 273
Loss: 0.0020022594319845113
Iteration: 274
Loss: 0.001999172337848363
Iteration: 275
Loss: 0.0019928903902491813
Iteration: 276
Loss: 0.002009467482625778
Iteration: 277
Loss: 0.0019965636252154465
Iteration: 278
Loss: 0.0019942434059597456
Iteration: 279
Loss: 0.0020043753585601344
Iteration: 280
Loss: 0.0019893102035445126
Iteration: 281
Loss: 0.001989317088258229
Iteration: 282
Loss: 0.0019985996451983346
Iteration: 283
Loss: 0.001990388346470515
Iteration: 284
Loss: 0.0019972614053919723
Iteration: 285
Loss: 0.0019872937767023715
Iteration: 286
Loss: 0.001986787056173139
Iteration: 287
Loss: 0.001978095840438729
Iteration: 288
Loss: 0.001976641281095298
Iteration: 289
Loss: 0.0019931405490924473
Iteration: 290
Loss: 0.0019835112100655707
Iteration: 291
Loss: 0.001975400218610428
Iteration: 292
Loss: 0.0019760950058816543
Iteration: 293
Loss: 0.0019858791433934922
Iteration: 294
Loss: 0.0019856815983812456
Iteration: 295
Loss: 0.001983343082763066
Iteration: 296
Loss: 0.001997567462746667
Iteration: 297
Loss: 0.0019788649124741742
Iteration: 298
Loss: 0.0019834631172751514
Iteration: 299
Loss: 0.0019859351465313494
Iteration: 300
Loss: 0.001977496226377125
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.3719074221867518
accuracy: 0.977546362339515
confusion: 233 319 468 34030
precision: 0.4221014492753623
recall: 0.33238231098430815
Loading train data...
Loading val data...
Loading test data...
Training the TuckER model...
Number of training data points: 26745
Starting training...
Iteration: 1
Loss: 0.7028061709826505
Iteration: 2
Loss: 0.6954801784286017
Iteration: 3
Loss: 0.6756245086464701
Iteration: 4
Loss: 0.6333855579170999
Iteration: 5
Loss: 0.5742746463304833
Iteration: 6
Loss: 0.5071156688883335
Iteration: 7
Loss: 0.4470362768897527
Iteration: 8
Loss: 0.4004233777523041
Iteration: 9
Loss: 0.3665641512297377
Iteration: 10
Loss: 0.34060803621630126
Iteration: 11
Loss: 0.31999916819077506
Iteration: 12
Loss: 0.30297293542306636
Iteration: 13
Loss: 0.2889633389967906
Iteration: 14
Loss: 0.27664891639842265
Iteration: 15
Loss: 0.2644979746281346
Iteration: 16
Loss: 0.25250505307052706
Iteration: 17
Loss: 0.24113451019872592
Iteration: 18
Loss: 0.23043070243129246
Iteration: 19
Loss: 0.21918550642985332
Iteration: 20
Loss: 0.2084033396802371
Iteration: 21
Loss: 0.19837036219578755
Iteration: 22
Loss: 0.18747580975671357
Iteration: 23
Loss: 0.1773285105635848
Iteration: 24
Loss: 0.16656985878944397
Iteration: 25
Loss: 0.15649618453617337
Iteration: 26
Loss: 0.14615075327927554
Iteration: 27
Loss: 0.13709005920947354
Iteration: 28
Loss: 0.1276110667216627
Iteration: 29
Loss: 0.11836007388332222
Iteration: 30
Loss: 0.11051816153752653
Iteration: 31
Loss: 0.10196392832300331
Iteration: 32
Loss: 0.09469681572687777
Iteration: 33
Loss: 0.08765221717236918
Iteration: 34
Loss: 0.08110620807620543
Iteration: 35
Loss: 0.07456707265935367
Iteration: 36
Loss: 0.06895022009369693
Iteration: 37
Loss: 0.0639399823414374
Iteration: 38
Loss: 0.05867161182097242
Iteration: 39
Loss: 0.054284003857947606
Iteration: 40
Loss: 0.05061828500673741
Iteration: 41
Loss: 0.046841886884804014
Iteration: 42
Loss: 0.04317764159810694
Iteration: 43
Loss: 0.04016690790841851
Iteration: 44
Loss: 0.03705576675224908
Iteration: 45
Loss: 0.03465259028009222
Iteration: 46
Loss: 0.032284881188711034
Iteration: 47
Loss: 0.029974081305952013
Iteration: 48
Loss: 0.02791116740318793
Iteration: 49
Loss: 0.026143871224190617
Iteration: 50
Loss: 0.024582715516414822
Iteration: 51
Loss: 0.022937377679008473
Iteration: 52
Loss: 0.02158166590747954
Iteration: 53
Loss: 0.020282453562639937
Iteration: 54
Loss: 0.01903303399021867
Iteration: 55
Loss: 0.018036134776812564
Iteration: 56
Loss: 0.016897450071535532
Iteration: 57
Loss: 0.016042548007791556
Iteration: 58
Loss: 0.01516845169229598
Iteration: 59
Loss: 0.0143567042519586
Iteration: 60
Loss: 0.013607761869796469
Iteration: 61
Loss: 0.012941092500298084
Iteration: 62
Loss: 0.012417451042351844
Iteration: 63
Loss: 0.011755535048963149
Iteration: 64
Loss: 0.011218683098596109
Iteration: 65
Loss: 0.01067359038168871
Iteration: 66
Loss: 0.010235260205366943
Iteration: 67
Loss: 0.009820777321635167
Iteration: 68
Loss: 0.009415197103649756
Iteration: 69
Loss: 0.00898410217199899
Iteration: 70
Loss: 0.008656093473487262
Iteration: 71
Loss: 0.008306428200647802
Iteration: 72
Loss: 0.00803623154547207
Iteration: 73
Loss: 0.007761886567348921
Iteration: 74
Loss: 0.007482789303588716
Iteration: 75
Loss: 0.0071960711922449405
Iteration: 76
Loss: 0.006960291975425391
Iteration: 77
Loss: 0.006708170399305564
Iteration: 78
Loss: 0.006537242497824416
Iteration: 79
Loss: 0.006325061839734075
Iteration: 80
Loss: 0.006130937410259172
Iteration: 81
Loss: 0.005979136958765455
Iteration: 82
Loss: 0.0057789048215350775
Iteration: 83
Loss: 0.005630921507607909
Iteration: 84
Loss: 0.005474703628098285
Iteration: 85
Loss: 0.005352925998453475
Iteration: 86
Loss: 0.005204465582118004
Iteration: 87
Loss: 0.005077735833304969
Iteration: 88
Loss: 0.004947690953370891
Iteration: 89
Loss: 0.004844691203553466
Iteration: 90
Loss: 0.004751634784042835
Iteration: 91
Loss: 0.004627337808944756
Iteration: 92
Loss: 0.004563836439783814
Iteration: 93
Loss: 0.004454525500017253
Iteration: 94
Loss: 0.0043690928458412995
Iteration: 95
Loss: 0.004286311015698917
Iteration: 96
Loss: 0.004212213317639654
Iteration: 97
Loss: 0.004153735010259891
Iteration: 98
Loss: 0.004087703662063879
Iteration: 99
Loss: 0.0040297659668081165
Iteration: 100
Loss: 0.003958800524661812
Iteration: 101
Loss: 0.003904894731137194
Iteration: 102
Loss: 0.003850540824093019
Iteration: 103
Loss: 0.003856498135041587
Iteration: 104
Loss: 0.003748989549076444
Iteration: 105
Loss: 0.00370844010310837
Iteration: 106
Loss: 0.00365999073992613
Iteration: 107
Loss: 0.0036362293854214347
Iteration: 108
Loss: 0.003585233288239452
Iteration: 109
Loss: 0.003555051516741514
Iteration: 110
Loss: 0.003502653001017774
Iteration: 111
Loss: 0.0034771311491774985
Iteration: 112
Loss: 0.0034362926071250365
Iteration: 113
Loss: 0.003408734518680957
Iteration: 114
Loss: 0.0033772224413018814
Iteration: 115
Loss: 0.0033593658929619986
Iteration: 116
Loss: 0.0033391421525208634
Iteration: 117
Loss: 0.0033358782382469764
Iteration: 118
Loss: 0.0032844624918284297
Iteration: 119
Loss: 0.0032536364573089384
Iteration: 120
Loss: 0.0032211092736902118
Iteration: 121
Loss: 0.003227724166162595
Iteration: 122
Loss: 0.003186598260993067
Iteration: 123
Loss: 0.0031609164132394745
Iteration: 124
Loss: 0.0031420665887432007
Iteration: 125
Loss: 0.0031331118897688164
Iteration: 126
Loss: 0.0031073786011790927
Iteration: 127
Loss: 0.0031043996450880284
Iteration: 128
Loss: 0.0030885209075965083
Iteration: 129
Loss: 0.0030526629548897092
Iteration: 130
Loss: 0.003031934387510336
Iteration: 131
Loss: 0.00302159608506798
Iteration: 132
Loss: 0.00299977073799583
Iteration: 133
Loss: 0.002986954737313186
Iteration: 134
Loss: 0.003001567156089446
Iteration: 135
Loss: 0.0029457320011209084
Iteration: 136
Loss: 0.002936291013243078
Iteration: 137
Loss: 0.0029448053322119424
Iteration: 138
Loss: 0.0029164504029941334
Iteration: 139
Loss: 0.0028905266206239975
Iteration: 140
Loss: 0.002887362263554448
Iteration: 141
Loss: 0.002861918233146396
Iteration: 142
Loss: 0.00285234982796202
Iteration: 143
Loss: 0.002818205644364787
Iteration: 144
Loss: 0.0028047726866848107
Iteration: 145
Loss: 0.002803736711156708
Iteration: 146
Loss: 0.0027815619029717734
Iteration: 147
Loss: 0.0027660733896390168
Iteration: 148
Loss: 0.0027539710266680658
Iteration: 149
Loss: 0.0027380213170913577
Iteration: 150
Loss: 0.002712209549838606
Iteration: 151
Loss: 0.0026963473757422424
Iteration: 152
Loss: 0.0026804877863607452
Iteration: 153
Loss: 0.0026690177413151613
Iteration: 154
Loss: 0.0026570349413005613
Iteration: 155
Loss: 0.002629982566908945
Iteration: 156
Loss: 0.002629529917612672
Iteration: 157
Loss: 0.002617692433391945
Iteration: 158
Loss: 0.0025890038048117596
Iteration: 159
Loss: 0.002570694565914477
Iteration: 160
Loss: 0.0025638277801578934
Iteration: 161
Loss: 0.0025465658223374358
Iteration: 162
Loss: 0.002550181269622113
Iteration: 163
Loss: 0.002521382456150236
Iteration: 164
Loss: 0.0024981925299367574
Iteration: 165
Loss: 0.0024972664033168857
Iteration: 166
Loss: 0.0024793113970869706
Iteration: 167
Loss: 0.002458992375862561
Iteration: 168
Loss: 0.002439514848792666
Iteration: 169
Loss: 0.00243476951270824
Iteration: 170
Loss: 0.002432047351133786
Iteration: 171
Loss: 0.0024199195683615496
Iteration: 172
Loss: 0.0024028334242020602
Iteration: 173
Loss: 0.0024035145540403416
Iteration: 174
Loss: 0.002385560567549701
Iteration: 175
Loss: 0.0023725214561659703
Iteration: 176
Loss: 0.002352866597615088
Iteration: 177
Loss: 0.002344427111593983
Iteration: 178
Loss: 0.0023499672253958033
Iteration: 179
Loss: 0.0023301432728531617
Iteration: 180
Loss: 0.0023230621001765696
Iteration: 181
Loss: 0.002302400216672428
Iteration: 182
Loss: 0.002313260413423369
Iteration: 183
Loss: 0.0022870861465417885
Iteration: 184
Loss: 0.002293924173293046
Iteration: 185
Loss: 0.0022934088704494545
Iteration: 186
Loss: 0.002268100779244228
Iteration: 187
Loss: 0.0022635918826784326
Iteration: 188
Loss: 0.0022520372921105802
Iteration: 189
Loss: 0.0022509856653529442
Iteration: 190
Loss: 0.0022487418160219734
Iteration: 191
Loss: 0.002232475019107231
Iteration: 192
Loss: 0.0022383404282640806
Iteration: 193
Loss: 0.0022274483985538726
Iteration: 194
Loss: 0.0022299329977176047
Iteration: 195
Loss: 0.002211251411044711
Iteration: 196
Loss: 0.002199526770111126
Iteration: 197
Loss: 0.0022024934569114373
Iteration: 198
Loss: 0.002188608688420227
Iteration: 199
Loss: 0.0021894917958946544
Iteration: 200
Loss: 0.0021870079924812234
Iteration: 201
Loss: 0.0021797604888347507
Iteration: 202
Loss: 0.0021721215379318292
Iteration: 203
Loss: 0.0021719330968924715
Iteration: 204
Loss: 0.0021641491322860686
Iteration: 205
Loss: 0.0021778749997574316
Iteration: 206
Loss: 0.00216132173715513
Iteration: 207
Loss: 0.002172125862982077
Iteration: 208
Loss: 0.002150053092089918
Iteration: 209
Loss: 0.0021435271703956434
Iteration: 210
Loss: 0.00215366230010279
Iteration: 211
Loss: 0.002140745009248487
Iteration: 212
Loss: 0.0021319097700776366
Iteration: 213
Loss: 0.002161004278466954
Iteration: 214
Loss: 0.0021258123245538225
Iteration: 215
Loss: 0.00213411799779374
Iteration: 216
Loss: 0.002113155055425684
Iteration: 217
Loss: 0.0021067420580671934
Iteration: 218
Loss: 0.0021103029846673525
Iteration: 219
Loss: 0.002105467677222494
Iteration: 220
Loss: 0.0021135178674601867
Iteration: 221
Loss: 0.0021070189261219547
Iteration: 222
Loss: 0.0021072457827439038
Iteration: 223
Loss: 0.002109492266892538
Iteration: 224
Loss: 0.002103773822501019
Iteration: 225
Loss: 0.002094454276434417
Iteration: 226
Loss: 0.0020885859421584047
Iteration: 227
Loss: 0.00209385327275701
Iteration: 228
Loss: 0.0020951599846417202
Iteration: 229
Loss: 0.002097394426435798
Iteration: 230
Loss: 0.0020917763143683534
Iteration: 231
Loss: 0.0020787587044524807
Iteration: 232
Loss: 0.0020692945278214313
Iteration: 233
Loss: 0.002086211639880851
Iteration: 234
Loss: 0.0020733878644230434
Iteration: 235
Loss: 0.00207242753380273
Iteration: 236
Loss: 0.002066146205120449
Iteration: 237
Loss: 0.0020711111417629673
Iteration: 238
Loss: 0.0020551384541595097
Iteration: 239
Loss: 0.0020616866863344477
Iteration: 240
Loss: 0.0020538537757164694
Iteration: 241
Loss: 0.0020488462281189387
Iteration: 242
Loss: 0.0020532489981195784
Iteration: 243
Loss: 0.002051110112674157
Iteration: 244
Loss: 0.002048055099064036
Iteration: 245
Loss: 0.0020596450681008302
Iteration: 246
Loss: 0.0020501936721037837
Iteration: 247
Loss: 0.0020511091592474075
Iteration: 248
Loss: 0.0020533049776796488
Iteration: 249
Loss: 0.002038637851070188
Iteration: 250
Loss: 0.002044521493224215
Iteration: 251
Loss: 0.0020364714709782527
Iteration: 252
Loss: 0.00203637254447971
Iteration: 253
Loss: 0.002038541613912941
Iteration: 254
Loss: 0.002036481789207157
Iteration: 255
Loss: 0.002040028614817258
Iteration: 256
Loss: 0.002044562804453735
Iteration: 257
Loss: 0.002026025588989635
Iteration: 258
Loss: 0.0020313289184476944
Iteration: 259
Loss: 0.0020300718584345488
Iteration: 260
Loss: 0.00202481724952548
Iteration: 261
Loss: 0.00201323041043987
Iteration: 262
Loss: 0.002018033436947514
Iteration: 263
Loss: 0.0020387962628509613
Iteration: 264
Loss: 0.0020165224810119106
Iteration: 265
Loss: 0.0020168257753447262
Iteration: 266
Loss: 0.0020195228884680363
Iteration: 267
Loss: 0.0020187864333532654
Iteration: 268
Loss: 0.0020288512938879903
Iteration: 269
Loss: 0.0020229302470489772
Iteration: 270
Loss: 0.0020307831958384266
Iteration: 271
Loss: 0.002011516980260988
Iteration: 272
Loss: 0.0020122921176820617
Iteration: 273
Loss: 0.0020106159728256207
Iteration: 274
Loss: 0.002026000508119034
Iteration: 275
Loss: 0.0020050095170264758
Iteration: 276
Loss: 0.0020151587978264763
Iteration: 277
Loss: 0.0020114670321934773
Iteration: 278
Loss: 0.0020312612074651296
Iteration: 279
Loss: 0.0020080614360520922
Iteration: 280
Loss: 0.001999511744508732
Iteration: 281
Loss: 0.0019974071676901813
Iteration: 282
Loss: 0.0019922849731048263
Iteration: 283
Loss: 0.0020042936497403287
Iteration: 284
Loss: 0.001994924743577272
Iteration: 285
Loss: 0.001999867897292104
Iteration: 286
Loss: 0.001996632646238785
Iteration: 287
Loss: 0.00198954656938256
Iteration: 288
Loss: 0.00199852968253714
Iteration: 289
Loss: 0.001981879196931384
Iteration: 290
Loss: 0.00198855165162277
Iteration: 291
Loss: 0.0019976560503857425
Iteration: 292
Loss: 0.0019830934426311076
Iteration: 293
Loss: 0.0019883547048968604
Iteration: 294
Loss: 0.0020004504498778077
Iteration: 295
Loss: 0.001991005530404045
Iteration: 296
Loss: 0.0019834450787947146
Iteration: 297
Loss: 0.0019930262778755986
Iteration: 298
Loss: 0.0019860996826422178
Iteration: 299
Loss: 0.0019792136485700176
Iteration: 300
Loss: 0.0019854835773924296
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.38110236220472443
accuracy: 0.9775748930099857
confusion: 242 327 459 34022
precision: 0.4253075571177504
recall: 0.34522111269614836
Loading train data...
Loading val data...
Loading test data...
Training the TuckER model...
Number of training data points: 26745
Starting training...
Iteration: 1
Loss: 0.7027660570567167
Iteration: 2
Loss: 0.6953041319605671
Iteration: 3
Loss: 0.6746229718003092
Iteration: 4
Loss: 0.6325532220586946
Iteration: 5
Loss: 0.5730065809020514
Iteration: 6
Loss: 0.5070450517195689
Iteration: 7
Loss: 0.4475529333458671
Iteration: 8
Loss: 0.3998767835429952
Iteration: 9
Loss: 0.36727064055732533
Iteration: 10
Loss: 0.34055181998240797
Iteration: 11
Loss: 0.3211610683157474
Iteration: 12
Loss: 0.3031165301799774
Iteration: 13
Loss: 0.2893772460991823
Iteration: 14
Loss: 0.27577725052833557
Iteration: 15
Loss: 0.26428451492816585
Iteration: 16
Loss: 0.2532821948392482
Iteration: 17
Loss: 0.24246251884895034
Iteration: 18
Loss: 0.23143790190732932
Iteration: 19
Loss: 0.22070486258856858
Iteration: 20
Loss: 0.20947138474711888
Iteration: 21
Loss: 0.19939974610564076
Iteration: 22
Loss: 0.1888282768711259
Iteration: 23
Loss: 0.17783395964888077
Iteration: 24
Loss: 0.1676980019370212
Iteration: 25
Loss: 0.15773139365866215
Iteration: 26
Loss: 0.14785300119768216
Iteration: 27
Loss: 0.1379986096032058
Iteration: 28
Loss: 0.12861253726708738
Iteration: 29
Loss: 0.119395425991167
Iteration: 30
Loss: 0.11080219596624374
Iteration: 31
Loss: 0.10303596492055096
Iteration: 32
Loss: 0.09506877436290813
Iteration: 33
Loss: 0.08776225828671758
Iteration: 34
Loss: 0.08133378059049196
Iteration: 35
Loss: 0.07509311062248447
Iteration: 36
Loss: 0.06940257794494871
Iteration: 37
Loss: 0.06392211254827584
Iteration: 38
Loss: 0.05928917941224726
Iteration: 39
Loss: 0.05472134256476088
Iteration: 40
Loss: 0.05037090856629082
Iteration: 41
Loss: 0.04660848179195501
Iteration: 42
Loss: 0.043385336151983166
Iteration: 43
Loss: 0.04025249103013473
Iteration: 44
Loss: 0.037109007516616506
Iteration: 45
Loss: 0.034666763830788525
Iteration: 46
Loss: 0.032031659154763706
Iteration: 47
Loss: 0.030186481821008877
Iteration: 48
Loss: 0.02810191664891907
Iteration: 49
Loss: 0.02622500300124476
Iteration: 50
Loss: 0.024651734488485736
Iteration: 51
Loss: 0.02299306848192517
Iteration: 52
Loss: 0.021471146871394748
Iteration: 53
Loss: 0.020274456543258473
Iteration: 54
Loss: 0.019158541543197027
Iteration: 55
Loss: 0.01791627688592748
Iteration: 56
Loss: 0.0169077462080536
Iteration: 57
Loss: 0.01611087747106824
Iteration: 58
Loss: 0.015262516085765784
Iteration: 59
Loss: 0.01442180274502386
Iteration: 60
Loss: 0.013669179481324515
Iteration: 61
Loss: 0.013035140471839452
Iteration: 62
Loss: 0.012356173812965804
Iteration: 63
Loss: 0.011761691951770572
Iteration: 64
Loss: 0.011183793232127835
Iteration: 65
Loss: 0.010688638680060453
Iteration: 66
Loss: 0.010237007861650443
Iteration: 67
Loss: 0.00980577304299119
Iteration: 68
Loss: 0.009436752576428124
Iteration: 69
Loss: 0.008989587661963476
Iteration: 70
Loss: 0.008653962277347528
Iteration: 71
Loss: 0.008331780003595956
Iteration: 72
Loss: 0.00804420550577814
Iteration: 73
Loss: 0.007707121051093445
Iteration: 74
Loss: 0.0074584725360044195
Iteration: 75
Loss: 0.007204613701405027
Iteration: 76
Loss: 0.006938641790677852
Iteration: 77
Loss: 0.006729350348675176
Iteration: 78
Loss: 0.006503699046760043
Iteration: 79
Loss: 0.006352262606820728
Iteration: 80
Loss: 0.006160404810186805
Iteration: 81
Loss: 0.005978387650809711
Iteration: 82
Loss: 0.005771131573032729
Iteration: 83
Loss: 0.005635563779292228
Iteration: 84
Loss: 0.005487152988303311
Iteration: 85
Loss: 0.005363229797752221
Iteration: 86
Loss: 0.005207902857963043
Iteration: 87
Loss: 0.005099790123633191
Iteration: 88
Loss: 0.004975664364527675
Iteration: 89
Loss: 0.004851151363853412
Iteration: 90
Loss: 0.004738327605013228
Iteration: 91
Loss: 0.004652322688481853
Iteration: 92
Loss: 0.004577666273505627
Iteration: 93
Loss: 0.004469614358076566
Iteration: 94
Loss: 0.004388289991766214
Iteration: 95
Loss: 0.004308608180807927
Iteration: 96
Loss: 0.004230444552376866
Iteration: 97
Loss: 0.004168039509036307
Iteration: 98
Loss: 0.00408922757988772
Iteration: 99
Loss: 0.004029589080357853
Iteration: 100
Loss: 0.003973258803657527
Iteration: 101
Loss: 0.003915721096711446
Iteration: 102
Loss: 0.003858956384555071
Iteration: 103
Loss: 0.0038113564406109007
Iteration: 104
Loss: 0.0037687522524236876
Iteration: 105
Loss: 0.0037195927646199735
Iteration: 106
Loss: 0.003672034061171963
Iteration: 107
Loss: 0.003640790466997254
Iteration: 108
Loss: 0.003603185105573716
Iteration: 109
Loss: 0.0035736904497246577
Iteration: 110
Loss: 0.0035255318478079915
Iteration: 111
Loss: 0.0035084572715001015
Iteration: 112
Loss: 0.003470002302216201
Iteration: 113
Loss: 0.0034359026157969163
Iteration: 114
Loss: 0.003412571924302397
Iteration: 115
Loss: 0.003378995726639523
Iteration: 116
Loss: 0.0033448651267946523
Iteration: 117
Loss: 0.0033237374759031626
Iteration: 118
Loss: 0.0032922840924768508
Iteration: 119
Loss: 0.0032720711005592273
Iteration: 120
Loss: 0.0032636605150101685
Iteration: 121
Loss: 0.0032295980137077313
Iteration: 122
Loss: 0.0032255697341140688
Iteration: 123
Loss: 0.003185887122526765
Iteration: 124
Loss: 0.003173037246459081
Iteration: 125
Loss: 0.0031429069452695076
Iteration: 126
Loss: 0.0031366415905377154
Iteration: 127
Loss: 0.0030956532529119074
Iteration: 128
Loss: 0.0030869110369512553
Iteration: 129
Loss: 0.0030621965209470144
Iteration: 130
Loss: 0.00305096350633834
Iteration: 131
Loss: 0.003031984675982142
Iteration: 132
Loss: 0.003015923915030081
Iteration: 133
Loss: 0.0029937636160256363
Iteration: 134
Loss: 0.002976501163071658
Iteration: 135
Loss: 0.0029589985762545957
Iteration: 136
Loss: 0.002945059595063706
Iteration: 137
Loss: 0.0029248000871201483
Iteration: 138
Loss: 0.0029067752493804767
Iteration: 139
Loss: 0.0028924664682885514
Iteration: 140
Loss: 0.002856514228012743
Iteration: 141
Loss: 0.0028661877665480105
Iteration: 142
Loss: 0.002864676881345767
Iteration: 143
Loss: 0.0028255360961384787
Iteration: 144
Loss: 0.002797862765367461
Iteration: 145
Loss: 0.002797984571160772
Iteration: 146
Loss: 0.0027764617418280886
Iteration: 147
Loss: 0.00274894751690894
Iteration: 148
Loss: 0.0027322936714685793
Iteration: 149
Loss: 0.0027186721504394765
Iteration: 150
Loss: 0.0027055952733359007
Iteration: 151
Loss: 0.0026906892462763227
Iteration: 152
Loss: 0.0026828886033330537
Iteration: 153
Loss: 0.002657278261112073
Iteration: 154
Loss: 0.0026277661282286236
Iteration: 155
Loss: 0.002619216940660454
Iteration: 156
Loss: 0.002590508474319985
Iteration: 157
Loss: 0.0025821281272823673
Iteration: 158
Loss: 0.002574621461025333
Iteration: 159
Loss: 0.0025526232887766783
Iteration: 160
Loss: 0.0025390572875408053
Iteration: 161
Loss: 0.002519991888393518
Iteration: 162
Loss: 0.0025148815298570862
Iteration: 163
Loss: 0.002491326315941501
Iteration: 164
Loss: 0.002479627858140046
Iteration: 165
Loss: 0.0024724059702852103
Iteration: 166
Loss: 0.0024655277434076314
Iteration: 167
Loss: 0.002444140857698608
Iteration: 168
Loss: 0.002433493085016933
Iteration: 169
Loss: 0.00241763182161258
Iteration: 170
Loss: 0.0024384647884842336
Iteration: 171
Loss: 0.002410966566987807
Iteration: 172
Loss: 0.0024157721413700264
Iteration: 173
Loss: 0.002377128947667683
Iteration: 174
Loss: 0.0023640108860651906
Iteration: 175
Loss: 0.0023543027088140387
Iteration: 176
Loss: 0.0023486230587233094
Iteration: 177
Loss: 0.0023354807856810996
Iteration: 178
Loss: 0.0023400590024134026
Iteration: 179
Loss: 0.002328262785213846
Iteration: 180
Loss: 0.0023156654567139437
Iteration: 181
Loss: 0.002299233134894903
Iteration: 182
Loss: 0.0023094230932714063
Iteration: 183
Loss: 0.00229814504803736
Iteration: 184
Loss: 0.002282425765643674
Iteration: 185
Loss: 0.002269524283695353
Iteration: 186
Loss: 0.002266693684932646
Iteration: 187
Loss: 0.0022588874879469977
Iteration: 188
Loss: 0.0022485460585006806
Iteration: 189
Loss: 0.002238704408872637
Iteration: 190
Loss: 0.002238926490992779
Iteration: 191
Loss: 0.002239423581469757
Iteration: 192
Loss: 0.0022388599176384224
Iteration: 193
Loss: 0.0022218596172530816
Iteration: 194
Loss: 0.002221828946973421
Iteration: 195
Loss: 0.0022106746659625934
Iteration: 196
Loss: 0.002201326331418432
Iteration: 197
Loss: 0.00223545988732831
Iteration: 198
Loss: 0.0021884788396544282
Iteration: 199
Loss: 0.002190022103052256
Iteration: 200
Loss: 0.002189045166302048
Iteration: 201
Loss: 0.002190849036449873
Iteration: 202
Loss: 0.002182809709677402
Iteration: 203
Loss: 0.0021619550615572664
Iteration: 204
Loss: 0.0021623964981423525
Iteration: 205
Loss: 0.002157896892698128
Iteration: 206
Loss: 0.0021575966575831363
Iteration: 207
Loss: 0.0021485627799801813
Iteration: 208
Loss: 0.0021411016085361944
Iteration: 209
Loss: 0.0021566719117373984
Iteration: 210
Loss: 0.002145298902546586
Iteration: 211
Loss: 0.002139384958515816
Iteration: 212
Loss: 0.0021227453908989136
Iteration: 213
Loss: 0.0021367892652419925
Iteration: 214
Loss: 0.002131672103040486
Iteration: 215
Loss: 0.0021175602863578104
Iteration: 216
Loss: 0.002124177885854829
Iteration: 217
Loss: 0.0021104323751399224
Iteration: 218
Loss: 0.0021123282576003405
Iteration: 219
Loss: 0.0021124821645502425
Iteration: 220
Loss: 0.0021056901012205436
Iteration: 221
Loss: 0.0020884825373540007
Iteration: 222
Loss: 0.002093675314991063
Iteration: 223
Loss: 0.002085384951102771
Iteration: 224
Loss: 0.00208925669040248
Iteration: 225
Loss: 0.0020883433635735626
Iteration: 226
Loss: 0.0020966793431110584
Iteration: 227
Loss: 0.00208331114919031
Iteration: 228
Loss: 0.00208173271068195
Iteration: 229
Loss: 0.0020829373764322152
Iteration: 230
Loss: 0.0020786182368409973
Iteration: 231
Loss: 0.002070658200691584
Iteration: 232
Loss: 0.0020758698877135786
Iteration: 233
Loss: 0.0020687808430034527
Iteration: 234
Loss: 0.002074527202020812
Iteration: 235
Loss: 0.0020577291680995047
Iteration: 236
Loss: 0.0020588984332766524
Iteration: 237
Loss: 0.0020693456503574417
Iteration: 238
Loss: 0.0020696082553248616
Iteration: 239
Loss: 0.002049771035856367
Iteration: 240
Loss: 0.0020488412959406834
Iteration: 241
Loss: 0.002066825780298419
Iteration: 242
Loss: 0.002045247448855846
Iteration: 243
Loss: 0.002055000463689241
Iteration: 244
Loss: 0.002039109306598577
Iteration: 245
Loss: 0.0020528996946832423
Iteration: 246
Loss: 0.002052059570987579
Iteration: 247
Loss: 0.0020409224634471384
Iteration: 248
Loss: 0.0020440360604891484
Iteration: 249
Loss: 0.0020388342687695086
Iteration: 250
Loss: 0.0020444891135550187
Iteration: 251
Loss: 0.0020324652439832123
Iteration: 252
Loss: 0.002038655544725494
Iteration: 253
Loss: 0.0020421312391003476
Iteration: 254
Loss: 0.0020322519711136254
Iteration: 255
Loss: 0.002026188156355314
Iteration: 256
Loss: 0.002025695040209007
Iteration: 257
Loss: 0.0020214671186536927
Iteration: 258
Loss: 0.0020177551365422107
Iteration: 259
Loss: 0.0020214335291493164
Iteration: 260
Loss: 0.002025314432839993
Iteration: 261
Loss: 0.0020346582095032628
Iteration: 262
Loss: 0.0020147826422383134
Iteration: 263
Loss: 0.002022682725970598
Iteration: 264
Loss: 0.002023762346929104
Iteration: 265
Loss: 0.0020474202922085606
Iteration: 266
Loss: 0.0020223774098425728
Iteration: 267
Loss: 0.0020097043715959673
Iteration: 268
Loss: 0.002010189316565572
Iteration: 269
Loss: 0.0020134568102422017
Iteration: 270
Loss: 0.0020109368458720326
Iteration: 271
Loss: 0.0020004395142056144
Iteration: 272
Loss: 0.0020033497955720824
Iteration: 273
Loss: 0.0020020469607596723
Iteration: 274
Loss: 0.0020094815850894474
Iteration: 275
Loss: 0.0020116519527745586
Iteration: 276
Loss: 0.002000860168445337
Iteration: 277
Loss: 0.001995350795656537
Iteration: 278
Loss: 0.0020028383432757818
Iteration: 279
Loss: 0.0020058354351056525
Iteration: 280
Loss: 0.0020023800220481955
Iteration: 281
Loss: 0.0020019343635656788
Iteration: 282
Loss: 0.001984336802476569
Iteration: 283
Loss: 0.001993342672616149
Iteration: 284
Loss: 0.0019795360468045065
Iteration: 285
Loss: 0.00199453346136533
Iteration: 286
Loss: 0.0020058397704711822
Iteration: 287
Loss: 0.0019843338478851735
Iteration: 288
Loss: 0.0019814652505719776
Iteration: 289
Loss: 0.0019877637379885284
Iteration: 290
Loss: 0.001985503835132039
Iteration: 291
Loss: 0.001978250076004033
Iteration: 292
Loss: 0.001988535148645692
Iteration: 293
Loss: 0.001994843893873069
Iteration: 294
Loss: 0.001998008387988504
Iteration: 295
Loss: 0.001990308948273806
Iteration: 296
Loss: 0.0019792327672078073
Iteration: 297
Loss: 0.001981010944936189
Iteration: 298
Loss: 0.001977695442465004
Iteration: 299
Loss: 0.0019783933788443666
Iteration: 300
Loss: 0.0019714544391660374
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.37745098039215685
accuracy: 0.9782596291012838
confusion: 231 292 470 34057
precision: 0.4416826003824092
recall: 0.32952924393723254
Loading train data...
Loading val data...
Loading test data...
Training the TuckER model...
Number of training data points: 26745
Starting training...
Iteration: 1
Loss: 0.7028743820854381
Iteration: 2
Loss: 0.6949552478669565
Iteration: 3
Loss: 0.6743881355358076
Iteration: 4
Loss: 0.6311876555032367
Iteration: 5
Loss: 0.5710701851905147
Iteration: 6
Loss: 0.5046241109884237
Iteration: 7
Loss: 0.44611643311343613
Iteration: 8
Loss: 0.4021486602252043
Iteration: 9
Loss: 0.36534058217760884
Iteration: 10
Loss: 0.3406177381925945
Iteration: 11
Loss: 0.32000262752363956
Iteration: 12
Loss: 0.30430279992803744
Iteration: 13
Loss: 0.28934374564810644
Iteration: 14
Loss: 0.2767940646485437
Iteration: 15
Loss: 0.26516824998433075
Iteration: 16
Loss: 0.25344103631339493
Iteration: 17
Loss: 0.24239254356185091
Iteration: 18
Loss: 0.23131830122651934
Iteration: 19
Loss: 0.22059451345401473
Iteration: 20
Loss: 0.210110519317132
Iteration: 21
Loss: 0.19934700900995278
Iteration: 22
Loss: 0.1889828701939764
Iteration: 23
Loss: 0.17817262909080409
Iteration: 24
Loss: 0.1684555826307852
Iteration: 25
Loss: 0.1580883347535435
Iteration: 26
Loss: 0.1476257575463645
Iteration: 27
Loss: 0.13768279986290993
Iteration: 28
Loss: 0.12875165149003645
Iteration: 29
Loss: 0.12006324058092094
Iteration: 30
Loss: 0.11111282491231267
Iteration: 31
Loss: 0.10307854480004008
Iteration: 32
Loss: 0.095369405289994
Iteration: 33
Loss: 0.08806072175502777
Iteration: 34
Loss: 0.08141697830037226
Iteration: 35
Loss: 0.07551176544231704
Iteration: 36
Loss: 0.06964190275985983
Iteration: 37
Loss: 0.06404868980185895
Iteration: 38
Loss: 0.05946718939120257
Iteration: 39
Loss: 0.05464643508761744
Iteration: 40
Loss: 0.050629355060526085
Iteration: 41
Loss: 0.0469229070917715
Iteration: 42
Loss: 0.04349285854568964
Iteration: 43
Loss: 0.04039336222259304
Iteration: 44
Loss: 0.037345007344891754
Iteration: 45
Loss: 0.03495067409888099
Iteration: 46
Loss: 0.03233624400594567
Iteration: 47
Loss: 0.03021160741867144
Iteration: 48
Loss: 0.028188650319470637
Iteration: 49
Loss: 0.026330087263184258
Iteration: 50
Loss: 0.0245526269596966
Iteration: 51
Loss: 0.023016327874192707
Iteration: 52
Loss: 0.021634858408117595
Iteration: 53
Loss: 0.020245046555241453
Iteration: 54
Loss: 0.019098520915530905
Iteration: 55
Loss: 0.018008357739146753
Iteration: 56
Loss: 0.016944454900354525
Iteration: 57
Loss: 0.016094528185793117
Iteration: 58
Loss: 0.015260842051121253
Iteration: 59
Loss: 0.01435696878293647
Iteration: 60
Loss: 0.013651815844298918
Iteration: 61
Loss: 0.012979140964961505
Iteration: 62
Loss: 0.01236408845156054
Iteration: 63
Loss: 0.011833674639840669
Iteration: 64
Loss: 0.011208782962794546
Iteration: 65
Loss: 0.01072121999827744
Iteration: 66
Loss: 0.010291379605290255
Iteration: 67
Loss: 0.009793487966909438
Iteration: 68
Loss: 0.009447024864014945
Iteration: 69
Loss: 0.009024762383461752
Iteration: 70
Loss: 0.008709128073687795
Iteration: 71
Loss: 0.008334217168673684
Iteration: 72
Loss: 0.00805352292247588
Iteration: 73
Loss: 0.007765873028790649
Iteration: 74
Loss: 0.0074534708916952335
Iteration: 75
Loss: 0.00719859143954855
Iteration: 76
Loss: 0.006995141240944968
Iteration: 77
Loss: 0.006763705117274312
Iteration: 78
Loss: 0.006564599137658937
Iteration: 79
Loss: 0.006378397504692968
Iteration: 80
Loss: 0.006167807592715644
Iteration: 81
Loss: 0.0059720173100881934
Iteration: 82
Loss: 0.00579286987386361
Iteration: 83
Loss: 0.005631767779211455
Iteration: 84
Loss: 0.005476640366442218
Iteration: 85
Loss: 0.0053410848685174804
Iteration: 86
Loss: 0.005210875456893369
Iteration: 87
Loss: 0.005101704009207366
Iteration: 88
Loss: 0.004965515182459656
Iteration: 89
Loss: 0.004884043390143521
Iteration: 90
Loss: 0.0047591332216523115
Iteration: 91
Loss: 0.004663860431247497
Iteration: 92
Loss: 0.004564411089390139
Iteration: 93
Loss: 0.004481420917082814
Iteration: 94
Loss: 0.004395504688396107
Iteration: 95
Loss: 0.004330138496087888
Iteration: 96
Loss: 0.004230258281967497
Iteration: 97
Loss: 0.004161887846391978
Iteration: 98
Loss: 0.004083358494069757
Iteration: 99
Loss: 0.004028673882042116
Iteration: 100
Loss: 0.003999939960533682
Iteration: 101
Loss: 0.003928180450950808
Iteration: 102
Loss: 0.0038671418051883766
Iteration: 103
Loss: 0.0038281043496312976
Iteration: 104
Loss: 0.0037742005336935383
Iteration: 105
Loss: 0.003722542997072392
Iteration: 106
Loss: 0.0036737597400110355
Iteration: 107
Loss: 0.0036338616628199816
Iteration: 108
Loss: 0.003613586177978712
Iteration: 109
Loss: 0.003556844009133645
Iteration: 110
Loss: 0.00353430553421944
Iteration: 111
Loss: 0.0034897542275677
Iteration: 112
Loss: 0.003478657035746529
Iteration: 113
Loss: 0.0034405168269678386
Iteration: 114
Loss: 0.003412376968425852
Iteration: 115
Loss: 0.003386709438283232
Iteration: 116
Loss: 0.003351803247144894
Iteration: 117
Loss: 0.0033334993427218515
Iteration: 118
Loss: 0.003310344237952104
Iteration: 119
Loss: 0.0032967219329615937
Iteration: 120
Loss: 0.0032630226592521502
Iteration: 121
Loss: 0.0032453450751549835
Iteration: 122
Loss: 0.0032116402754017825
Iteration: 123
Loss: 0.003192066975362316
Iteration: 124
Loss: 0.0031729180194861903
Iteration: 125
Loss: 0.0031775383520399846
Iteration: 126
Loss: 0.0031357542346408473
Iteration: 127
Loss: 0.003124283679725626
Iteration: 128
Loss: 0.003092890847002781
Iteration: 129
Loss: 0.0030783570955118422
Iteration: 130
Loss: 0.003065819728270739
Iteration: 131
Loss: 0.0030399952673270734
Iteration: 132
Loss: 0.003027225900559297
Iteration: 133
Loss: 0.0030110963248895315
Iteration: 134
Loss: 0.0029968386840169567
Iteration: 135
Loss: 0.0029816256771217796
Iteration: 136
Loss: 0.002964295296564321
Iteration: 137
Loss: 0.0029477404277253
Iteration: 138
Loss: 0.0029211137679558768
Iteration: 139
Loss: 0.002898632112560393
Iteration: 140
Loss: 0.0028970262825583357
Iteration: 141
Loss: 0.002880975925304656
Iteration: 142
Loss: 0.002861288650244286
Iteration: 143
Loss: 0.0028386349536337055
Iteration: 144
Loss: 0.002826112166306452
Iteration: 145
Loss: 0.0027972464007620193
Iteration: 146
Loss: 0.002794492698003408
Iteration: 147
Loss: 0.002785302931442857
Iteration: 148
Loss: 0.0027609869802394243
Iteration: 149
Loss: 0.002744465055462869
Iteration: 150
Loss: 0.002739190174078074
Iteration: 151
Loss: 0.002711766151876389
Iteration: 152
Loss: 0.0026880636357384017
Iteration: 153
Loss: 0.0026704111914563028
Iteration: 154
Loss: 0.002650712510641617
Iteration: 155
Loss: 0.0026297888223411918
Iteration: 156
Loss: 0.0026195519810094485
Iteration: 157
Loss: 0.0026045450469172455
Iteration: 158
Loss: 0.002579535089456771
Iteration: 159
Loss: 0.0025660376671607357
Iteration: 160
Loss: 0.0025541205106515297
Iteration: 161
Loss: 0.0025459523194858546
Iteration: 162
Loss: 0.002539287589466836
Iteration: 163
Loss: 0.0025187737980409512
Iteration: 164
Loss: 0.002491581630951996
Iteration: 165
Loss: 0.002479460190604382
Iteration: 166
Loss: 0.0024624972374331726
Iteration: 167
Loss: 0.0024586602827369034
Iteration: 168
Loss: 0.0024365639650038903
Iteration: 169
Loss: 0.0024309717826073683
Iteration: 170
Loss: 0.002404839189463778
Iteration: 171
Loss: 0.002428574900178215
Iteration: 172
Loss: 0.002398792757428711
Iteration: 173
Loss: 0.002378275676901582
Iteration: 174
Loss: 0.0023629217544169742
Iteration: 175
Loss: 0.0023597217388922655
Iteration: 176
Loss: 0.0023549945311785877
Iteration: 177
Loss: 0.0023308335582407406
Iteration: 178
Loss: 0.002348110082712543
Iteration: 179
Loss: 0.002311272806004633
Iteration: 180
Loss: 0.0023043803705633443
Iteration: 181
Loss: 0.002307293683149015
Iteration: 182
Loss: 0.002296254722566544
Iteration: 183
Loss: 0.0022881083128549443
Iteration: 184
Loss: 0.0022707727437108
Iteration: 185
Loss: 0.0022664474120033503
Iteration: 186
Loss: 0.0022547772224945357
Iteration: 187
Loss: 0.0022729128065719444
Iteration: 188
Loss: 0.002252211271125022
Iteration: 189
Loss: 0.00224061971500846
Iteration: 190
Loss: 0.002232993471058958
Iteration: 191
Loss: 0.0022111384366061302
Iteration: 192
Loss: 0.0022095003521918686
Iteration: 193
Loss: 0.002210785144103007
Iteration: 194
Loss: 0.0022020819213807207
Iteration: 195
Loss: 0.0021933772456773286
Iteration: 196
Loss: 0.0021862683661541416
Iteration: 197
Loss: 0.002185790112855125
Iteration: 198
Loss: 0.0021673948887169737
Iteration: 199
Loss: 0.00217696941381061
Iteration: 200
Loss: 0.002179351081199284
Iteration: 201
Loss: 0.002167989995000483
Iteration: 202
Loss: 0.0021731326256324594
Iteration: 203
Loss: 0.002163114916250298
Iteration: 204
Loss: 0.0021491287116998737
Iteration: 205
Loss: 0.002147846827224557
Iteration: 206
Loss: 0.0021479607403537705
Iteration: 207
Loss: 0.0021336745703592896
Iteration: 208
Loss: 0.002147216377838786
Iteration: 209
Loss: 0.002132734846724551
Iteration: 210
Loss: 0.0021289784411157044
Iteration: 211
Loss: 0.0021243591577734186
Iteration: 212
Loss: 0.0021211138920551992
Iteration: 213
Loss: 0.0021230056174571
Iteration: 214
Loss: 0.0021198398943495333
Iteration: 215
Loss: 0.002121994840688532
Iteration: 216
Loss: 0.0021201936318833805
Iteration: 217
Loss: 0.0021087635556070864
Iteration: 218
Loss: 0.002102380790865591
Iteration: 219
Loss: 0.0021109597114964
Iteration: 220
Loss: 0.002099529448071424
Iteration: 221
Loss: 0.002091678845271751
Iteration: 222
Loss: 0.0021013469785994177
Iteration: 223
Loss: 0.002093049312965308
Iteration: 224
Loss: 0.002092490102388436
Iteration: 225
Loss: 0.0020877053692960473
Iteration: 226
Loss: 0.0020840939228670505
Iteration: 227
Loss: 0.002083530785115082
Iteration: 228
Loss: 0.002075730252692688
Iteration: 229
Loss: 0.0020822765809613503
Iteration: 230
Loss: 0.002076378535726874
Iteration: 231
Loss: 0.002064798083274236
Iteration: 232
Loss: 0.002060529042058919
Iteration: 233
Loss: 0.0020684954839982565
Iteration: 234
Loss: 0.0020550301215976853
Iteration: 235
Loss: 0.002057550668044464
Iteration: 236
Loss: 0.002066008879249043
Iteration: 237
Loss: 0.002051989620115278
Iteration: 238
Loss: 0.002059023660796258
Iteration: 239
Loss: 0.0020489793247248553
Iteration: 240
Loss: 0.0020656875921177525
Iteration: 241
Loss: 0.0020424094171958822
Iteration: 242
Loss: 0.0020450637661085665
Iteration: 243
Loss: 0.0020465462209208855
Iteration: 244
Loss: 0.0020385533423882118
Iteration: 245
Loss: 0.0020570941682009005
Iteration: 246
Loss: 0.0020405379097460756
Iteration: 247
Loss: 0.0020443531389855133
Iteration: 248
Loss: 0.002037454549741896
Iteration: 249
Loss: 0.002052313325443317
Iteration: 250
Loss: 0.0020287438027584287
Iteration: 251
Loss: 0.0020358706087675654
Iteration: 252
Loss: 0.002038082592719812
Iteration: 253
Loss: 0.0020369972305702448
Iteration: 254
Loss: 0.0020332172781205443
Iteration: 255
Loss: 0.0020295719194662154
Iteration: 256
Loss: 0.0020151546422415706
Iteration: 257
Loss: 0.002021222117451267
Iteration: 258
Loss: 0.0020196032975623502
Iteration: 259
Loss: 0.002020122742728342
Iteration: 260
Loss: 0.002015769605442316
Iteration: 261
Loss: 0.002025425834934923
Iteration: 262
Loss: 0.002031043223459038
Iteration: 263
Loss: 0.002020205776325038
Iteration: 264
Loss: 0.0020119942285568466
Iteration: 265
Loss: 0.002021823287646793
Iteration: 266
Loss: 0.0020143263531212188
Iteration: 267
Loss: 0.002009692199563584
Iteration: 268
Loss: 0.0020153909756601613
Iteration: 269
Loss: 0.001999410097723049
Iteration: 270
Loss: 0.001992840451023341
Iteration: 271
Loss: 0.001994681887953436
Iteration: 272
Loss: 0.0020011122680351702
Iteration: 273
Loss: 0.002012830438339918
Iteration: 274
Loss: 0.0019978179030497615
Iteration: 275
Loss: 0.0019906330249519853
Iteration: 276
Loss: 0.001994857589619918
Iteration: 277
Loss: 0.001999356991705732
Iteration: 278
Loss: 0.0019938206370872786
Iteration: 279
Loss: 0.001990371805179534
Iteration: 280
Loss: 0.00199605411956016
Iteration: 281
Loss: 0.0019897505231319538
Iteration: 282
Loss: 0.0019994465548757324
Iteration: 283
Loss: 0.00198056285646803
Iteration: 284
Loss: 0.0019772175944091966
Iteration: 285
Loss: 0.001979475521152438
Iteration: 286
Loss: 0.001983813954741234
Iteration: 287
Loss: 0.0019859163550353504
Iteration: 288
Loss: 0.001991462227711572
Iteration: 289
Loss: 0.001993212530601628
Iteration: 290
Loss: 0.0019817196887835296
Iteration: 291
Loss: 0.001993461369088839
Iteration: 292
Loss: 0.0019856198437370456
Iteration: 293
Loss: 0.0019791292592505868
Iteration: 294
Loss: 0.001977228378299388
Iteration: 295
Loss: 0.001986940731766009
Iteration: 296
Loss: 0.001979792217983381
Iteration: 297
Loss: 0.001988586386123413
Iteration: 298
Loss: 0.001969410389473167
Iteration: 299
Loss: 0.0019791271505122886
Iteration: 300
Loss: 0.001984562259167433
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.37652111667859706
accuracy: 0.9751497860199715
confusion: 263 433 438 33916
precision: 0.37787356321839083
recall: 0.3751783166904422
Loading train data...
Loading val data...
Loading test data...
Training the TuckER model...
Number of training data points: 26745
Starting training...
Iteration: 1
Loss: 0.7028041217900529
Iteration: 2
Loss: 0.6950908054279376
Iteration: 3
Loss: 0.6754682712917086
Iteration: 4
Loss: 0.6334320570849166
Iteration: 5
Loss: 0.5724440722525874
Iteration: 6
Loss: 0.5063412777985199
Iteration: 7
Loss: 0.44691038207162787
Iteration: 8
Loss: 0.4036600823643841
Iteration: 9
Loss: 0.3663952599597883
Iteration: 10
Loss: 0.3405715792993956
Iteration: 11
Loss: 0.3202870891818517
Iteration: 12
Loss: 0.3034390241284914
Iteration: 13
Loss: 0.28798133662984343
Iteration: 14
Loss: 0.27592489349691174
Iteration: 15
Loss: 0.2637205452104158
Iteration: 16
Loss: 0.2528232166284247
Iteration: 17
Loss: 0.24120656995079184
Iteration: 18
Loss: 0.23018942622444297
Iteration: 19
Loss: 0.21955390142489084
Iteration: 20
Loss: 0.2086527473941634
Iteration: 21
Loss: 0.19814141585102565
Iteration: 22
Loss: 0.1877653696868993
Iteration: 23
Loss: 0.1774489772093447
Iteration: 24
Loss: 0.16694334545467474
Iteration: 25
Loss: 0.1565550224313253
Iteration: 26
Loss: 0.14708542201337935
Iteration: 27
Loss: 0.13713042645514767
Iteration: 28
Loss: 0.12794175281932083
Iteration: 29
Loss: 0.11913383516329754
Iteration: 30
Loss: 0.11065090824908848
Iteration: 31
Loss: 0.10224625015560584
Iteration: 32
Loss: 0.09471256483959246
Iteration: 33
Loss: 0.0873017396919335
Iteration: 34
Loss: 0.08101644546170778
Iteration: 35
Loss: 0.0751706829553918
Iteration: 36
Loss: 0.0692272331518463
Iteration: 37
Loss: 0.06409845873713493
Iteration: 38
Loss: 0.059127087693048426
Iteration: 39
Loss: 0.05459613593507417
Iteration: 40
Loss: 0.050421688186971445
Iteration: 41
Loss: 0.04674808817762363
Iteration: 42
Loss: 0.04343243131909189
Iteration: 43
Loss: 0.04020265244607684
Iteration: 44
Loss: 0.03711507445837878
Iteration: 45
Loss: 0.0344928666194783
Iteration: 46
Loss: 0.03230494284365751
Iteration: 47
Loss: 0.030232093025800547
Iteration: 48
Loss: 0.028027889448442037
Iteration: 49
Loss: 0.026088189310099506
Iteration: 50
Loss: 0.02453942332841173
Iteration: 51
Loss: 0.023112422586241854
Iteration: 52
Loss: 0.021556015798374066
Iteration: 53
Loss: 0.02026661537304709
Iteration: 54
Loss: 0.01907862579049189
Iteration: 55
Loss: 0.017913669583540928
Iteration: 56
Loss: 0.016951590186998815
Iteration: 57
Loss: 0.015974207840199713
Iteration: 58
Loss: 0.01517599408479431
Iteration: 59
Loss: 0.014386468142553975
Iteration: 60
Loss: 0.013667258622619924
Iteration: 61
Loss: 0.012956713031553015
Iteration: 62
Loss: 0.01235367679567654
Iteration: 63
Loss: 0.011784891173526456
Iteration: 64
Loss: 0.011186116445762447
Iteration: 65
Loss: 0.010763354021820087
Iteration: 66
Loss: 0.010268081946274902
Iteration: 67
Loss: 0.009803313302182698
Iteration: 68
Loss: 0.009450979625122457
Iteration: 69
Loss: 0.009010056021941614
Iteration: 70
Loss: 0.008678872527295276
Iteration: 71
Loss: 0.008319271799129776
Iteration: 72
Loss: 0.008051002380449939
Iteration: 73
Loss: 0.007720429161445627
Iteration: 74
Loss: 0.007428795470608564
Iteration: 75
Loss: 0.00720436384170493
Iteration: 76
Loss: 0.006954293627339073
Iteration: 77
Loss: 0.006720012389830774
Iteration: 78
Loss: 0.006532178472586071
Iteration: 79
Loss: 0.006282245704927776
Iteration: 80
Loss: 0.006105133703662248
Iteration: 81
Loss: 0.005963416463589367
Iteration: 82
Loss: 0.0057887857237571404
Iteration: 83
Loss: 0.005618853329480449
Iteration: 84
Loss: 0.005490236343886656
Iteration: 85
Loss: 0.0053574451705134364
Iteration: 86
Loss: 0.0052243264427385
Iteration: 87
Loss: 0.005063902716376359
Iteration: 88
Loss: 0.004971588955885625
Iteration: 89
Loss: 0.004842805239973189
Iteration: 90
Loss: 0.004770608783899983
Iteration: 91
Loss: 0.004637037810598371
Iteration: 92
Loss: 0.004548435924645466
Iteration: 93
Loss: 0.004461841946585646
Iteration: 94
Loss: 0.004392627756335313
Iteration: 95
Loss: 0.004300138992670027
Iteration: 96
Loss: 0.0042089918050679225
Iteration: 97
Loss: 0.004158629805934203
Iteration: 98
Loss: 0.004069967192444432
Iteration: 99
Loss: 0.004023508855459999
Iteration: 100
Loss: 0.003967282397648956
Iteration: 101
Loss: 0.0039011553000611594
Iteration: 102
Loss: 0.0038588889579795585
Iteration: 103
Loss: 0.003802605524446957
Iteration: 104
Loss: 0.0037547079256818263
Iteration: 105
Loss: 0.0037194121794044215
Iteration: 106
Loss: 0.0036596368668200094
Iteration: 107
Loss: 0.0036302830351964584
Iteration: 108
Loss: 0.0035945794727841886
Iteration: 109
Loss: 0.003562485150804248
Iteration: 110
Loss: 0.0035158650937808463
Iteration: 111
Loss: 0.003465087842667782
Iteration: 112
Loss: 0.0034456941935740695
Iteration: 113
Loss: 0.0034359984035026046
Iteration: 114
Loss: 0.0033927784342601707
Iteration: 115
Loss: 0.003351858074340639
Iteration: 116
Loss: 0.0033378646078319114
Iteration: 117
Loss: 0.0033114483179170874
Iteration: 118
Loss: 0.0032690447766946843
Iteration: 119
Loss: 0.003269207749303572
Iteration: 120
Loss: 0.0032362957319975656
Iteration: 121
Loss: 0.0032202881330600647
Iteration: 122
Loss: 0.003213397710572315
Iteration: 123
Loss: 0.003169798737250363
Iteration: 124
Loss: 0.0031570306024219417
Iteration: 125
Loss: 0.0031444478656247822
Iteration: 126
Loss: 0.0031225993180105205
Iteration: 127
Loss: 0.0031058602388712424
Iteration: 128
Loss: 0.0030775362201317957
Iteration: 129
Loss: 0.0030607686295539517
Iteration: 130
Loss: 0.0030491346953130222
Iteration: 131
Loss: 0.003039646009572699
Iteration: 132
Loss: 0.0030253913159234615
Iteration: 133
Loss: 0.002993513694433849
Iteration: 134
Loss: 0.0029839454572412032
Iteration: 135
Loss: 0.0029615259263664484
Iteration: 136
Loss: 0.0029598382166056317
Iteration: 137
Loss: 0.0029391569433191533
Iteration: 138
Loss: 0.0029302132993795074
Iteration: 139
Loss: 0.00290020614202264
Iteration: 140
Loss: 0.0028859987454135207
Iteration: 141
Loss: 0.0028668525451912156
Iteration: 142
Loss: 0.002857817504908654
Iteration: 143
Loss: 0.002848298386140149
Iteration: 144
Loss: 0.0028177146693762344
Iteration: 145
Loss: 0.002815628194403422
Iteration: 146
Loss: 0.002800241657945363
Iteration: 147
Loss: 0.0027720501876282917
Iteration: 148
Loss: 0.0027644464516234173
Iteration: 149
Loss: 0.0027512485497407143
Iteration: 150
Loss: 0.002732798857968065
Iteration: 151
Loss: 0.002776983594875547
Iteration: 152
Loss: 0.0027217860363093734
Iteration: 153
Loss: 0.0026904049600604213
Iteration: 154
Loss: 0.0026776800934178164
Iteration: 155
Loss: 0.0026560183054519983
Iteration: 156
Loss: 0.002643289757727445
Iteration: 157
Loss: 0.002641857843374527
Iteration: 158
Loss: 0.0026119775050378676
Iteration: 159
Loss: 0.0025979916541542434
Iteration: 160
Loss: 0.0025916508357688973
Iteration: 161
Loss: 0.0025753142707097003
Iteration: 162
Loss: 0.0025554887472970197
Iteration: 163
Loss: 0.002552551624094006
Iteration: 164
Loss: 0.0025322092697024345
Iteration: 165
Loss: 0.0025167029964018473
Iteration: 166
Loss: 0.002516894359613144
Iteration: 167
Loss: 0.0024957929596399206
Iteration: 168
Loss: 0.002469627882200706
Iteration: 169
Loss: 0.0024637691087172
Iteration: 170
Loss: 0.0024656727261652676
Iteration: 171
Loss: 0.002441867835277432
Iteration: 172
Loss: 0.0024225836342695773
Iteration: 173
Loss: 0.0024091194920194676
Iteration: 174
Loss: 0.00240201690776533
Iteration: 175
Loss: 0.0024070559906525704
Iteration: 176
Loss: 0.002375538808682674
Iteration: 177
Loss: 0.0023693077507888592
Iteration: 178
Loss: 0.002364183587458315
Iteration: 179
Loss: 0.0023586510214954615
Iteration: 180
Loss: 0.0023562867943008868
Iteration: 181
Loss: 0.002324664366985612
Iteration: 182
Loss: 0.002312377161534998
Iteration: 183
Loss: 0.0023205161737160215
Iteration: 184
Loss: 0.0023078588706948147
Iteration: 185
Loss: 0.0023070700168963286
Iteration: 186
Loss: 0.0022898163039331574
Iteration: 187
Loss: 0.002279632539146497
Iteration: 188
Loss: 0.0022626406177171047
Iteration: 189
Loss: 0.002265288322114775
Iteration: 190
Loss: 0.0022551443802098494
Iteration: 191
Loss: 0.0022478860248870487
Iteration: 192
Loss: 0.002233098325897244
Iteration: 193
Loss: 0.00223279637549827
Iteration: 194
Loss: 0.002226568590495971
Iteration: 195
Loss: 0.002223888394434618
Iteration: 196
Loss: 0.002213560183284969
Iteration: 197
Loss: 0.0022023355800517
Iteration: 198
Loss: 0.0022016420453503916
Iteration: 199
Loss: 0.0021862967803343374
Iteration: 200
Loss: 0.002187302414198182
Iteration: 201
Loss: 0.002196773388599859
Iteration: 202
Loss: 0.002175400860289324
Iteration: 203
Loss: 0.0021662466401894446
Iteration: 204
Loss: 0.0021617027099798377
Iteration: 205
Loss: 0.0021582424693705536
Iteration: 206
Loss: 0.002159406386810956
Iteration: 207
Loss: 0.00215334525255227
Iteration: 208
Loss: 0.0021600970022875488
Iteration: 209
Loss: 0.002158925369016449
Iteration: 210
Loss: 0.002144555502300021
Iteration: 211
Loss: 0.002133640755392328
Iteration: 212
Loss: 0.0021253359633721883
Iteration: 213
Loss: 0.0021331910975277424
Iteration: 214
Loss: 0.0021208068739056963
Iteration: 215
Loss: 0.0021322889598322254
Iteration: 216
Loss: 0.002123130936340629
Iteration: 217
Loss: 0.00212914225957769
Iteration: 218
Loss: 0.002122394098086825
Iteration: 219
Loss: 0.002119527235214563
Iteration: 220
Loss: 0.0021017161389529893
Iteration: 221
Loss: 0.0021037335354316083
Iteration: 222
Loss: 0.0020979207356849427
Iteration: 223
Loss: 0.0020895005200836287
Iteration: 224
Loss: 0.002091145355112945
Iteration: 225
Loss: 0.002091761735928115
Iteration: 226
Loss: 0.002081516692473824
Iteration: 227
Loss: 0.0020916808449627853
Iteration: 228
Loss: 0.0020768807823710804
Iteration: 229
Loss: 0.0020934740844760326
Iteration: 230
Loss: 0.002081332591220831
Iteration: 231
Loss: 0.0020810617060294445
Iteration: 232
Loss: 0.0020829859894075536
Iteration: 233
Loss: 0.0020826683111512395
Iteration: 234
Loss: 0.002074470289664555
Iteration: 235
Loss: 0.002070927258604501
Iteration: 236
Loss: 0.0020724487361274187
Iteration: 237
Loss: 0.0020523803467756207
Iteration: 238
Loss: 0.002058146480187962
Iteration: 239
Loss: 0.002056847417821424
Iteration: 240
Loss: 0.0020479633461070964
Iteration: 241
Loss: 0.0020654205677889384
Iteration: 242
Loss: 0.0020514268183467695
Iteration: 243
Loss: 0.0020530269233674943
Iteration: 244
Loss: 0.0020405801965065204
Iteration: 245
Loss: 0.0020525660203722647
Iteration: 246
Loss: 0.0020514343278718334
Iteration: 247
Loss: 0.0020411842947684325
Iteration: 248
Loss: 0.0020411102177834584
Iteration: 249
Loss: 0.0020401325943302127
Iteration: 250
Loss: 0.0020297232608582024
Iteration: 251
Loss: 0.0020259643781081406
Iteration: 252
Loss: 0.002031992565357138
Iteration: 253
Loss: 0.0020304463031595643
Iteration: 254
Loss: 0.002021823679627497
Iteration: 255
Loss: 0.0020283088073299468
Iteration: 256
Loss: 0.002028719672367354
Iteration: 257
Loss: 0.0020216144695052807
Iteration: 258
Loss: 0.0020250221537544003
Iteration: 259
Loss: 0.0020158205850379944
Iteration: 260
Loss: 0.0020150082890250825
Iteration: 261
Loss: 0.0020274821023421384
Iteration: 262
Loss: 0.0020287528905215896
Iteration: 263
Loss: 0.0020150959674457582
Iteration: 264
Loss: 0.0020127166534149193
Iteration: 265
Loss: 0.002019701709770421
Iteration: 266
Loss: 0.0020079154379760164
Iteration: 267
Loss: 0.0020252938184863595
Iteration: 268
Loss: 0.0020100942302990377
Iteration: 269
Loss: 0.0020361275550661775
Iteration: 270
Loss: 0.0020085870880704325
Iteration: 271
Loss: 0.0020171938187006533
Iteration: 272
Loss: 0.002004313682017353
Iteration: 273
Loss: 0.0020232156487889116
Iteration: 274
Loss: 0.0020159007318294314
Iteration: 275
Loss: 0.0020030519447607705
Iteration: 276
Loss: 0.0020040738104571457
Iteration: 277
Loss: 0.001999067207444695
Iteration: 278
Loss: 0.002000071383906599
Iteration: 279
Loss: 0.0019929659113735905
Iteration: 280
Loss: 0.002000291983573403
Iteration: 281
Loss: 0.00198361252676224
Iteration: 282
Loss: 0.001995187531272539
Iteration: 283
Loss: 0.001992700070354946
Iteration: 284
Loss: 0.0019851566625381763
Iteration: 285
Loss: 0.0019813231634612702
Iteration: 286
Loss: 0.001996215099846072
Iteration: 287
Loss: 0.0019827166474954802
Iteration: 288
Loss: 0.0019937158176156727
Iteration: 289
Loss: 0.001989726251274159
Iteration: 290
Loss: 0.0019823195430438353
Iteration: 291
Loss: 0.0019892948367220315
Iteration: 292
Loss: 0.0019939711679928477
Iteration: 293
Loss: 0.001987764532265218
Iteration: 294
Loss: 0.0019891667666055166
Iteration: 295
Loss: 0.001994933252211047
Iteration: 296
Loss: 0.0019838724232314133
Iteration: 297
Loss: 0.0019772047636723972
Iteration: 298
Loss: 0.00197489807659265
Iteration: 299
Loss: 0.0019850834299326887
Iteration: 300
Loss: 0.0019743964739167424
Mode: None
<class 'list'>
<class 'numpy.ndarray'>
f1: 0.3917851500789889
accuracy: 0.9780313837375179
confusion: 248 317 453 34032
precision: 0.4389380530973451
recall: 0.3537803138373752
Finding results in directory: ../../output/ecoli/tucker
[{'f1': 0.3719074221867518, 'accuracy': 0.977546362339515, 'tn': 34030, 'fp': 319, 'fn': 468, 'tp': 233, 'precision': 0.4221014492753623, 'recall': 0.33238231098430815}, {'f1': 0.38110236220472443, 'accuracy': 0.9775748930099857, 'tn': 34022, 'fp': 327, 'fn': 459, 'tp': 242, 'precision': 0.4253075571177504, 'recall': 0.34522111269614836}, {'f1': 0.37745098039215685, 'accuracy': 0.9782596291012838, 'tn': 34057, 'fp': 292, 'fn': 470, 'tp': 231, 'precision': 0.4416826003824092, 'recall': 0.32952924393723254}, {'f1': 0.37652111667859706, 'accuracy': 0.9751497860199715, 'tn': 33916, 'fp': 433, 'fn': 438, 'tp': 263, 'precision': 0.37787356321839083, 'recall': 0.3751783166904422}, {'f1': 0.3917851500789889, 'accuracy': 0.9780313837375179, 'tn': 34032, 'fp': 317, 'fn': 453, 'tp': 248, 'precision': 0.4389380530973451, 'recall': 0.3537803138373752}]
Aggregated results before taking average:
{'f1': [0.3719074221867518, 0.38110236220472443, 0.37745098039215685, 0.37652111667859706, 0.3917851500789889], 'accuracy': [0.977546362339515, 0.9775748930099857, 0.9782596291012838, 0.9751497860199715, 0.9780313837375179], 'tn': [34030, 34022, 34057, 33916, 34032], 'fp': [319, 327, 292, 433, 317], 'fn': [468, 459, 470, 438, 453], 'tp': [233, 242, 231, 263, 248], 'precision': [0.4221014492753623, 0.4253075571177504, 0.4416826003824092, 0.37787356321839083, 0.4389380530973451], 'recall': [0.33238231098430815, 0.34522111269614836, 0.32952924393723254, 0.3751783166904422, 0.3537803138373752]}
Aggregated results after taking average:
{'f1': '0.380±0.007', 'accuracy': '0.977±0.001', 'tn': '34011.400±49.119', 'fp': '337.600±49.119', 'fn': '457.600±11.569', 'tp': '243.400±11.569', 'precision': '0.421±0.023', 'recall': '0.347±0.017'}
num_iterations: [5]
batch_size: [128]
learning_rate: [0.0002]
decay_rate: [1.0]
ent_vec_dim: [200]
rel_vec_dim: [30]
input_dropout: [0.2]
hidden_dropout1: [0.4]
hidden_dropout2: [0.5]
label_smoothing: [0.1]
Loading train data...
Loading test data...
Training the TuckER model...
Number of training data points: 27446
Starting training...
Iteration: 1
Loss: 0.7028251267969609
Iteration: 2
Loss: 0.695508199185133
Iteration: 3
Loss: 0.6742511622607708
Iteration: 4
Loss: 0.630283959209919
Iteration: 5
Loss: 0.5670566864311695
Mode: final
